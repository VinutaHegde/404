{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq final model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aOPPa7UY_8l5",
        "HbI4DkqkIF01",
        "JORIOVS1IMaB"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lX-B3q67XANg",
        "outputId": "3364723b-a818-4a47-b164-4da2ed98a2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU, Masking\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "# from numpy import argmax\n",
        "# from pickle import load\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# # from nltk.translate.bleu_score import corpus_bleu\n",
        "import json\n",
        "# import random\n",
        "import csv\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#Wesam\n",
        "# SEED = 10\n",
        "#IMAGE_EMBEDDING_VAL_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/validation/'\n",
        "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
        "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
        "#CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
        "\n",
        "\n",
        "\n",
        "#on my Colab\n",
        "ALL_CAPTIONS_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/all_captions.txt'\n",
        "COMPLETE_STORIES_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/complete_stories_all_splits.json'\n",
        "IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
        "GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'\n",
        "\n",
        "\n",
        "# #For my GCP:\n",
        "# ALL_CAPTIONS_FILE = 'all_captions.txt'\n",
        "# COMPLETE_STORIES_FILE = 'complete_stories_all_splits.json'\n",
        "# IMAGE_EMBEDDING_DIR = 'CNNFeatureVectors/'\n",
        "# GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "\n",
        "\n",
        "#Vinuta\n",
        "SEED = 10\n",
        "IMAGE_EMBEDDING_DIM = 2048\n",
        "#IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
        "NUM_IMAGE_EMBEDDING_CHUNKS = 10\n",
        "#GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 11\n",
        "WORD_EMBEDDING_DIM = 300\n",
        "#CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'\n",
        "SENTENCE_EMBEDDING_DIM = 512"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvM68FbJ7h6",
        "colab_type": "code",
        "outputId": "588c82cb-ad57-4d8d-e629-10f017aa4edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#commented for GCP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iuq6CmUtYsEE"
      },
      "source": [
        "#PreProcess Captions / Stories\n",
        "\n",
        "Either call this function or simply load preprocessed from a file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqQ7qPFcRDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to check if story ids are repeated or unique\n",
        "# story_list = list()\n",
        "# for key in list(all_captions_dict.keys()):\n",
        "#   story_list += list(all_captions_dict[key].keys())\n",
        "# from collections import Counter\n",
        "# print(len(story_dict))\n",
        "# d =  Counter(story_dict)\n",
        "# res = [k for k, v in d.items() if v > 1]\n",
        "# print(len(res)) ## gave zero .. so story ids are -in fact- unique\n",
        "\n",
        "\n",
        "#image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "\n",
        "def get_existing_stories(image_embeddings):\n",
        "  #load all_captions file\n",
        "  with open(ALL_CAPTIONS_FILE) as json_file:\n",
        "    all_captions_dict = json.load(json_file)\n",
        "\n",
        "  #Create a story dict (no album ids (already checked that story ids are unique))\n",
        "  story_dict = {}\n",
        "  for key in list(all_captions_dict.keys()):\n",
        "    story_dict.update(all_captions_dict[key])\n",
        "\n",
        "\n",
        "  # Create a Story dict where all images are available in the image_embeddings\n",
        "  existing_stories = {}\n",
        "  c=0\n",
        "  for key in list(story_dict.keys()):\n",
        "    lists = story_dict[key]\n",
        "    images = [item[0] for item in lists]\n",
        "    #captions = ['startseq ' + item[1] + ' endseq' for item in lists]\n",
        "    captions = [item[1] for item in lists]\n",
        "    if all(img in list(image_embeddings.keys()) for img in images):\n",
        "      existing_stories[key] = [images,captions]\n",
        "      c+=1\n",
        "      \n",
        "  print(\"Number of Stories Found: \")\n",
        "  print(c)\n",
        "\n",
        "  # Saving the complete existing story dict in a file\n",
        "  with open(COMPLETE_STORIES_FILE, 'w') as fp:\n",
        "      json.dump(existing_stories, fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEKE0VJHSsU",
        "colab_type": "text"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16o6JbtNXHVa",
        "colab": {}
      },
      "source": [
        " def vocab_fun(existing_stories_dict):\n",
        "  index_to_word = {}\n",
        "  word_to_index = {}\n",
        "  max_seq_len=0\n",
        "  all_words = {}\n",
        "  all_words['startseq'] = 1\n",
        "  all_words['endseq'] = 1\n",
        "  cap_list = list()\n",
        "  for story_id, lists in existing_stories_dict.items():\n",
        "    for cap in lists[1]:\n",
        "      if(len(cap.split())>max_seq_len):\n",
        "        max_seq_len = len(cap.split())\n",
        "      for word in cap.split():\n",
        "        all_words[word] = 1\n",
        "  all_vocab=[w for w in all_words]\n",
        "  index = 1\n",
        "  for word in all_vocab:\n",
        "      word_to_index[word] = index\n",
        "      index_to_word[index] = word\n",
        "      index += 1\n",
        "\n",
        "\n",
        "\n",
        "  #MAX_SEQUENCE_LENGTH = #max_seq_len + 1\n",
        "  return (all_vocab, word_to_index, index_to_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-PgnES0nhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "873526e1-b701-4c56-c672-315a13010ab0"
      },
      "source": [
        "#to deciede sentence length:\n",
        "\n",
        "cap_lengths=[]\n",
        "for key, lists in existing_stories.items():\n",
        "    cap_list=lists[1]\n",
        "    for x in cap_list:\n",
        "      for c in x.split():\n",
        "        cap_lengths.append(len(c))\n",
        "print(np.mean(cap_lengths)+ 2 * np.std(cap_lengths))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.402385321992504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl15hHkcYxYj"
      },
      "source": [
        "#Preprocess images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90V-9vXXYjgx",
        "colab": {}
      },
      "source": [
        "def Merge(dict1, dict2): \n",
        "    res = {**dict1, **dict2} \n",
        "    return res \n",
        "\n",
        "def getImageEmbedding(path):\n",
        "    image_embedding = {}\n",
        "    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):\n",
        "         file_name = path + 'cnn_group'+str(i+1)+'.json'\n",
        "         with open(file_name) as json_file:\n",
        "#    with open(file_name) as json_file:\n",
        "            print(file_name)\n",
        "            json_data = json.load(json_file)\n",
        "            json_data = json.loads(json_data)\n",
        "            image_embedding = Merge(image_embedding, json_data) \n",
        "            #image_embedding = json_data \n",
        "    return image_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKPEQrk-FBz",
        "colab_type": "text"
      },
      "source": [
        "#Load Stories (captions with corresponding Image ids)\n",
        "##Dict items as follows (per story)\n",
        "[ [img_id1, img_id2, img_id3, img_id4, img_id5] , [cap1, cap2, cap3, cap4, cap5] ]\n",
        "\n",
        "Each captions starts with **startseq** and ends with **endseq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsfYfloT9dz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_existing_stories_from_file():\n",
        "  with open(COMPLETE_STORIES_FILE, 'r') as fp:\n",
        "      existing_stories = json.load(fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dJwBxEGaYNy"
      },
      "source": [
        "#Use Prev to get captions / stories and images and pre_process them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XMqm0aZXaYgo",
        "outputId": "c8562294-87bc-4cbe-ff94-726d8cef7187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#for training\n",
        "image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "print(len(image_embd))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group1.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group2.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group3.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group4.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group5.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group6.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group7.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group8.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group9.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group10.json\n",
            "58197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FZmz0inN6Z",
        "colab_type": "code",
        "outputId": "340bd8b1-8d01-46e1-d2e3-9fd5abf38fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#get existing_stories (either load from file or using a function , preferably load from file) -- uncomment one of the following 2 lines\n",
        "existing_stories = get_existing_stories_from_file()\n",
        "#existing_stories = get_existing_stories(image_embd) #Number of Stories Found: 35565\n",
        "\n",
        "all_vocab, wordtoix, ixtoword=vocab_fun(existing_stories)\n",
        "print('Max Seq Len: %d' %MAX_SEQUENCE_LENGTH)\n",
        "vocab_size = len(all_vocab) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size) #26571"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Seq Len: 11\n",
            "Vocabulary Size: 26572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9U_jDKeohyJ",
        "colab_type": "code",
        "outputId": "a5da505f-9ba1-4493-9141-72cb262f535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# print(existing_stories['2905'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['4562798695', '4563429868', '4562801065', '4563434166', '4563436748'], ['my trip to location was amazing .', 'i saw some colorful people .', 'i even made some knew friends .', 'i visited a beautiful pagoda .', 'then i saw a ordinary bicycle .']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8amlOYORaY6U"
      },
      "source": [
        "#Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0TgQ6BKaZSA",
        "outputId": "632ee172-6c78-4d44-996b-38a9800e3060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get matrix embedding for glove\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8fQm4YyfNix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save embedding matrix:\n",
        "np.save('/content/drive/My Drive/Colab_Notebooks/DL_data/embedding_matrix.npy', embedding_matrix) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m_4EtnggGhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save dicts needed during inference:\n",
        "with open('/content/drive/My Drive/Colab_Notebooks/DL_data/ixtoword.json', 'w') as fp:\n",
        "    json.dump(ixtoword, fp)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab_Notebooks/DL_data/wordtoix.json', 'w') as fp:\n",
        "    json.dump(wordtoix, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr-rypCuIBVe",
        "colab_type": "text"
      },
      "source": [
        "#IMAGE EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QClEP4J8gpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_indices(image_embd):\n",
        "  index = 1\n",
        "  img_to_index = {}\n",
        "  index_to_img = {}\n",
        "  for img_id in image_embd:\n",
        "      img_to_index[img_id] = index\n",
        "      index_to_img[index] = img_id\n",
        "      index += 1\n",
        "  return img_to_index, index_to_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZWE5KGIFGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creat image to index and index to image\n",
        "imgtoix, ixtoimg = get_image_indices(image_embd)\n",
        "\n",
        "img_embedding_matrix = np.zeros((len(image_embd)+1, IMAGE_EMBEDDING_DIM))\n",
        "for img, ix in imgtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = image_embd.get(img)\n",
        "    if embedding_vector is not None:\n",
        "        img_embedding_matrix[ix] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfq2bjxq_WW7",
        "colab_type": "text"
      },
      "source": [
        "#Input and output for the model\n",
        "###X1, X2, X3, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8tkr49aQ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def all_data(stories_dict, image_embd, wordtoix, max_length, num_of_stories):\n",
        "  X1, X2, X3, y = list(), list(), list(), list()\n",
        "  # sentence_encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\") #512\n",
        "  #to generate X1,X2,X3 and y. ////append-> have a list of image embeds , have a list for curr caption, have sentences embed for previous sentences, next word\n",
        "  #for each story:\n",
        "  for key, lists in stories_dict.items():\n",
        "    #break after retreiving num_of_stories\n",
        "    if num_of_stories <= 0:\n",
        "      break\n",
        "    num_of_stories -= 1\n",
        "    #if all image_ids exists in image_embd: --- done-- no need to check since checking is done during preprocessing\n",
        "    img_list=lists[0]\n",
        "    img_list_embed=[imgtoix[img_id] for img_id in img_list]#[image_embd[img_id] for img_id in img_list]\n",
        "    prev_list=lists[1].copy()\n",
        "    prev_list.pop()\n",
        "    # with tf.Session() as session:\n",
        "    #   session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    #   all_sen_embeddings = session.run(sentence_encoder(prev_list))\n",
        "    # all_sen_embeddings.tolist().insert(0,[0]*SENTECE_EMBEDDING_DIM) #for first sentence (empty vector)\n",
        "    prev_list.insert(0,'')\n",
        "\n",
        "    for counter in range(5):\n",
        "      img_seq = img_list_embed[:counter+1]\n",
        "      img_seq = pad_sequences([img_seq], maxlen=5)[0]\n",
        "      prev_sents = ' '.join(prev_list[:counter+1])\n",
        "      prev_embeddings = [wordtoix[word] for word in prev_sents.split() if word in wordtoix]\n",
        "      prev_embeddings = pad_sequences([prev_embeddings], maxlen=max_length*4)[0]\n",
        "      cap=lists[1][counter]\n",
        "      cap_in = 'startseq ' + cap\n",
        "      cap_out = cap + ' endseq'\n",
        "      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]\n",
        "      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]\n",
        "      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      if len(cap_out.split()) > 11:\n",
        "        seq_out[-1]=wordtoix['endseq']\n",
        "      #seq_out = to_categorical([seq_out], num_classes=vocab_size)[0]\n",
        "      #\n",
        "      \n",
        "      X1.append(img_seq)\n",
        "      X2.append(seq_in)\n",
        "      X3.append(prev_embeddings)\n",
        "      y.append(seq_out)\n",
        "\n",
        "  return (array(X1), array(X2), array(X3), array(y))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJW5iaDPL_Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rfJRzcLME5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"X\")\n",
        "# print(X2)\n",
        "# print(\"Y\")\n",
        "# print(y)\n",
        "\n",
        "\n",
        "# print(np.shape(X2))\n",
        "# print(np.shape(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpo33OpBdU2L"
      },
      "source": [
        "#Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IaC1BgwxdZsM",
        "colab": {}
      },
      "source": [
        "# # Things I should try:\n",
        "# # 1- bidirectional\n",
        "# # 2- return sequences and merge the two layers\n",
        "# # 3- must get good results on validation (the model must be generalizable)\n",
        "# # 4- modify hyper parameters (hidden layers, batch size, learning rate) (with larger batch size we can use larger learning rate)\n",
        "\n",
        "\n",
        "from keras.regularizers import l2, l1\n",
        "\n",
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "    return loss_mean\n",
        "\n",
        "\n",
        "def build_model():\n",
        "#   inputs1 = Input(shape=(5,),name='images')\n",
        "#   img1 = Embedding(len(image_embd)+1, IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False)(inputs1)\n",
        "#   img2 = Dropout(0.5)(img1)\n",
        "#   img3 , hidden_img = GRU(300, recurrent_dropout=0, return_state=True , activity_regularizer=l2(0.001))(img2)\n",
        "\n",
        "# #Concatenate()([forward_h, backward_h])\n",
        "#   inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,),name='prev_sentences')\n",
        "#   sen1= Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs3)\n",
        "#   sen2 = Dropout(0.5)(sen1)\n",
        "#   #sen3, sen_hidden_f, sen_hidden_b = Bidirectional(GRU(300,recurrent_dropout=0, return_state=True, activity_regularizer=l2(0.001)), merge_mode='sum')(sen2)\n",
        "#   sen3,sen_hidden_f , sen_hidden_b = Bidirectional(GRU(300,recurrent_dropout=0, return_state=True, activity_regularizer=l2(0.001)), merge_mode='sum')(sen2)\n",
        "#   hidden_sen =  add([sen_hidden_f,sen_hidden_b])\n",
        "#   decoder_hidden = add([hidden_img,hidden_sen])\n",
        "# #img_hidden\n",
        "\n",
        "#   inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='sequences')\n",
        "#   seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs2)\n",
        "#   seq2 = Dropout(0.5)(seq1)\n",
        "#   decoder1 = GRU(300,recurrent_dropout=0, return_sequences=True ,activity_regularizer=l2(0.001))(seq2, initial_state=decoder_hidden)\n",
        "# #initial_state\n",
        "#   #decoder1 = add([img3, seq3, sen2]) #maybe use Add()()?\n",
        "#   decoder2 = Dense(300, activation=None, kernel_regularizer=l2(0.001))(decoder1)\n",
        "#   outputs = Dense(vocab_size, activation='linear')(decoder2) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "#   model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "#   model.summary()\n",
        "#   #opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "#   #opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#   #opt = Adams(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "#   #opt = Adam(lr=0.1, decay=0.1)\n",
        "#   #opt = RMSprop(lr=0.001, rho=0.9)\n",
        "#   decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "#   model.compile(loss=sparse_cross_entropy, optimizer='adam', target_tensors=[decoder_target])#'adam')\n",
        "\n",
        "    inputs1 = Input(shape=(5,),name='inputs1_layer')\n",
        "    img1 = Embedding(len(image_embd)+1, IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False, name='img1_layer')(inputs1) #toChange\n",
        "    img2 = Dropout(0.5, name='img2_layer')(img1)\n",
        "    img3 , hidden_img = GRU(300, recurrent_dropout=0, return_state=True , activity_regularizer=l2(0.001), name='img3_layer')(img2)\n",
        "\n",
        "    #Concatenate()([forward_h, backward_h])\n",
        "    #prev_sentences_layer\n",
        "    inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,), name='inputs3_layer')\n",
        "    sen1= Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False, name='sen1_layer')(inputs3)\n",
        "    sen2 = Dropout(0.5, name='sen2_layer')(sen1)\n",
        "    sen3,sen_hidden_f , sen_hidden_b = Bidirectional(GRU(300,recurrent_dropout=0, return_state=True, activity_regularizer=l2(0.001)), merge_mode='sum', name='sen3_layer')(sen2)\n",
        "    hidden_sen =  add([sen_hidden_f,sen_hidden_b], name='hidden_sen_layer')\n",
        "    decoder_hidden = add([hidden_img,hidden_sen], name='decoder_hidden_layer')\n",
        "\n",
        "    #Current caption layer\n",
        "    inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='inputs2_layer')\n",
        "    seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False, name='seq1_layer')(inputs2)\n",
        "    seq2 = Dropout(0.5, name='seq2_layer')(seq1)\n",
        "    decoder1 = GRU(300,recurrent_dropout=0, return_sequences=True ,activity_regularizer=l2(0.001), name='decoder1_layer')(seq2, initial_state=decoder_hidden)\n",
        "\n",
        "\n",
        "    decoder2 = Dense(300, activation=None, kernel_regularizer=l2(0.001), name='decoder2_layer')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='linear', name='outputs_layer')(decoder2) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "    model.summary()\n",
        "    decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "    model.compile(loss=sparse_cross_entropy, optimizer='adam', target_tensors=[decoder_target])#'adam')\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCFNBmPWdZ71"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9RFuY7wfx4t",
        "outputId": "e96cfa21-30ac-4c6f-91c4-aed6af9c260d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=build_model()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs3_layer (InputLayer)      (None, 44)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inputs1_layer (InputLayer)      (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sen1_layer (Embedding)          (None, 44, 300)      7971600     inputs3_layer[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "img1_layer (Embedding)          (None, 5, 2048)      119189504   inputs1_layer[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sen2_layer (Dropout)            (None, 44, 300)      0           sen1_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "inputs2_layer (InputLayer)      (None, 11)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "img2_layer (Dropout)            (None, 5, 2048)      0           img1_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "sen3_layer (Bidirectional)      [(None, 300), (None, 1081800     sen2_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "seq1_layer (Embedding)          (None, 11, 300)      7971600     inputs2_layer[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "img3_layer (GRU)                [(None, 300), (None, 2114100     img2_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "hidden_sen_layer (Add)          (None, 300)          0           sen3_layer[0][1]                 \n",
            "                                                                 sen3_layer[0][2]                 \n",
            "__________________________________________________________________________________________________\n",
            "seq2_layer (Dropout)            (None, 11, 300)      0           seq1_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "decoder_hidden_layer (Add)      (None, 300)          0           img3_layer[0][1]                 \n",
            "                                                                 hidden_sen_layer[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_layer (GRU)            (None, 11, 300)      540900      seq2_layer[0][0]                 \n",
            "                                                                 decoder_hidden_layer[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_layer (Dense)          (None, 11, 300)      90300       decoder1_layer[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "outputs_layer (Dense)           (None, 11, 26572)    7998172     decoder2_layer[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 146,957,976\n",
            "Trainable params: 11,825,272\n",
            "Non-trainable params: 135,132,704\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V620wYUxnhZh",
        "colab_type": "text"
      },
      "source": [
        "#Or load saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6PjcblnmDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = load_model('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_checkpoint-ep033-loss3.835.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR4FhSY2cW1C",
        "colab_type": "code",
        "outputId": "b518d3e2-98bd-4fe0-b180-42a2c057a25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 35565)#35565\n",
        "\n",
        "# #for GCP\n",
        "# filepath_checkpoint = \"final_model_basic_all_stories-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "# filepath_model = 'final_model_basic_all_stories_finale.h5'\n",
        "\n",
        "# #for Colab\n",
        "filepath_checkpoint = \"/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_allstories_seqlen11-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "filepath_model = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_allstories_seqlen11_epoch1.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=False, mode='min')\n",
        "history = model.fit([X1,X2,X3], y, epochs=1, verbose=1, batch_size=32, validation_split=0.1, shuffle=True, workers=10, callbacks=[checkpoint], use_multiprocessing=True) #callbacks=[checkpoint]\n",
        "model.save(filepath_model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 160042 samples, validate on 17783 samples\n",
            "Epoch 1/1\n",
            "160042/160042 [==============================] - 905s 6ms/step - loss: 5.0656 - val_loss: 4.5747\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_allstories_seqlen11-ep001-loss5.066.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IWWkFQDqdij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save weights:\n",
        "model.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_all_weights_sure.h5')\n",
        "model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_all_stories_finale_pad_sure.h5')\n",
        "##model2.load_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiNcLTa2vorh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath_model_load = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_finale.h5'\n",
        "#sen3_layer\n",
        "# model_loaded = load_model(filepath_model_load, custom_objects={'sparse_cross_entropy': tf.nn.softmax_cross_entropy_with_logits})\n",
        "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "weights = model.get_weights()\n",
        "\n",
        "for name, weight in zip(names, weights):\n",
        "    print(name, weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lYar-m1MsHh",
        "colab_type": "text"
      },
      "source": [
        "#Another epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stDcdEbNMq-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "ebde0a4a-ca5a-4d34-f66e-c181d660652e"
      },
      "source": [
        "#filepath_model_epoch2 = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_all_stories_finale_pad_epoch2.h5'\n",
        "\n",
        "history = model.fit([X1,X2,X3], y, epochs=20, verbose=1, batch_size=32, validation_split=0.1, shuffle=True, workers=10, use_multiprocessing=True) #callbacks=[checkpoint]\n",
        "model.save(filepath_model)\n",
        "model.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_allstories_seqlen11_weights.h5')\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 160042 samples, validate on 17783 samples\n",
            "Epoch 1/20\n",
            "160042/160042 [==============================] - 901s 6ms/step - loss: 4.5790 - val_loss: 4.3767\n",
            "Epoch 2/20\n",
            "160042/160042 [==============================] - 896s 6ms/step - loss: 4.4186 - val_loss: 4.2632\n",
            "Epoch 3/20\n",
            "160042/160042 [==============================] - 889s 6ms/step - loss: 4.3187 - val_loss: 4.1857\n",
            "Epoch 4/20\n",
            "160042/160042 [==============================] - 888s 6ms/step - loss: 4.2499 - val_loss: 4.1484\n",
            "Epoch 5/20\n",
            "160042/160042 [==============================] - 889s 6ms/step - loss: 4.1973 - val_loss: 4.0966\n",
            "Epoch 6/20\n",
            "160042/160042 [==============================] - 887s 6ms/step - loss: 4.1592 - val_loss: 4.0895\n",
            "Epoch 7/20\n",
            "160042/160042 [==============================] - 890s 6ms/step - loss: 4.1266 - val_loss: 4.0649\n",
            "Epoch 8/20\n",
            "160042/160042 [==============================] - 894s 6ms/step - loss: 4.1164 - val_loss: 4.0274\n",
            "Epoch 9/20\n",
            "160042/160042 [==============================] - 901s 6ms/step - loss: 4.0858 - val_loss: 4.0159\n",
            "Epoch 10/20\n",
            "160042/160042 [==============================] - 898s 6ms/step - loss: 4.0914 - val_loss: 4.0693\n",
            "Epoch 11/20\n",
            "160042/160042 [==============================] - 898s 6ms/step - loss: 4.0952 - val_loss: 4.0101\n",
            "Epoch 12/20\n",
            "160042/160042 [==============================] - 890s 6ms/step - loss: 4.0492 - val_loss: 3.9896\n",
            "Epoch 13/20\n",
            " 33472/160042 [=====>........................] - ETA: 11:10 - loss: 4.0293Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqgR8n5oqf4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_all_stories_finale_pad_epoch2_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FoLOOVLH8Qg",
        "colab_type": "text"
      },
      "source": [
        "#Plot loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fu6qxITIplR",
        "colab_type": "code",
        "outputId": "ef9f3e13-4aab-42bc-8b52-dc50292f4140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.legend(['validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB37QjiG+qUkQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgcYWUXyUo_4",
        "colab_type": "text"
      },
      "source": [
        "#Rest is useless... ignore............................."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOPPa7UY_8l5",
        "colab_type": "text"
      },
      "source": [
        "#load trained weights and re-structure the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ0zRI0b_ngl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_weights = [layer.get_weights() for layer in model.layers]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyGrGnyuARk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#restructure\n",
        "#img_layer\n",
        "#inputs1 = Input(shape=(5,IMAGE_EMBEDDING_DIM),name='images')\n",
        "#img1 = Embedding(len(image_embd), IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False)(inputs1) #to remove\n",
        "inputs1 = Input(shape=(5,),name='inputs1_layer')\n",
        "img1 = Embedding(len(image_embd), IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False, name='img1_layer')(inputs1) #toChange\n",
        "img2 = Dropout(0.5, name='img2_layer')(img1)\n",
        "img3 , hidden_img = GRU(300, recurrent_dropout=0, return_state=True , activity_regularizer=l2(0.001), name='img3_layer')(img2)\n",
        "\n",
        "#Concatenate()([forward_h, backward_h])\n",
        "#prev_sentences_layer\n",
        "inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,), name='inputs3_layer')\n",
        "sen1= Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False, name='sen1_layer')(inputs3)\n",
        "sen2 = Dropout(0.5, name='sen2_layer')(sen1)\n",
        "sen3,sen_hidden_f , sen_hidden_b = Bidirectional(GRU(300,recurrent_dropout=0, return_state=True, activity_regularizer=l2(0.001)), merge_mode='sum', name='sen3_layer')(sen2)\n",
        "hidden_sen =  add([sen_hidden_f,sen_hidden_b], name='hidden_sen_layer')\n",
        "decoder_hidden = add([hidden_img,hidden_sen], name='decoder_hidden_layer')\n",
        "\n",
        "#Current caption layer\n",
        "inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='inputs2_layer')\n",
        "seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False, name='seq1_layer')(inputs2)\n",
        "seq2 = Dropout(0.5, name='seq2_layer')(seq1)\n",
        "decoder1 = GRU(300,recurrent_dropout=0, return_sequences=True ,activity_regularizer=l2(0.001), name='decoder1_layer')(seq2, initial_state=decoder_hidden)\n",
        "\n",
        "\n",
        "decoder2 = Dense(300, activation=None, kernel_regularizer=l2(0.001), name='decoder2_layer')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='linear', name='outputs_layer')(decoder2) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "model_inference_tmp = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "model_inference_tmp.summary()\n",
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "model_inference_tmp.compile(loss=sparse_cross_entropy, optimizer='adam', target_tensors=[decoder_target])#'adam')\n",
        "\n",
        "# #fill in weights back\n",
        "# for i in range(len(temp_weights)):\n",
        "#     model_inference.layers[i].set_weights(temp_weights[i])\n",
        "model_inference_tmp.set_weights(model.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48lZnfoQa7Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_inference_tmp.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_tmp_all_weights.h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJhfUtsbR5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_inference.load_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_tmp_all_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zd-sB2WSPAY",
        "colab_type": "text"
      },
      "source": [
        "#another model for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxC7MEtaR4hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #another model for inference\n",
        "# #restructure\n",
        "# #img_layer\n",
        "# inputs1 = model_inference_tmp.get_layer('images')#Input(shape=(5,),name='inputs1')\n",
        "# #img1 = #Embedding(len(image_embd), IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False, name='img1')(inputs1) #toChange\n",
        "# img2 = model_inference_tmp.get_layer('img2')#Dropout(0.5, name='img2')(img1)\n",
        "# img3 , hidden_img = GRU(300, recurrent_dropout=0, return_state=True , activity_regularizer=l2(0.001), name='img3')(img2)\n",
        "\n",
        "# #Concatenate()([forward_h, backward_h])\n",
        "# #prev_sentences_layer\n",
        "# inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,), name='inputs3')\n",
        "# sen1= Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False, name='sen1')(inputs3)\n",
        "# sen2 = Dropout(0.5, name='sen2')(sen1)\n",
        "# sen3,sen_hidden_f , sen_hidden_b = Bidirectional(GRU(300,recurrent_dropout=0, return_state=True, activity_regularizer=l2(0.001)), merge_mode='sum', name='sen3')(sen2)\n",
        "# hidden_sen =  add([sen_hidden_f,sen_hidden_b], name='hidden_sen')\n",
        "# decoder_hidden = add([hidden_img,hidden_sen], name='decoder_hidden')\n",
        "\n",
        "# #Current caption layer\n",
        "# inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='inputs2')\n",
        "# seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False, name='seq1')(inputs2)\n",
        "# seq2 = Dropout(0.5, name='seq2')(seq1)\n",
        "# decoder1 = GRU(300,recurrent_dropout=0, return_sequences=True ,activity_regularizer=l2(0.001), name='decoder1')(seq2, initial_state=decoder_hidden)\n",
        "\n",
        "\n",
        "# decoder2 = Dense(300, activation=None, kernel_regularizer=l2(0.001), name='decoder2')(decoder1)\n",
        "# outputs = Dense(vocab_size, activation='linear', name='outputs')(decoder2) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "# model_inference = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "# model_inference.summary()\n",
        "# decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "# model_inference.compile(loss=sparse_cross_entropy, optimizer='adam', target_tensors=[decoder_target])#'adam')\n",
        "\n",
        "# # #fill in weights back\n",
        "# # for i in range(len(temp_weights)):\n",
        "# #     model_inference.layers[i].set_weights(temp_weights[i])\n",
        "# model_inference.set_weights(model_inference_tmp.get_weights())\n",
        "\n",
        "\n",
        "# for layer in model_inference_tmp.get\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtbvI3bQUxRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "weights = model.get_weights()\n",
        "\n",
        "for name, weight in zip(names, weights):\n",
        "    print(name, weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sV_tGX5VY6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [weight.name for layer in model_inference.layers for weight in layer.weights]\n",
        "weights = model_inference.get_weights()\n",
        "\n",
        "for name, weight in zip(names, weights):\n",
        "    print(name, weight)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDS9WzzXFglf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #modify model_inference\n",
        "# #!pip3 install kerassurgeon\n",
        "\n",
        "# from kerassurgeon.operations import delete_layer, insert_layer, replace_layer\n",
        "# inputs1_new = Input(shape=(5,IMAGE_EMBEDDING_DIM),name='images')\n",
        "# model_inference = replace_layer(model_inference, model_inference.get_layer('images'), inputs1_new, copy=False)\n",
        "# model_inference = delete_layer(model_inference, model_inference.get_layer('image_embeds_layer'), copy=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgF7M3lCGPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_inference.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbI4DkqkIF01",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing Needed for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QPwcGZLhbMz",
        "colab": {}
      },
      "source": [
        "import os.path as osp\n",
        "import os\n",
        "from pprint import pprint\n",
        "from skimage.transform import rescale, resize\n",
        "from skimage import data, color, io\n",
        "import skimage\n",
        "import PIL\n",
        "import scipy\n",
        "import json\n",
        "import os.path\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications.xception import preprocess_input\n",
        "from keras.applications.xception import Xception\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "def load_image(image_path,target_size):\n",
        "    img = skimage.io.imread(image_path)\n",
        "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
        "    return image_resized\n",
        "\n",
        "def load_cnn_model():\n",
        "    model = Xception()\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    return model\n",
        "\n",
        "def extract_features_from_images(image_path):\n",
        "    model = load_cnn_model()\n",
        "    if path.exists(image_path):\n",
        "        print(image_path)\n",
        "        image = load_image(image_path, target_size=(299, 299))\n",
        "        if image.shape == (299, 299, 3):\n",
        "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "            image = preprocess_input(image)\n",
        "            feature = model.predict(image, verbose=0)\n",
        "            print(feature)\n",
        "            return feature\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_desc(model, photos, max_length):\n",
        "    prev_sents='' ##add to this\n",
        "    for counter in range(5):\n",
        "      in_text = 'startseq'\n",
        "      img_seq = photos[:counter+1]\n",
        "      img_seq = pad_sequences([img_seq], maxlen=5)[0]\n",
        "      prev_embeddings = [wordtoix[word] for word in prev_sents.split() if word in wordtoix]\n",
        "      prev_embeddings = pad_sequences([prev_embeddings], maxlen=max_length*4)[0]\n",
        "\n",
        "      cap=in_text\n",
        "      cap_in = 'startseq ' + cap\n",
        "      cap_out = cap + ' endseq'\n",
        "      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]\n",
        "      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post')[0]\n",
        "      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]\n",
        "      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post')[0]\n",
        "\n",
        "      yhat = model.predict([photos,sequence], verbose=0)\n",
        "      yhat = np.argmax(yhat)\n",
        "      # map integer to word\n",
        "      #word = ixtoword[yhat]\n",
        "      sentence = [ixtoword[word] for word in yhat if word in ixtoword]\n",
        "      if sentence is None:\n",
        "          break\n",
        "      in_text += ' ' + sentence\n",
        "      if sentence.split()[-1] == 'endseq':\n",
        "          break\n",
        "      ### add sentence to prev_sents\n",
        "    return prev_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORIOVS1IMaB",
        "colab_type": "text"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2aIfCVPhBAN",
        "colab": {}
      },
      "source": [
        "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg'#'/Users/vinutahegde/Documents/Personal/IMG_3501.JPG'\n",
        "feature=extract_features_from_images(image_path)\n",
        "features=[2,66,2,200]\n",
        "print(generate_desc(model, features, MAX_SEQUENCE_LENGTH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef857nMlhfGJ",
        "colab": {}
      },
      "source": [
        "# image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg'\n",
        "# feature=extract_features_from_images(image_path)\n",
        "# print(generate_desc(model, feature, max_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-THdS55IplV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e90ca20f-f772-4731-8010-e2bee128dccf"
      },
      "source": [
        "!python3 -c 'import keras; print(keras.__version__)'\n",
        "!python3 --version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2.2.5\n",
            "Python 3.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTdswli8CeQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f37aa7d0-330a-42fd-a8a8-66e542763d02"
      },
      "source": [
        "!python3 -c 'import tensorflow; print(tensorflow.__version__)'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}