{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq final model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lX-B3q67XANg",
        "outputId": "f2e584dd-ed2c-4c86-a0bb-309f80478e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU, Masking\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "# from numpy import argmax\n",
        "# from pickle import load\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# # from nltk.translate.bleu_score import corpus_bleu\n",
        "import json\n",
        "# import random\n",
        "import csv\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#Wesam\n",
        "# SEED = 10\n",
        "IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
        "#IMAGE_EMBEDDING_VAL_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/validation/'\n",
        "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
        "GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'\n",
        "CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
        "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
        "ALL_CAPTIONS_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/all_captions.txt'\n",
        "COMPLETE_STORIES_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/complete_stories_all_splits.json'\n",
        "IMAGE_EMBEDDING_DIM = 2048\n",
        "\n",
        "#Vinuta\n",
        "SEED = 10\n",
        "#IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
        "NUM_IMAGE_EMBEDDING_CHUNKS = 10\n",
        "#GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 94\n",
        "WORD_EMBEDDING_DIM = 300\n",
        "#CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'\n",
        "SENTENCE_EMBEDDING_DIM = 512"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvM68FbJ7h6",
        "colab_type": "code",
        "outputId": "c9bd319f-704a-48df-c45b-a7f64453f3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iuq6CmUtYsEE"
      },
      "source": [
        "#PreProcess Captions / Stories\n",
        "\n",
        "Either call this function or simply load preprocessed from a file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqQ7qPFcRDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to check if story ids are repeated or unique\n",
        "# story_list = list()\n",
        "# for key in list(all_captions_dict.keys()):\n",
        "#   story_list += list(all_captions_dict[key].keys())\n",
        "# from collections import Counter\n",
        "# print(len(story_dict))\n",
        "# d =  Counter(story_dict)\n",
        "# res = [k for k, v in d.items() if v > 1]\n",
        "# print(len(res)) ## gave zero .. so story ids are -in fact- unique\n",
        "\n",
        "\n",
        "#image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "\n",
        "def get_existing_stories(image_embeddings):\n",
        "  #load all_captions file\n",
        "  with open(ALL_CAPTIONS_FILE) as json_file:\n",
        "    all_captions_dict = json.load(json_file)\n",
        "\n",
        "  #Create a story dict (no album ids (already checked that story ids are unique))\n",
        "  story_dict = {}\n",
        "  for key in list(all_captions_dict.keys()):\n",
        "    story_dict.update(all_captions_dict[key])\n",
        "\n",
        "\n",
        "  # Create a Story dict where all images are available in the image_embeddings\n",
        "  existing_stories = {}\n",
        "  c=0\n",
        "  for key in list(story_dict.keys()):\n",
        "    lists = story_dict[key]\n",
        "    images = [item[0] for item in lists]\n",
        "    #captions = ['startseq ' + item[1] + ' endseq' for item in lists]\n",
        "    captions = [item[1] for item in lists]\n",
        "    if all(img in list(image_embeddings.keys()) for img in images):\n",
        "      existing_stories[key] = [images,captions]\n",
        "      c+=1\n",
        "      \n",
        "  print(\"Number of Stories Found: \")\n",
        "  print(c)\n",
        "\n",
        "  # Saving the complete existing story dict in a file\n",
        "  with open(COMPLETE_STORIES_FILE, 'w') as fp:\n",
        "      json.dump(existing_stories, fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEKE0VJHSsU",
        "colab_type": "text"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16o6JbtNXHVa",
        "colab": {}
      },
      "source": [
        " def vocab_fun(existing_stories_dict):\n",
        "  index_to_word = {}\n",
        "  word_to_index = {}\n",
        "  max_seq_len=0\n",
        "  all_words = {}\n",
        "  all_words['startseq'] = 1\n",
        "  all_words['endseq'] = 1\n",
        "  cap_list = list()\n",
        "  for story_id, lists in existing_stories_dict.items():\n",
        "    for cap in lists[1]:\n",
        "      if(len(cap.split())>max_seq_len):\n",
        "        max_seq_len = len(cap.split())\n",
        "      for word in cap.split():\n",
        "        all_words[word] = 1\n",
        "  all_vocab=[w for w in all_words]\n",
        "  index = 0\n",
        "  for word in all_vocab:\n",
        "      word_to_index[word] = index\n",
        "      index_to_word[index] = word\n",
        "      index += 1\n",
        "  MAX_SEQUENCE_LENGTH = max_seq_len + 2\n",
        "  return (all_vocab, word_to_index, index_to_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl15hHkcYxYj"
      },
      "source": [
        "#Preprocess images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90V-9vXXYjgx",
        "colab": {}
      },
      "source": [
        "def Merge(dict1, dict2): \n",
        "    res = {**dict1, **dict2} \n",
        "    return res \n",
        "\n",
        "def getImageEmbedding(path):\n",
        "    image_embedding = {}\n",
        "    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):\n",
        "         file_name = path + 'cnn_group'+str(i+1)+'.json'\n",
        "         with open(file_name) as json_file:\n",
        "#    with open(file_name) as json_file:\n",
        "            print(file_name)\n",
        "            json_data = json.load(json_file)\n",
        "            json_data = json.loads(json_data)\n",
        "            image_embedding = Merge(image_embedding, json_data) \n",
        "            #image_embedding = json_data \n",
        "    return image_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKPEQrk-FBz",
        "colab_type": "text"
      },
      "source": [
        "#Load Stories (captions with corresponding Image ids)\n",
        "##Dict items as follows (per story)\n",
        "[ [img_id1, img_id2, img_id3, img_id4, img_id5] , [cap1, cap2, cap3, cap4, cap5] ]\n",
        "\n",
        "Each captions starts with **startseq** and ends with **endseq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsfYfloT9dz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_existing_stories_from_file():\n",
        "  with open(COMPLETE_STORIES_FILE, 'r') as fp:\n",
        "      existing_stories = json.load(fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dJwBxEGaYNy"
      },
      "source": [
        "#Use Prev to get captions / stories and images and pre_process them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XMqm0aZXaYgo",
        "outputId": "0bcf06de-da77-44ea-d256-a6fd10ee1213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#for training\n",
        "image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "print(len(image_embd))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group1.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group2.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group3.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group4.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group5.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group6.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group7.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group8.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group9.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group10.json\n",
            "58197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FZmz0inN6Z",
        "colab_type": "code",
        "outputId": "071fb856-c4ae-4929-b98e-8ca477deb203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#get existing_stories (either load from file or using a function , preferably load from file) -- uncomment one of the following 2 lines\n",
        "existing_stories = get_existing_stories_from_file()\n",
        "#existing_stories = get_existing_stories(image_embd) #Number of Stories Found: 35565\n",
        "\n",
        "all_vocab, wordtoix, ixtoword=vocab_fun(existing_stories)\n",
        "print('Max Seq Len: %d' %MAX_SEQUENCE_LENGTH)\n",
        "vocab_size = len(all_vocab)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Seq Len: 94\n",
            "Vocabulary Size: 26571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9U_jDKeohyJ",
        "colab_type": "code",
        "outputId": "a5da505f-9ba1-4493-9141-72cb262f535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(existing_stories['2905'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['4562798695', '4563429868', '4562801065', '4563434166', '4563436748'], ['my trip to location was amazing .', 'i saw some colorful people .', 'i even made some knew friends .', 'i visited a beautiful pagoda .', 'then i saw a ordinary bicycle .']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8amlOYORaY6U"
      },
      "source": [
        "#Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0TgQ6BKaZSA",
        "outputId": "a348d196-87bd-49d9-888b-5a9f5a727b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get matrix embedding for glove\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr-rypCuIBVe",
        "colab_type": "text"
      },
      "source": [
        "#IMAGE EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QClEP4J8gpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_indices(image_embd):\n",
        "  index = 0\n",
        "  img_to_index = {}\n",
        "  index_to_img = {}\n",
        "  for img_id in image_embd:\n",
        "      img_to_index[img_id] = index\n",
        "      index_to_img[index] = img_id\n",
        "      index += 1\n",
        "  return img_to_index, index_to_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZWE5KGIFGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creat image to index and index to image\n",
        "imgtoix, ixtoimg = get_image_indices(image_embd)\n",
        "\n",
        "img_embedding_matrix = np.zeros((len(image_embd), IMAGE_EMBEDDING_DIM))\n",
        "for img, ix in imgtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = image_embd.get(img)\n",
        "    if embedding_vector is not None:\n",
        "        img_embedding_matrix[ix] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfq2bjxq_WW7",
        "colab_type": "text"
      },
      "source": [
        "#Input and output for the model\n",
        "###X1, X2, X3, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8tkr49aQ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def all_data(stories_dict, image_embd, wordtoix, max_length, num_of_stories):\n",
        "  X1, X2, X3, y = list(), list(), list(), list()\n",
        "  # sentence_encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\") #512\n",
        "  #to generate X1,X2,X3 and y. ////append-> have a list of image embeds , have a list for curr caption, have sentences embed for previous sentences, next word\n",
        "  #for each story:\n",
        "  for key, lists in stories_dict.items():\n",
        "    #break after retreiving num_of_stories\n",
        "    if num_of_stories <= 0:\n",
        "      break\n",
        "    num_of_stories -= 1\n",
        "    #if all image_ids exists in image_embd: --- done-- no need to check since checking is done during preprocessing\n",
        "    img_list=lists[0]\n",
        "    img_list_embed=[imgtoix[img_id] for img_id in img_list]#[image_embd[img_id] for img_id in img_list]\n",
        "    prev_list=lists[1].copy()\n",
        "    prev_list.pop()\n",
        "    # with tf.Session() as session:\n",
        "    #   session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    #   all_sen_embeddings = session.run(sentence_encoder(prev_list))\n",
        "    # all_sen_embeddings.tolist().insert(0,[0]*SENTECE_EMBEDDING_DIM) #for first sentence (empty vector)\n",
        "    prev_list.insert(0,'')\n",
        "\n",
        "    for counter in range(5):\n",
        "      img_seq = img_list_embed[:counter+1]\n",
        "      img_seq = pad_sequences([img_seq], maxlen=5)[0]\n",
        "      prev_sents = ' '.join(prev_list[:counter+1])\n",
        "      prev_embeddings = [wordtoix[word] for word in prev_sents.split() if word in wordtoix]\n",
        "      prev_embeddings = pad_sequences([prev_embeddings], maxlen=max_length*4)[0]\n",
        "      cap=lists[1][counter]\n",
        "      cap_in = 'startseq ' + cap\n",
        "      cap_out = cap + ' endseq'\n",
        "      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]\n",
        "      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post')[0]\n",
        "      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]\n",
        "      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post')[0]\n",
        "      #seq_out = to_categorical([seq_out], num_classes=vocab_size)[0]\n",
        "      \n",
        "      X1.append(img_seq)\n",
        "      X2.append(seq_in)\n",
        "      X3.append(prev_embeddings)\n",
        "      y.append(seq_out)\n",
        "\n",
        "  return (array(X1), array(X2), array(X3), array(y))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJW5iaDPL_Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 35565)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rfJRzcLME5h",
        "colab_type": "code",
        "outputId": "b1cb0f80-d508-432c-bab4-6deee9eb7885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print(np.shape(X2))\n",
        "# print(np.shape(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(177825, 94)\n",
            "(177825, 94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpo33OpBdU2L"
      },
      "source": [
        "#Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IaC1BgwxdZsM",
        "colab": {}
      },
      "source": [
        "# Things I should try:\n",
        "# 1- bidirectional\n",
        "# 2- return sequences and merge the two layers\n",
        "# 3- must get good results on validation (the model must be generalizable)\n",
        "# 4- modify hyper parameters (hidden layers, batch size, learning rate) (with larger batch size we can use larger learning rate)\n",
        "\n",
        "# from keras.regularizers import l2, l1\n",
        "\n",
        "# def build_model():\n",
        "#   inputs1 = Input(shape=(5,),name='images')\n",
        "#   img1 = Embedding(len(image_embd), IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False)(inputs1)\n",
        "#   img2 = Dropout(0.5)(img1)\n",
        "#   img3 = GRU(256, recurrent_dropout=0.3, activity_regularizer=l2(0.001))(img2)\n",
        "\n",
        "#   inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='sequences')\n",
        "#   seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs2)\n",
        "#   seq2 = Dropout(0.5)(seq1)\n",
        "#   seq3 = GRU(256,recurrent_dropout=0.3, activity_regularizer=l2(0.001))(seq2)\n",
        "  \n",
        "#   inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,),name='prev_sentences')\n",
        "#   sen1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs3)\n",
        "#   seq2 = Dropout(0.5)(seq1)\n",
        "#   sen2 = GRU(256,recurrent_dropout=0.3, activity_regularizer=l2(0.001))(sen1)\n",
        "\n",
        "\n",
        "#   decoder1 = add([img3, seq3, sen2])\n",
        "#   decoder2 = Dense(256, activation=None, kernel_regularizer=l2(0.001))(decoder1)\n",
        "#   outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "#   model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "#   return model\n",
        "\n",
        "\n",
        "\n",
        "from keras.regularizers import l2, l1\n",
        "\n",
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "    return loss_mean\n",
        "\n",
        "\n",
        "def build_model():\n",
        "  inputs1 = Input(shape=(5,),name='images')\n",
        "  img1 = Embedding(len(image_embd), IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False)(inputs1)\n",
        "  img2 = Dropout(0.5)(img1)\n",
        "  img3, img_hidden = GRU(300, recurrent_dropout=0, return_state=True , activity_regularizer=l2(0.001))(img2)\n",
        "\n",
        "\n",
        "#Concatenate()([forward_h, backward_h])\n",
        "  inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,),name='prev_sentences')\n",
        "  sen1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs3)\n",
        "  sen2 = Dropout(0.5)(sen1)\n",
        "  #sen3, sen_hidden_f, sen_hidden_b = Bidirectional(GRU(300,recurrent_dropout=0, return_state=True, activity_regularizer=l2(0.001)), merge_mode='sum')(sen2)\n",
        "  sen_hidden= Bidirectional(GRU(300,recurrent_dropout=0, activity_regularizer=l2(0.001)), merge_mode='sum')(sen2)\n",
        "  #sen_hidden =  add([sen_hidden_f,sen_hidden_b])\n",
        "  decoder_hidden = add([img_hidden,sen_hidden])\n",
        "#img_hidden\n",
        "\n",
        "  inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='sequences')\n",
        "  seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs2)\n",
        "  seq2 = Dropout(0.5)(seq1)\n",
        "  decoder1 = GRU(300,recurrent_dropout=0, return_sequences=True ,activity_regularizer=l2(0.001))(seq2, initial_state=decoder_hidden)\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "#initial_state\n",
        "\n",
        "  #decoder1 = add([img3, seq3, sen2]) #maybe use Add()()?\n",
        "  decoder2 = Dense(300, activation=None, kernel_regularizer=l2(0.001))(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='linear')(decoder2) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "  model.summary()\n",
        "  #opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "  #opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  #opt = Adams(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "  #opt = Adam(lr=0.1, decay=0.1)\n",
        "  #opt = RMSprop(lr=0.001, rho=0.9)\n",
        "  decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "  model.compile(loss=sparse_cross_entropy, optimizer='adam', target_tensors=[decoder_target])#'adam')\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCFNBmPWdZ71"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9RFuY7wfx4t",
        "outputId": "d190a174-0faa-49d4-a886-425b7762edf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "model=build_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "images (InputLayer)             (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "prev_sentences (InputLayer)     (None, 376)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_49 (Embedding)        (None, 5, 2048)      119187456   images[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_50 (Embedding)        (None, 376, 300)     7971300     prev_sentences[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "sequences (InputLayer)          (None, 94)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 5, 2048)      0           embedding_49[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 376, 300)     0           embedding_50[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_51 (Embedding)        (None, 94, 300)      7971300     sequences[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_49 (GRU)                    [(None, 300), (None, 2114100     dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_17 (Bidirectional (None, 300)          1081800     dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 94, 300)      0           embedding_51[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 300)          0           gru_49[0][1]                     \n",
            "                                                                 bidirectional_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gru_51 (GRU)                    (None, 94, 300)      540900      dropout_51[0][0]                 \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 94, 300)      90300       gru_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 94, 26571)    7997871     dense_33[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 146,955,027\n",
            "Trainable params: 11,824,971\n",
            "Non-trainable params: 135,130,056\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V620wYUxnhZh",
        "colab_type": "text"
      },
      "source": [
        "#Or load saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6PjcblnmDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = load_model('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_checkpoint-ep033-loss3.835.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR4FhSY2cW1C",
        "colab_type": "code",
        "outputId": "cd6dc73c-e435-49c7-93bb-90b5cbe2828d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 35565)#35565\n",
        "filepath_checkpoint = \"/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "#checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=False, mode='min')\n",
        "history = model.fit([X1,X2,X3], y, epochs=200, verbose=1, batch_size=32, validation_split=0.1, shuffle=True, workers=10, use_multiprocessing=True) #callbacks=[checkpoint]\n",
        "model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_finale.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 160042 samples, validate on 17783 samples\n",
            "Epoch 1/200\n",
            "140608/160042 [=========================>....] - ETA: 17:00 - loss: 1.4849"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FoLOOVLH8Qg",
        "colab_type": "text"
      },
      "source": [
        "#Plot loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fu6qxITIplR",
        "colab_type": "code",
        "outputId": "f6b8b105-945f-4546-8d49-5f1489e49fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbI4DkqkIF01",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing Needed for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QPwcGZLhbMz",
        "colab": {}
      },
      "source": [
        "import os.path as osp\n",
        "import os\n",
        "from pprint import pprint\n",
        "from skimage.transform import rescale, resize\n",
        "from skimage import data, color, io\n",
        "import skimage\n",
        "import PIL\n",
        "import scipy\n",
        "import json\n",
        "import os.path\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications.xception import preprocess_input\n",
        "from keras.applications.xception import Xception\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "def load_image(image_path,target_size):\n",
        "    img = skimage.io.imread(image_path)\n",
        "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
        "    return image_resized\n",
        "\n",
        "def load_cnn_model():\n",
        "    model = Xception()\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    return model\n",
        "\n",
        "def extract_features_from_images(image_path):\n",
        "    model = load_cnn_model()\n",
        "    if path.exists(image_path):\n",
        "        print(image_path)\n",
        "        image = load_image(image_path, target_size=(299, 299))\n",
        "        if image.shape == (299, 299, 3):\n",
        "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "            image = preprocess_input(image)\n",
        "            feature = model.predict(image, verbose=0)\n",
        "            print(feature)\n",
        "            return feature\n",
        "\n",
        "def generate_desc(model, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[word] for word in in_text.split() if word in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = ixtoword[yhat]\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORIOVS1IMaB",
        "colab_type": "text"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2aIfCVPhBAN",
        "colab": {}
      },
      "source": [
        "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg'#'/Users/vinutahegde/Documents/Personal/IMG_3501.JPG'\n",
        "feature=extract_features_from_images(image_path)\n",
        "print(generate_desc(model, feature, max_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef857nMlhfGJ",
        "colab": {}
      },
      "source": [
        "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg'\n",
        "feature=extract_features_from_images(image_path)\n",
        "print(generate_desc(model, feature, max_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-THdS55IplV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}