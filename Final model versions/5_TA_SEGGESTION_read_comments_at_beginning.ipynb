{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_TA_SEGGESTION_read comments at beginning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfSNell0rRx9",
        "colab_type": "text"
      },
      "source": [
        "###NOTES TO MYSELF:\n",
        "+ I'm using 2 inputs here\n",
        "\n",
        "+ I'm also using bidirectional decoder, and feeding the output from cap_i to the initial state of cap_i+1\n",
        "\n",
        "+ No image embedding\n",
        "\n",
        "+ max sequence length is 12\n",
        "\n",
        "+ using rmsprop optimizer\n",
        "\n",
        "-- Check validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lX-B3q67XANg",
        "outputId": "72e6232a-fa09-42ae-b61c-76f19a08dbea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU, Masking, Lambda, Concatenate, Average\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "import json\n",
        "import csv\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Wesam\n",
        "# SEED = 10\n",
        "#IMAGE_EMBEDDING_VAL_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/validation/'\n",
        "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
        "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
        "#CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
        "\n",
        "\n",
        "#on my Colab\n",
        "ALL_CAPTIONS_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/all_captions.txt'\n",
        "COMPLETE_STORIES_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/complete_stories_all_splits.json'\n",
        "IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
        "GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'\n",
        "\n",
        "\n",
        "# #For my GCP:\n",
        "# ALL_CAPTIONS_FILE = 'all_captions.txt'\n",
        "# COMPLETE_STORIES_FILE = 'complete_stories_all_splits.json'\n",
        "# IMAGE_EMBEDDING_DIR = 'CNNFeatureVectors/'\n",
        "# GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "\n",
        "\n",
        "#Vinuta\n",
        "SEED = 10\n",
        "IMAGE_EMBEDDING_DIM = 2048\n",
        "#IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
        "NUM_IMAGE_EMBEDDING_CHUNKS = 10\n",
        "#GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "WORD_EMBEDDING_DIM = 300\n",
        "#CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'\n",
        "SENTENCE_EMBEDDING_DIM = 512"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvM68FbJ7h6",
        "colab_type": "code",
        "outputId": "caa2ae3a-6fc4-4191-df8d-fd03910e640e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#commented for GCP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iuq6CmUtYsEE"
      },
      "source": [
        "#PreProcess Captions / Stories\n",
        "\n",
        "Either call this function or simply load preprocessed from a file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqQ7qPFcRDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to check if story ids are repeated or unique\n",
        "# story_list = list()\n",
        "# for key in list(all_captions_dict.keys()):\n",
        "#   story_list += list(all_captions_dict[key].keys())\n",
        "# from collections import Counter\n",
        "# print(len(story_dict))\n",
        "# d =  Counter(story_dict)\n",
        "# res = [k for k, v in d.items() if v > 1]\n",
        "# print(len(res)) ## gave zero .. so story ids are -in fact- unique\n",
        "\n",
        "\n",
        "#image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "\n",
        "def get_existing_stories(image_embeddings):\n",
        "  #load all_captions file\n",
        "  with open(ALL_CAPTIONS_FILE) as json_file:\n",
        "    all_captions_dict = json.load(json_file)\n",
        "\n",
        "  #Create a story dict (no album ids (already checked that story ids are unique))\n",
        "  story_dict = {}\n",
        "  for key in list(all_captions_dict.keys()):\n",
        "    story_dict.update(all_captions_dict[key])\n",
        "\n",
        "\n",
        "  # Create a Story dict where all images are available in the image_embeddings\n",
        "  existing_stories = {}\n",
        "  c=0\n",
        "  for key in list(story_dict.keys()):\n",
        "    lists = story_dict[key]\n",
        "    images = [item[0] for item in lists]\n",
        "    #captions = ['startseq ' + item[1] + ' endseq' for item in lists]\n",
        "    captions = [item[1] for item in lists]\n",
        "    if all(img in list(image_embeddings.keys()) for img in images):\n",
        "      existing_stories[key] = [images,captions]\n",
        "      c+=1\n",
        "      \n",
        "  print(\"Number of Stories Found: \")\n",
        "  print(c)\n",
        "\n",
        "  # Saving the complete existing story dict in a file\n",
        "  with open(COMPLETE_STORIES_FILE, 'w') as fp:\n",
        "      json.dump(existing_stories, fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEKE0VJHSsU",
        "colab_type": "text"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16o6JbtNXHVa",
        "colab": {}
      },
      "source": [
        " def vocab_fun(existing_stories_dict):\n",
        "  index_to_word = {}\n",
        "  word_to_index = {}\n",
        "  max_seq_len=0\n",
        "  all_words = {}\n",
        "  all_words['startseq'] = 1\n",
        "  all_words['endseq'] = 1\n",
        "  cap_list = list()\n",
        "  for story_id, lists in existing_stories_dict.items():\n",
        "    for cap in lists[1]:\n",
        "      if(len(cap.split())>max_seq_len):\n",
        "        max_seq_len = len(cap.split())\n",
        "      for word in cap.split():\n",
        "        all_words[word] = 1\n",
        "  all_vocab=[w for w in all_words]\n",
        "  index = 1\n",
        "  for word in all_vocab:\n",
        "      word_to_index[word] = index\n",
        "      index_to_word[index] = word\n",
        "      index += 1\n",
        "\n",
        "\n",
        "\n",
        "  #MAX_SEQUENCE_LENGTH = #max_seq_len + 1\n",
        "  return (all_vocab, word_to_index, index_to_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-PgnES0nhe",
        "colab_type": "code",
        "outputId": "d7c15411-7f34-49cc-9532-06a592d914e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# #to deciede sentence length:\n",
        "\n",
        "# cap_lengths=[]\n",
        "# for key, lists in existing_stories.items():\n",
        "#     cap_list=lists[1]\n",
        "#     for x in cap_list:\n",
        "#       for c in x.split():\n",
        "#         cap_lengths.append(len(c))\n",
        "# print(np.mean(cap_lengths)+ 2 * np.std(cap_lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.402385321992504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl15hHkcYxYj"
      },
      "source": [
        "#Preprocess images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90V-9vXXYjgx",
        "colab": {}
      },
      "source": [
        "def Merge(dict1, dict2): \n",
        "    res = {**dict1, **dict2} \n",
        "    return res \n",
        "\n",
        "def getImageEmbedding(path):\n",
        "    image_embedding = {}\n",
        "    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):\n",
        "         file_name = path + 'cnn_group'+str(i+1)+'.json'\n",
        "         with open(file_name) as json_file:\n",
        "#    with open(file_name) as json_file:\n",
        "            print(file_name)\n",
        "            json_data = json.load(json_file)\n",
        "            json_data = json.loads(json_data)\n",
        "            image_embedding = Merge(image_embedding, json_data) \n",
        "            #image_embedding = json_data \n",
        "    return image_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKPEQrk-FBz",
        "colab_type": "text"
      },
      "source": [
        "#Load Stories (captions with corresponding Image ids)\n",
        "##Dict items as follows (per story)\n",
        "[ [img_id1, img_id2, img_id3, img_id4, img_id5] , [cap1, cap2, cap3, cap4, cap5] ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsfYfloT9dz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_existing_stories_from_file():\n",
        "  with open(COMPLETE_STORIES_FILE, 'r') as fp:\n",
        "      existing_stories = json.load(fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dJwBxEGaYNy"
      },
      "source": [
        "#Use Prev to get captions / stories and images and pre_process them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XMqm0aZXaYgo",
        "outputId": "2a40c30e-5a36-4ea4-a7f5-7fcd027b16bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#for training\n",
        "image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "print(len(image_embd))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group1.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group2.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group3.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group4.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group5.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group6.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group7.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group8.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group9.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group10.json\n",
            "58197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FZmz0inN6Z",
        "colab_type": "code",
        "outputId": "6eac2b7d-ec28-4760-a5f0-e9c114656cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#get existing_stories (either load from file or using a function , preferably load from file) -- uncomment one of the following 2 lines\n",
        "existing_stories = get_existing_stories_from_file()\n",
        "#existing_stories = get_existing_stories(image_embd) #Number of Stories Found: 35565\n",
        "\n",
        "all_vocab, wordtoix, ixtoword=vocab_fun(existing_stories)\n",
        "print('Max Seq Len: %d' %MAX_SEQUENCE_LENGTH)\n",
        "vocab_size = len(all_vocab) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size) #26571"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Seq Len: 20\n",
            "Vocabulary Size: 26572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8amlOYORaY6U"
      },
      "source": [
        "#Word Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0TgQ6BKaZSA",
        "outputId": "e248225d-2d3d-4c13-89e9-17d978b56fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get matrix embedding for glove\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8fQm4YyfNix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #save embedding matrix:\n",
        "# np.save('/content/drive/My Drive/Colab_Notebooks/DL_data/embedding_matrix.npy', embedding_matrix) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m_4EtnggGhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #save dicts needed during inference:\n",
        "# with open('/content/drive/My Drive/Colab_Notebooks/DL_data/ixtoword.json', 'w') as fp:\n",
        "#     json.dump(ixtoword, fp)\n",
        "\n",
        "# with open('/content/drive/My Drive/Colab_Notebooks/DL_data/wordtoix.json', 'w') as fp:\n",
        "#     json.dump(wordtoix, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfq2bjxq_WW7",
        "colab_type": "text"
      },
      "source": [
        "#Input and output for the model\n",
        "###X1, X2, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8tkr49aQ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def all_data(stories_dict, image_embd, wordtoix, max_length, num_of_stories):\n",
        "  X1, X2, X3, y = list(), list(), list(), list()\n",
        "  #to generate X1,X2,X3 and y. ////append-> have a list of image embeds , have a list for curr caption, have sentences embed for previous sentences, next word\n",
        "  #for each story:\n",
        "  for key, lists in stories_dict.items():\n",
        "    #break after retreiving num_of_stories\n",
        "    if num_of_stories <= 0:\n",
        "      break\n",
        "    num_of_stories -= 1\n",
        "    \n",
        "    img_list=lists[0]\n",
        "    img_list_embed=[image_embd[img_id] for img_id in img_list]#[imgtoix[img_id] for img_id in img_list]\n",
        "    # prev_list=lists[1].copy()\n",
        "    # prev_list.pop()\n",
        "    # prev_list.insert(0,'')\n",
        "    in_cap_list = np.zeros((5,max_length))\n",
        "    out_cap_list = np.zeros((5,max_length))\n",
        "    for c in range(5):\n",
        "      #prev_embeddings = [wordtoix[word] for word in prev_list[c].split() if word in wordtoix]\n",
        "      #prev_list[c] = pad_sequences([prev_embeddings], maxlen=max_length)[0]\n",
        "\n",
        "      cap=lists[1][c]\n",
        "      cap_in = 'startseq ' + cap\n",
        "      cap_out = cap + ' endseq'\n",
        "      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]\n",
        "      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]\n",
        "      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      if len(cap_out.split()) > max_length:\n",
        "        seq_out[-1]=wordtoix['endseq']\n",
        "      in_cap_list[c] = np.array(seq_in)\n",
        "      out_cap_list[c] = np.array(seq_out)\n",
        "    X1.append(img_list_embed)\n",
        "    X2.append(in_cap_list)\n",
        "    #X3.append(prev_list)\n",
        "    y.append(np.concatenate(out_cap_list).ravel())##out_cap_list\n",
        "\n",
        "  return (array(X1), array(X2),  array(y))#array(X3),\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rfJRzcLME5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X1,X2,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 2)#35565\n",
        "\n",
        "# print(\"X\")\n",
        "# print(X2)\n",
        "# print(\"Y\")\n",
        "# print(y)\n",
        "# print(np.shape(X2))\n",
        "# print(np.shape(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y31ZtUjq0cle",
        "colab_type": "code",
        "outputId": "0443897e-5cb8-42d1-d2d6-0ec52766b572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print(\"X\")\n",
        "# print(np.shape(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "(2, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ZXWjtxxkMG",
        "colab_type": "code",
        "outputId": "23ddd744-ada4-42ac-9436-cee0c6a7bb56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 2)\n",
        "\n",
        "# print(\"X\")\n",
        "# print(X2)\n",
        "# print(\"Y\")\n",
        "# print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "[[[ 1.  3.  4.  5.  6.  7.  8.  9.  0.  0.  0.]\n",
            "  [ 1. 10. 11. 12. 13. 14.  9.  0.  0.  0.  0.]\n",
            "  [ 1. 10. 15. 16. 12. 17. 18.  9.  0.  0.  0.]\n",
            "  [ 1. 10. 19. 20. 21. 22.  9.  0.  0.  0.  0.]\n",
            "  [ 1. 23. 10. 11. 20. 24. 25.  9.  0.  0.  0.]]\n",
            "\n",
            " [[ 1. 26. 27. 20. 28. 29. 30. 31. 32. 33. 28.]\n",
            "  [ 1. 28. 34. 20. 35. 36. 31. 14. 37. 38. 39.]\n",
            "  [ 1. 14. 44. 45. 46. 45. 47. 48. 49. 31. 50.]\n",
            "  [ 1. 31. 51. 42. 21. 45. 52.  9.  0.  0.  0.]\n",
            "  [ 1. 53. 54. 55. 56. 57. 36. 58. 31. 50. 59.]]]\n",
            "Y\n",
            "[[ 3.  4.  5.  6.  7.  8.  9.  2.  0.  0.  0. 10. 11. 12. 13. 14.  9.  2.\n",
            "   0.  0.  0.  0. 10. 15. 16. 12. 17. 18.  9.  2.  0.  0.  0. 10. 19. 20.\n",
            "  21. 22.  9.  2.  0.  0.  0.  0. 23. 10. 11. 20. 24. 25.  9.  2.  0.  0.\n",
            "   0.]\n",
            " [26. 27. 20. 28. 29. 30. 31. 32. 33. 28.  2. 28. 34. 20. 35. 36. 31. 14.\n",
            "  37. 38. 39.  2. 14. 44. 45. 46. 45. 47. 48. 49. 31. 50.  2. 31. 51. 42.\n",
            "  21. 45. 52.  9.  2.  0.  0.  0. 53. 54. 55. 56. 57. 36. 58. 31. 50. 59.\n",
            "   2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSm7P42Yy-Kp",
        "colab_type": "code",
        "outputId": "28b2b96c-54a7-4dba-e4d5-952bdb4c9a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print(np.shape(y))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpo33OpBdU2L"
      },
      "source": [
        "#Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IaC1BgwxdZsM",
        "colab": {}
      },
      "source": [
        "# # Things I should try:\n",
        "# # 1- bidirectional\n",
        "# # 2- return sequences and merge the two layers\n",
        "# # 3- must get good results on validation (the model must be generalizable)\n",
        "# # 4- modify hyper parameters (hidden layers, batch size, learning rate) (with larger batch size we can use larger learning rate)\n",
        "\n",
        "\n",
        "from keras.regularizers import l2, l1\n",
        "from keras import backend as K\n",
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "    return loss_mean\n",
        "\n",
        "# def my_sparse_categorical_accuracy(y_true, y_pred):\n",
        "#   return K.cast(\n",
        "#         K.equal(K.cast(K.max(y_true), K.floatx()),\n",
        "#                 K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n",
        "#         K.floatx()\n",
        "#         )\n",
        "\n",
        "# def custom_sparse_categorical_accuracy(y_true, y_pred):\n",
        "#     return K.cast(K.equal(K.max(y_true, axis=-1),\n",
        "#                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),K.floatx())\n",
        "def build_model():\n",
        "###slice = Lambda(lambda x: x[:, i])(input)\n",
        "    #Image Encoder\n",
        "    img_input = Input(shape=(5,IMAGE_EMBEDDING_DIM))\n",
        "    img_encoder_1 = Dropout(0)(img_input)\n",
        "    img_encoder = GRU(300, recurrent_dropout=0, return_sequences=True , activity_regularizer=l2(0.000))(img_encoder_1) ##might add another layer\n",
        "    img1_enc = Lambda(lambda x: x[:, 0, :])(img_encoder)\n",
        "    img2_enc = Lambda(lambda x: x[:, 1, :])(img_encoder)\n",
        "    img3_enc = Lambda(lambda x: x[:, 2, :])(img_encoder)\n",
        "    img4_enc = Lambda(lambda x: x[:, 3, :])(img_encoder)\n",
        "    img5_enc = Lambda(lambda x: x[:, 4, :])(img_encoder)\n",
        "    #Concatenate()([forward_h, backward_h])\n",
        "\n",
        "    #Previous Sentences Encoder\n",
        "    # prev_sents_input = Input(shape=(5,MAX_SEQUENCE_LENGTH))\n",
        "    # sent1_in = Lambda(lambda x: x[:, 0, :])(prev_sents_input)\n",
        "    # sent2_in = Lambda(lambda x: x[:, 1, :])(prev_sents_input)\n",
        "    # sent3_in = Lambda(lambda x: x[:, 2, :])(prev_sents_input)\n",
        "    # sent4_in = Lambda(lambda x: x[:, 3, :])(prev_sents_input)\n",
        "    # sent5_in = Lambda(lambda x: x[:, 4, :])(prev_sents_input)\n",
        "    \n",
        "    #Embed each:\n",
        "    Word_Embedder = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)\n",
        "    # sent1_emb = Word_Embedder(sent1_in)\n",
        "    # sent2_emb = Word_Embedder(sent2_in)\n",
        "    # sent3_emb = Word_Embedder(sent3_in)\n",
        "    # sent4_emb = Word_Embedder(sent4_in)\n",
        "    # sent5_emb = Word_Embedder(sent5_in)\n",
        "    \n",
        "    #define previouse sentences encoder:\n",
        "    prev_encoder = GRU(100, recurrent_dropout=0, dropout=0.0 , return_sequences=False, activity_regularizer=l2(0.000))\n",
        "\n",
        "    # #now use the encoder:\n",
        "    # sent1_enc = prev_encoder(sent1_emb)\n",
        "    # sent2_enc = prev_encoder(sent2_emb , initial_state = sent1_enc)\n",
        "    # sent3_enc = prev_encoder(sent3_emb , initial_state = sent2_enc)\n",
        "    # sent4_enc = prev_encoder(sent4_emb , initial_state = sent3_enc)\n",
        "    # sent5_enc = prev_encoder(sent5_emb , initial_state = sent4_enc)\n",
        "\n",
        "\n",
        "    # #now prepare for decoder:\n",
        "    # hidden_1 = concatenate([sent1_enc,img1_enc])\n",
        "    # hidden_2 = concatenate([sent2_enc,img2_enc])\n",
        "    # hidden_3 = concatenate([sent3_enc,img3_enc])\n",
        "    # hidden_4 = concatenate([sent4_enc,img4_enc])\n",
        "    # hidden_5 = concatenate([sent5_enc,img5_enc])\n",
        "\n",
        "    #Decoder:\n",
        "    decoder = Bidirectional(GRU(300,recurrent_dropout=0, dropout=0.0 ,return_sequences=True, return_state=True ,activity_regularizer=l2(0.000)),  merge_mode='concat' )\n",
        "    \n",
        "    #Current captions\n",
        "    captions_input = Input(shape=(5,MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "    #split\n",
        "    cap1_in = Lambda(lambda x: x[:, 0, :])(captions_input)\n",
        "    cap2_in = Lambda(lambda x: x[:, 1, :])(captions_input)\n",
        "    cap3_in = Lambda(lambda x: x[:, 2, :])(captions_input)\n",
        "    cap4_in = Lambda(lambda x: x[:, 3, :])(captions_input)\n",
        "    cap5_in = Lambda(lambda x: x[:, 4, :])(captions_input)\n",
        "    \n",
        "    #Embed each:\n",
        "    cap1_emb = Word_Embedder(cap1_in)\n",
        "    cap2_emb = Word_Embedder(cap2_in)\n",
        "    cap3_emb = Word_Embedder(cap3_in)\n",
        "    cap4_emb = Word_Embedder(cap4_in)\n",
        "    cap5_emb = Word_Embedder(cap5_in)\n",
        "\n",
        "    # Concat\n",
        "    #hidden_1 = concatenate([sent1_enc,img1_enc])\n",
        "\n",
        "\n",
        "\n",
        "    #Decode\n",
        "    cap1_dec, prev_1_f , prev_1_b = decoder(cap1_emb, initial_state = [img1_enc,img1_enc] )\n",
        "\n",
        "    fprev_1 = Average()([prev_1_f,img2_enc])\n",
        "    bprev_1 = Average()([prev_1_b,img2_enc])\n",
        "    hidden_2 = [fprev_1,bprev_1]\n",
        "    cap2_dec, prev_2_f, prev_2_b= decoder(cap2_emb , initial_state = hidden_2)\n",
        "\n",
        "    fprev_2 = Average()([prev_2_f,img3_enc])\n",
        "    bprev_2 = Average()([prev_2_b,img3_enc])\n",
        "    hidden_3 = [fprev_2,bprev_2]\n",
        "    cap3_dec, prev_3_f, prev_3_b = decoder(cap3_emb , initial_state = hidden_3)\n",
        "\n",
        "    fprev_3 = Average()([prev_3_f,img4_enc])\n",
        "    bprev_3 = Average()([prev_3_b,img4_enc])\n",
        "    hidden_4 = [fprev_3,bprev_3]\n",
        "    cap4_dec, prev_4_f, prev_4_b = decoder(cap4_emb , initial_state = hidden_4)\n",
        "\n",
        "    fprev_4 = Average()([prev_4_f,img5_enc])\n",
        "    bprev_4 = Average()([prev_4_b,img5_enc])\n",
        "    hidden_5 = [fprev_4,bprev_4]\n",
        "    cap5_dec, prev_5_f, prev_5_b = decoder(cap5_emb , initial_state = hidden_5)\n",
        "\n",
        "    decoder_out = concatenate([cap1_dec, cap2_dec, cap3_dec, cap4_dec, cap5_dec], axis=-2)\n",
        "\n",
        "    decoder_dense = Dense(200, activation=None, kernel_regularizer=l2(0.000))(decoder_out)\n",
        "    outputs = Dense(vocab_size, activation='linear')(decoder_dense) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "    model = Model(inputs=[img_input, captions_input], outputs=outputs)\n",
        "\n",
        "    model.summary()\n",
        "    decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "    model.compile(loss=sparse_cross_entropy, optimizer='rmsprop', target_tensors=[decoder_target])#'adam') #, target_tensors=[decoder_target]\n",
        "#'sparse_categorical_crossentropy'\n",
        "#tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "# metrics=[custom_sparse_categorical_accuracy]\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCFNBmPWdZ71"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9RFuY7wfx4t",
        "outputId": "d05e9f99-f734-4bed-ecc1-bd62a52367d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=build_model()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 5, 2048)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 5, 20)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 5, 2048)      0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 20)           0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 5, 300)       2114100     dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 300)      7971600     lambda_6[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "                                                                 lambda_8[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 300)          0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) [(None, 20, 600), (N 1081800     embedding_1[0][0]                \n",
            "                                                                 lambda_1[0][0]                   \n",
            "                                                                 lambda_1[0][0]                   \n",
            "                                                                 embedding_1[1][0]                \n",
            "                                                                 average_1[0][0]                  \n",
            "                                                                 average_2[0][0]                  \n",
            "                                                                 embedding_1[2][0]                \n",
            "                                                                 average_3[0][0]                  \n",
            "                                                                 average_4[0][0]                  \n",
            "                                                                 embedding_1[3][0]                \n",
            "                                                                 average_5[0][0]                  \n",
            "                                                                 average_6[0][0]                  \n",
            "                                                                 embedding_1[4][0]                \n",
            "                                                                 average_7[0][0]                  \n",
            "                                                                 average_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 20)           0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 300)          0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_1 (Average)             (None, 300)          0           bidirectional_1[0][1]            \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_2 (Average)             (None, 300)          0           bidirectional_1[0][2]            \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 20)           0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 300)          0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_3 (Average)             (None, 300)          0           bidirectional_1[1][1]            \n",
            "                                                                 lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_4 (Average)             (None, 300)          0           bidirectional_1[1][2]            \n",
            "                                                                 lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 20)           0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 300)          0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_5 (Average)             (None, 300)          0           bidirectional_1[2][1]            \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_6 (Average)             (None, 300)          0           bidirectional_1[2][2]            \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 20)           0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 300)          0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_7 (Average)             (None, 300)          0           bidirectional_1[3][1]            \n",
            "                                                                 lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_8 (Average)             (None, 300)          0           bidirectional_1[3][2]            \n",
            "                                                                 lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 100, 600)     0           bidirectional_1[0][0]            \n",
            "                                                                 bidirectional_1[1][0]            \n",
            "                                                                 bidirectional_1[2][0]            \n",
            "                                                                 bidirectional_1[3][0]            \n",
            "                                                                 bidirectional_1[4][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100, 200)     120200      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 100, 26572)   5340972     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 16,628,672\n",
            "Trainable params: 8,657,072\n",
            "Non-trainable params: 7,971,600\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V620wYUxnhZh",
        "colab_type": "text"
      },
      "source": [
        "#Or load saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6PjcblnmDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = load_model('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_checkpoint-ep033-loss3.835.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGbmz4VFhb83",
        "colab_type": "text"
      },
      "source": [
        "#Prep for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR4FhSY2cW1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1,X2,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 1000)#35565\n",
        "#for GCP\n",
        "# filepath_checkpoint = \"final_model_all_stories-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "# filepath_model = 'final_model_all_stories_finale.h5'\n",
        "\n",
        "# # #for Colab\n",
        "# filepath_checkpoint = \"/content/drive/My Drive/Colab_Notebooks/DL_data/models/Suggested_10_-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "# filepath_model = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/Suggested_10_epoch1.h5'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnO5JdeKhhtS",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vHfOqvchi3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "6618711c-e484-487d-cf0f-b4c2e7030f05"
      },
      "source": [
        "\n",
        "#checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "history = model.fit([X1,X2], y, epochs=20, verbose=2, batch_size=32, validation_split=0.1, shuffle=True, workers=10, use_multiprocessing=True) #callbacks=[checkpoint]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 900 samples, validate on 100 samples\n",
            "Epoch 1/20\n",
            " - 18s - loss: 4.2788 - val_loss: 2.8136\n",
            "Epoch 2/20\n",
            " - 10s - loss: 2.7624 - val_loss: 2.1815\n",
            "Epoch 3/20\n",
            " - 11s - loss: 2.0458 - val_loss: 1.7114\n",
            "Epoch 4/20\n",
            " - 11s - loss: 1.5326 - val_loss: 1.3702\n",
            "Epoch 5/20\n",
            " - 11s - loss: 1.1564 - val_loss: 1.1449\n",
            "Epoch 6/20\n",
            " - 11s - loss: 0.8693 - val_loss: 0.9801\n",
            "Epoch 7/20\n",
            " - 10s - loss: 0.6613 - val_loss: 0.8878\n",
            "Epoch 8/20\n",
            " - 11s - loss: 0.5026 - val_loss: 0.8357\n",
            "Epoch 9/20\n",
            " - 11s - loss: 0.3785 - val_loss: 0.7880\n",
            "Epoch 10/20\n",
            " - 11s - loss: 0.2795 - val_loss: 0.7818\n",
            "Epoch 11/20\n",
            " - 10s - loss: 0.2011 - val_loss: 0.7657\n",
            "Epoch 12/20\n",
            " - 11s - loss: 0.1397 - val_loss: 0.7970\n",
            "Epoch 13/20\n",
            " - 11s - loss: 0.0950 - val_loss: 0.7773\n",
            "Epoch 14/20\n",
            " - 11s - loss: 0.0648 - val_loss: 0.7837\n",
            "Epoch 15/20\n",
            " - 11s - loss: 0.0443 - val_loss: 0.7779\n",
            "Epoch 16/20\n",
            " - 11s - loss: 0.0308 - val_loss: 0.8006\n",
            "Epoch 17/20\n",
            " - 11s - loss: 0.0227 - val_loss: 0.8036\n",
            "Epoch 18/20\n",
            " - 11s - loss: 0.0168 - val_loss: 0.8143\n",
            "Epoch 19/20\n",
            " - 11s - loss: 0.0139 - val_loss: 0.8047\n",
            "Epoch 20/20\n",
            " - 11s - loss: 0.0103 - val_loss: 0.8735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FoLOOVLH8Qg",
        "colab_type": "text"
      },
      "source": [
        "#Plot loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fu6qxITIplR",
        "colab_type": "code",
        "outputId": "8470cea8-3021-4e60-d4f9-01c18ee6a5b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.legend(['validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr063A7xbNgAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fsSx4SPhK1b",
        "colab_type": "code",
        "outputId": "7e5deb59-9541-48d5-afe3-9384a4904d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(y[10])\n",
        "x1 = X1[10]\n",
        "x2 = X2[10].copy()\n",
        "tmp=''\n",
        "for w in y[10]:\n",
        "  if w in ixtoword:\n",
        "    if w == 'endseq': continue\n",
        "    tmp += ' ' + ixtoword[w]\n",
        "print(tmp)\n",
        "\n",
        "yhat = model.predict([x1[np.newaxis,...],x2[np.newaxis,...]], verbose=0)\n",
        "print(\"yhat\")\n",
        "print(np.shape(yhat))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 31. 221. 222.   7.  31. 223. 224.   9.   2.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.  31. 225.  72. 226. 227.   9.   2.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  62. 228.\n",
            "   7. 229.   9.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.  62. 230.  16.  62. 231. 232.   5.  31. 233. 234.\n",
            " 137.   2.   0.   0.   0.   0.   0.   0.   0.   0. 100.  80.  68.  93.\n",
            "  62. 152.   9.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.]\n",
            " the fish pond was the main attraction . endseq the flowers were also pretty . endseq this bug was interesting . endseq this women made this kid go to the plant garden , endseq time for fun on this ride . endseq\n",
            "yhat\n",
            "(1, 100, 26572)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Kz14jsimQV",
        "colab_type": "code",
        "outputId": "b550fdc7-d7f5-4654-c75d-88b82b8a2d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "yhat_s = yhat[0,:,:]\n",
        "print(yhat_s.shape)\n",
        "print(np.argmax(yhat_s, axis=-1))\n",
        "tmp=''\n",
        "for w in np.argmax(yhat_s, axis=-1):\n",
        "  if w in ixtoword:\n",
        "    if w == 'endseq': continue\n",
        "    tmp += ' ' + ixtoword[w]\n",
        "print(tmp)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 26572)\n",
            "[ 31 221 222   7  31 223 224   9   2   0   0   0   0   0   0   0   0   0\n",
            "   0   0  31 225  72 226 227   9   2   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0  62 228   7 229   9   2   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0  62 230  16  62 231 232   5  31 233 234 137   2\n",
            "   0   0   0   0   0   0   0   0 100  80  68  93  62 152   9   2   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n",
            " the fish pond was the main attraction . endseq the flowers were also pretty . endseq this bug was interesting . endseq this women made this kid go to the plant garden , endseq time for fun on this ride . endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR7eneAV-Fos",
        "colab_type": "text"
      },
      "source": [
        "#TRY validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfzGfkAc-Iub",
        "colab_type": "code",
        "outputId": "ce9d75b4-1f03-4ad5-8d80-53c795e8edb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import shift\n",
        "#X1: 1x5x2048 --images\n",
        "#X2: 1x5x20  --captions\n",
        "index_v = 15\n",
        "x1_v = X1[index_v]\n",
        "x2_v = X2[index_v].copy()\n",
        "x2_v_in = np.zeros_like(x2_v)\n",
        "x2_v_out = np.zeros_like(x2_v)\n",
        "for i in range(5):\n",
        "  x2_v_in[i][0] = wordtoix['startseq']\n",
        "  for j in range(MAX_SEQUENCE_LENGTH): \n",
        "    yhat = model.predict([x1_v[np.newaxis,...],x2_v_in[np.newaxis,...]], verbose=0)\n",
        "    yhat_s = yhat[0,:,:]\n",
        "    cur = np.argmax(yhat_s[j])\n",
        "    #if ixtoword[cur] == 'endseq': break\n",
        "    if j+1<MAX_SEQUENCE_LENGTH:\n",
        "      x2_v_in[i][j+1] = cur\n",
        "    x2_v_out[i][j] = cur\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(x2_v_in)\n",
        "print(x2_v_out)\n",
        "\n",
        "caps=''\n",
        "here = np.concatenate(x2_v_out).ravel()\n",
        "for w in here:\n",
        "  if w in ixtoword:\n",
        "    if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue\n",
        "    caps += \" \" + ixtoword[w]\n",
        "print(\"Captions:\")\n",
        "print(caps)\n",
        "\n",
        "print('Original X2')\n",
        "print(X2[index_v])\n",
        "or_f = np.concatenate(X2[index_v]).ravel()\n",
        "\n",
        "o=''\n",
        "for w in or_f:\n",
        "  if w in ixtoword:\n",
        "    if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue\n",
        "    o += \" \"+ ixtoword[w]\n",
        "print(\"Original\") #X3[1]\n",
        "print(o)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.\n",
            "    2.   2.   2.   2.   2.   2.]\n",
            " [  1. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863. 863.   0.]\n",
            " [  1. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863. 863.   0.]\n",
            " [  1. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863. 863.   0.]\n",
            " [  1. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863. 863.   0.]]\n",
            "[[  2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.   2.\n",
            "    2.   2.   2.   2.   2.   2.]\n",
            " [863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863.   0.   2.]\n",
            " [863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863.   0.   2.]\n",
            " [863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863.   0.   2.]\n",
            " [863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863. 863.\n",
            "  863. 863. 863. 863.   0.   2.]]\n",
            "Captions:\n",
            " [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male] [male]\n",
            "Original X2\n",
            "[[  1.  60. 110.  20.   4.  88.  31. 294.   9.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.]\n",
            " [  1. 153. 295.   7.  20. 296. 297.   9.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.]\n",
            " [  1. 153. 295. 159.  15. 298. 299. 300.   5. 301.   9.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.]\n",
            " [  1.  60. 302. 216. 108.  14.  72.  93.  55. 303.   9.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.]\n",
            " [  1.  60.  11.  20. 292. 292. 304. 166. 305.   9.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.]]\n",
            "Original\n",
            " we took a trip at the boardwalk . one man was a street performer . one man had even brought his parrot to perform . we noticed that many people were on their bikes . we saw a organization organization int he distance .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IWWkFQDqdij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save weights:\n",
        "model.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_all_weights_sure.h5')\n",
        "model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_all_stories_finale_pad_sure.h5')\n",
        "##model2.load_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiNcLTa2vorh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath_model_load = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_finale.h5'\n",
        "#sen3_layer\n",
        "# model_loaded = load_model(filepath_model_load, custom_objects={'sparse_cross_entropy': tf.nn.softmax_cross_entropy_with_logits})\n",
        "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "weights = model.get_weights()\n",
        "\n",
        "for name, weight in zip(names, weights):\n",
        "    print(name, weight)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}