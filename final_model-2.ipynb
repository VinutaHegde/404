{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lX-B3q67XANg",
        "outputId": "35fd637b-f75f-40f3-93ab-d9833e847e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "# from numpy import argmax\n",
        "# from pickle import load\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import load_model\n",
        "# # from nltk.translate.bleu_score import corpus_bleu\n",
        "import json\n",
        "# import random\n",
        "import csv\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#Wesam\n",
        "# SEED = 10\n",
        "IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
        "#IMAGE_EMBEDDING_VAL_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/validation/'\n",
        "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
        "GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.50d.txt'\n",
        "CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
        "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
        "ALL_CAPTIONS_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/all_captions.txt'\n",
        "COMPLETE_STORIES_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/complete_stories_all_splits.json'\n",
        "IMAGE_EMBEDDING_DIM = 2048\n",
        "\n",
        "#Vinuta\n",
        "SEED = 10\n",
        "#IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
        "NUM_IMAGE_EMBEDDING_CHUNKS = 10\n",
        "#GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 94\n",
        "WORD_EMBEDDING_DIM = 50\n",
        "#CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'\n",
        "SENTENCE_EMBEDDING_DIM = 512"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvM68FbJ7h6",
        "colab_type": "code",
        "outputId": "763fef8d-e8b6-4161-ae1a-ea0920bc8414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iuq6CmUtYsEE"
      },
      "source": [
        "#PreProcess Captions / Stories\n",
        "\n",
        "Either call this function or simply load preprocessed from a file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqQ7qPFcRDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to check if story ids are repeated or unique\n",
        "# story_list = list()\n",
        "# for key in list(all_captions_dict.keys()):\n",
        "#   story_list += list(all_captions_dict[key].keys())\n",
        "# from collections import Counter\n",
        "# print(len(story_dict))\n",
        "# d =  Counter(story_dict)\n",
        "# res = [k for k, v in d.items() if v > 1]\n",
        "# print(len(res)) ## gave zero .. so story ids are -in fact- unique\n",
        "\n",
        "\n",
        "#image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "\n",
        "def get_existing_stories(image_embeddings):\n",
        "  #load all_captions file\n",
        "  with open(ALL_CAPTIONS_FILE) as json_file:\n",
        "    all_captions_dict = json.load(json_file)\n",
        "\n",
        "  #Create a story dict (no album ids (already checked that story ids are unique))\n",
        "  story_dict = {}\n",
        "  for key in list(all_captions_dict.keys()):\n",
        "    story_dict.update(all_captions_dict[key])\n",
        "\n",
        "\n",
        "  # Create a Story dict where all images are available in the image_embeddings\n",
        "  existing_stories = {}\n",
        "  c=0\n",
        "  for key in list(story_dict.keys()):\n",
        "    lists = story_dict[key]\n",
        "    images = [item[0] for item in lists]\n",
        "    captions = ['startseq ' + item[1] + ' endseq' for item in lists]\n",
        "    if all(img in list(image_embeddings.keys()) for img in images):\n",
        "      existing_stories[key] = [images,captions]\n",
        "      c+=1\n",
        "      \n",
        "  print(\"Number of Stories Found: \")\n",
        "  print(c)\n",
        "\n",
        "  # Saving the complete existing story dict in a file\n",
        "  with open(COMPLETE_STORIES_FILE, 'w') as fp:\n",
        "      json.dump(existing_stories, fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEKE0VJHSsU",
        "colab_type": "text"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16o6JbtNXHVa",
        "colab": {}
      },
      "source": [
        " def vocab_fun(existing_stories_dict):\n",
        "  index_to_word = {}\n",
        "  word_to_index = {}\n",
        "  max_seq_len=0\n",
        "  all_words = {}\n",
        "  cap_list = list()\n",
        "  for story_id, lists in existing_stories_dict.items():\n",
        "    for cap in lists[1]:\n",
        "      if(len(cap.split())>max_seq_len):\n",
        "        max_seq_len = len(cap.split())\n",
        "      for word in cap.split():\n",
        "        all_words[word] = 1\n",
        "  all_vocab=[w for w in all_words]\n",
        "  index = 0\n",
        "  for word in all_vocab:\n",
        "      word_to_index[word] = index\n",
        "      index_to_word[index] = word\n",
        "      index += 1\n",
        "  MAX_SEQUENCE_LENGTH = max_seq_len\n",
        "  return (all_vocab, word_to_index, index_to_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl15hHkcYxYj"
      },
      "source": [
        "#Preprocess images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90V-9vXXYjgx",
        "colab": {}
      },
      "source": [
        "def Merge(dict1, dict2): \n",
        "    res = {**dict1, **dict2} \n",
        "    return res \n",
        "\n",
        "def getImageEmbedding(path):\n",
        "    image_embedding = {}\n",
        "    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):\n",
        "         file_name = path + 'cnn_group'+str(i+1)+'.json'\n",
        "         with open(file_name) as json_file:\n",
        "#    with open(file_name) as json_file:\n",
        "            print(file_name)\n",
        "            json_data = json.load(json_file)\n",
        "            json_data = json.loads(json_data)\n",
        "            image_embedding = Merge(image_embedding, json_data) \n",
        "            #image_embedding = json_data \n",
        "    return image_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKPEQrk-FBz",
        "colab_type": "text"
      },
      "source": [
        "#Load Stories (captions with corresponding Image ids)\n",
        "##Dict items as follows (per story)\n",
        "[ [img_id1, img_id2, img_id3, img_id4, img_id5] , [cap1, cap2, cap3, cap4, cap5] ]\n",
        "\n",
        "Each captions starts with **startseq** and ends with **endseq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsfYfloT9dz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_existing_stories_from_file():\n",
        "  with open(COMPLETE_STORIES_FILE, 'r') as fp:\n",
        "      existing_stories = json.load(fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dJwBxEGaYNy"
      },
      "source": [
        "#Use Prev to get captions / stories and images and pre_process them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XMqm0aZXaYgo",
        "outputId": "d9acae70-ceca-488c-ff0f-67eb77de7d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#for training\n",
        "image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "print(len(image_embd))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group1.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group2.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group3.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group4.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group5.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group6.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group7.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group8.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group9.json\n",
            "/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/cnn_group10.json\n",
            "58197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FZmz0inN6Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b757c5ce-467d-4d2b-e2c0-f2995a9b7606"
      },
      "source": [
        "#get existing_stories (either load from file or using a function , preferably load from file) -- uncomment one of the following 2 lines\n",
        "existing_stories = get_existing_stories_from_file()\n",
        "#existing_stories = get_existing_stories(image_embd)\n",
        "\n",
        "all_vocab, wordtoix, ixtoword=vocab_fun(existing_stories)\n",
        "print('Max Seq Len: %d' %MAX_SEQUENCE_LENGTH)\n",
        "vocab_size = len(all_vocab)\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Seq Len: 94\n",
            "Vocabulary Size: 26571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9U_jDKeohyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c11f3eb7-19ff-4929-b914-534a665fca45"
      },
      "source": [
        "print(existing_stories['2905'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['4562798695', '4563429868', '4562801065', '4563434166', '4563436748'], ['startseq my trip to location was amazing . endseq', 'startseq i saw some colorful people . endseq', 'startseq i even made some knew friends . endseq', 'startseq i visited a beautiful pagoda . endseq', 'startseq then i saw a ordinary bicycle . endseq']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8amlOYORaY6U"
      },
      "source": [
        "#Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0TgQ6BKaZSA",
        "outputId": "69aebf96-6394-4d4b-f669-58b0b7e454c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get matrxi embedding for glove\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr-rypCuIBVe",
        "colab_type": "text"
      },
      "source": [
        "#IMAGE EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QClEP4J8gpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_indices(image_embd):\n",
        "  index = 0\n",
        "  img_to_index = {}\n",
        "  index_to_img = {}\n",
        "  for img_id in image_embd:\n",
        "      img_to_index[img_id] = index\n",
        "      index_to_img[index] = img_id\n",
        "      index += 1\n",
        "  return img_to_index, index_to_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZWE5KGIFGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creat image to index and index to image\n",
        "imgtoix, ixtoimg = get_image_indices(image_embd)\n",
        "\n",
        "img_embedding_matrix = np.zeros((len(image_embd), IMAGE_EMBEDDING_DIM))\n",
        "for img, ix in imgtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = image_embd.get(img)\n",
        "    if embedding_vector is not None:\n",
        "        img_embedding_matrix[ix] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfq2bjxq_WW7",
        "colab_type": "text"
      },
      "source": [
        "#Input and output for the model\n",
        "###X1, X2, X3, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8tkr49aQ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def all_data(stories_dict, image_embd, wordtoix, max_length, num_of_stories):\n",
        "  X1, X2, X3, y = list(), list(), list(), list()\n",
        "  # sentence_encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\") #512\n",
        "  #to generate X1,X2,X3 and y. ////append-> have a list of image embeds , have a list for curr caption, have sentences embed for previous sentences, next word\n",
        "  #for each story:\n",
        "  for key, lists in stories_dict.items():\n",
        "    #break after retreiving num_of_stories\n",
        "    if num_of_stories <= 0:\n",
        "      break\n",
        "    num_of_stories -= 1\n",
        "    #if all image_ids exists in image_embd: --- done-- no need to check since checking is done during preprocessing\n",
        "    img_list=lists[0]\n",
        "    img_list_embed=[imgtoix[img_id] for img_id in img_list]#[image_embd[img_id] for img_id in img_list]\n",
        "    prev_list=lists[1].copy()\n",
        "    prev_list.pop()\n",
        "    # with tf.Session() as session:\n",
        "    #   session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    #   all_sen_embeddings = session.run(sentence_encoder(prev_list))\n",
        "    # all_sen_embeddings.tolist().insert(0,[0]*SENTECE_EMBEDDING_DIM) #for first sentence (empty vector)\n",
        "    prev_list.insert(0,'')\n",
        "\n",
        "    for counter in range(5):\n",
        "      img_seq = img_list_embed[:counter+1]\n",
        "      img_seq = pad_sequences([img_seq], maxlen=5)[0]\n",
        "      prev_sents = ' '.join(prev_list[:counter+1])\n",
        "      prev_embeddings = [wordtoix[word] for word in prev_sents.split() if word in wordtoix]\n",
        "      prev_embeddings = pad_sequences([prev_embeddings], maxlen=max_length*4)[0]\n",
        "      cap=lists[1][counter]\n",
        "\n",
        "      seq = [wordtoix[word] for word in cap.split() if word in wordtoix]\n",
        "\n",
        "      for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        X1.append(img_seq)\n",
        "        X2.append(in_seq)\n",
        "        X3.append(prev_embeddings)\n",
        "        y.append(out_seq)\n",
        "\n",
        "  return (array(X1), array(X2), array(X3), array(y))\n",
        "  #X1 img sequence ... list of list\n",
        "  #X2 current caption sequence ..\n",
        "  #X3 previous sentences' embeddings\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9shW1upf8KD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b1acfe8-9011-4ee9-8033-9091374c9bc8"
      },
      "source": [
        "# import time\n",
        "\n",
        "# start = time.time() #in seconds\n",
        "# X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 100)\n",
        "# end = time.time()\n",
        "# print(end - start)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.097018003463745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxGojbWbKsr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "85780c83-d8ad-4367-ac55-76f7ead57138"
      },
      "source": [
        "print(np.shape(X1))\n",
        "print(np.shape(X2))\n",
        "print(np.shape(X3))\n",
        "print(np.shape(y))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5996, 5)\n",
            "(5996, 94)\n",
            "(5996, 376)\n",
            "(5996, 26571)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88uWOMZ1dME-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with tf.Session() as session:\n",
        "#   session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#   prev_embeddings = session.run(sentence_encoder(['']))\n",
        "# print(prev_embeddings)\n",
        "# prev_sentences='this is some sentence'\n",
        "# print(sentence_encoder([prev_sentences]))\n",
        "# print(len(sentence_encoder(prev_sentences)))\n",
        "# prev_sentences='here is some sentence. and some other sentence, and many many more sentences. this is a long sentence embedding'\n",
        "# print(sentence_encoder(prev_sentences))\n",
        "# print(len(sentence_encoder(prev_sentences)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpo33OpBdU2L"
      },
      "source": [
        "#Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IaC1BgwxdZsM",
        "colab": {}
      },
      "source": [
        "# def build_model():\n",
        "#   inputs1 = Input(shape=(2048,),name='images')\n",
        "#   fe1 = Dropout(0.5)(inputs1)\n",
        "#   fe2 = Dense(256, activation='relu')(fe1)\n",
        "#   inputs2 = Input(shape=(max_length,),name='sequences')\n",
        "#   se1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
        "#   se2 = Dropout(0.5)(se1)\n",
        "#   se3 = GRU(256,recurrent_dropout=0.3)(se2)\n",
        "#   decoder1 = add([fe2, se3])\n",
        "#   decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "#   outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "#   model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "#   return model\n",
        "\n",
        "def build_model():\n",
        "  inputs1 = Input(shape=(5,),name='images')\n",
        "  img1 = Embedding(len(image_embd), IMAGE_EMBEDDING_DIM , mask_zero=True, weights=[img_embedding_matrix], trainable=False)(inputs1)\n",
        "  img2 = Dropout(0.5)(img1)\n",
        "  img3 = GRU(256, recurrent_dropout=0.3)(img2)\n",
        "\n",
        "  inputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,),name='sequences')\n",
        "  seq1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs2)\n",
        "  seq2 = Dropout(0.5)(seq1)\n",
        "  seq3 = GRU(256,recurrent_dropout=0.3)(seq2)\n",
        "  \n",
        "  inputs3 = Input(shape=(MAX_SEQUENCE_LENGTH*4,),name='prev_sentences')\n",
        "  sen1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs3)\n",
        "  sen2 = GRU(256,recurrent_dropout=0.3)(sen1)\n",
        "\n",
        "\n",
        "  decoder1 = add([img3, seq3, sen2])\n",
        "  decoder2 = Dense(256, activation=None)(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCFNBmPWdZ71"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9RFuY7wfx4t",
        "outputId": "18b7f5b8-a913-40ba-c3e5-ff21161418f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "model=build_model()\n",
        "model.summary()\n",
        "\n",
        "#optz = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "#optz = Adams(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "optz = Adam(lr=0.01, decay=0.1)\n",
        "#optz = RMSprop(lr=0.001, rho=0.9)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optz)#'adam')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "images (InputLayer)             (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequences (InputLayer)          (None, 94)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 5, 2048)      119187456   images[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 94, 50)       1328550     sequences[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "prev_sentences (InputLayer)     (None, 376)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 5, 2048)      0           embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 94, 50)       0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 376, 50)      1328550     prev_sentences[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (None, 256)          1770240     dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_9 (GRU)                     (None, 256)          235776      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_10 (GRU)                    (None, 256)          235776      embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 256)          0           gru_8[0][0]                      \n",
            "                                                                 gru_9[0][0]                      \n",
            "                                                                 gru_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 26571)        6828747     dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 130,980,887\n",
            "Trainable params: 9,136,331\n",
            "Non-trainable params: 121,844,556\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR4FhSY2cW1C",
        "colab_type": "code",
        "outputId": "f7604f30-8653-4457-82db-34fe7bebdac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 100)\n",
        "filepath_checkpoint = \"/content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=False, mode='min')\n",
        "history = model.fit([X1,X2,X3], y, epochs=200, verbose=1, batch_size=32, callbacks=[checkpoint], validation_split=0.1, shuffle=True, workers=10, use_multiprocessing=True)\n",
        "model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split_finale.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 5396 samples, validate on 600 samples\n",
            "Epoch 1/200\n",
            "5396/5396 [==============================] - 154s 29ms/step - loss: 6.2891 - val_loss: 5.8410\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep001-loss6.289.h5\n",
            "Epoch 2/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 5.0160 - val_loss: 5.8308\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep002-loss5.016.h5\n",
            "Epoch 3/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 4.7322 - val_loss: 5.8625\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep003-loss4.732.h5\n",
            "Epoch 4/200\n",
            "5396/5396 [==============================] - 145s 27ms/step - loss: 4.6082 - val_loss: 5.8571\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep004-loss4.608.h5\n",
            "Epoch 5/200\n",
            "5396/5396 [==============================] - 147s 27ms/step - loss: 4.4953 - val_loss: 5.9143\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep005-loss4.495.h5\n",
            "Epoch 6/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 4.4223 - val_loss: 5.9327\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep006-loss4.422.h5\n",
            "Epoch 7/200\n",
            "5396/5396 [==============================] - 144s 27ms/step - loss: 4.3626 - val_loss: 5.9651\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep007-loss4.363.h5\n",
            "Epoch 8/200\n",
            "5396/5396 [==============================] - 143s 26ms/step - loss: 4.3077 - val_loss: 5.9561\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep008-loss4.308.h5\n",
            "Epoch 9/200\n",
            "5396/5396 [==============================] - 144s 27ms/step - loss: 4.2550 - val_loss: 5.9724\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep009-loss4.255.h5\n",
            "Epoch 10/200\n",
            "5396/5396 [==============================] - 143s 26ms/step - loss: 4.2208 - val_loss: 5.9935\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep010-loss4.221.h5\n",
            "Epoch 11/200\n",
            "5396/5396 [==============================] - 145s 27ms/step - loss: 4.1928 - val_loss: 5.9858\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep011-loss4.193.h5\n",
            "Epoch 12/200\n",
            "5396/5396 [==============================] - 143s 27ms/step - loss: 4.1596 - val_loss: 6.0044\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep012-loss4.160.h5\n",
            "Epoch 13/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 4.1169 - val_loss: 6.0246\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep013-loss4.117.h5\n",
            "Epoch 14/200\n",
            "5396/5396 [==============================] - 145s 27ms/step - loss: 4.1010 - val_loss: 6.0180\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep014-loss4.101.h5\n",
            "Epoch 15/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 4.0712 - val_loss: 6.0235\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep015-loss4.071.h5\n",
            "Epoch 16/200\n",
            "5396/5396 [==============================] - 144s 27ms/step - loss: 4.0525 - val_loss: 6.0349\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep016-loss4.053.h5\n",
            "Epoch 17/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 4.0430 - val_loss: 6.0371\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep017-loss4.043.h5\n",
            "Epoch 18/200\n",
            "5396/5396 [==============================] - 144s 27ms/step - loss: 4.0133 - val_loss: 6.0581\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep018-loss4.013.h5\n",
            "Epoch 19/200\n",
            "5396/5396 [==============================] - 151s 28ms/step - loss: 4.0033 - val_loss: 6.0564\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep019-loss4.003.h5\n",
            "Epoch 20/200\n",
            "5396/5396 [==============================] - 146s 27ms/step - loss: 3.9873 - val_loss: 6.0559\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep020-loss3.987.h5\n",
            "Epoch 21/200\n",
            "5396/5396 [==============================] - 150s 28ms/step - loss: 3.9819 - val_loss: 6.0449\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep021-loss3.982.h5\n",
            "Epoch 22/200\n",
            "5396/5396 [==============================] - 147s 27ms/step - loss: 3.9612 - val_loss: 6.0549\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep022-loss3.961.h5\n",
            "Epoch 23/200\n",
            "5396/5396 [==============================] - 148s 27ms/step - loss: 3.9331 - val_loss: 6.0549\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep023-loss3.933.h5\n",
            "Epoch 24/200\n",
            "5396/5396 [==============================] - 145s 27ms/step - loss: 3.9204 - val_loss: 6.0608\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/Colab_Notebooks/DL_data/fit_adam_50d_1_split-ep024-loss3.920.h5\n",
            "Epoch 25/200\n",
            "1568/5396 [=======>......................] - ETA: 1:41 - loss: 3.9312"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FoLOOVLH8Qg",
        "colab_type": "text"
      },
      "source": [
        "#Plot loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fu6qxITIplR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbI4DkqkIF01",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing Needed for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QPwcGZLhbMz",
        "colab": {}
      },
      "source": [
        "import os.path as osp\n",
        "import os\n",
        "from pprint import pprint\n",
        "from skimage.transform import rescale, resize\n",
        "from skimage import data, color, io\n",
        "import skimage\n",
        "import PIL\n",
        "import scipy\n",
        "import json\n",
        "import os.path\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications.xception import preprocess_input\n",
        "from keras.applications.xception import Xception\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "def load_image(image_path,target_size):\n",
        "    img = skimage.io.imread(image_path)\n",
        "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
        "    return image_resized\n",
        "\n",
        "def load_cnn_model():\n",
        "    model = Xception()\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    return model\n",
        "\n",
        "def extract_features_from_images(image_path):\n",
        "    model = load_cnn_model()\n",
        "    if path.exists(image_path):\n",
        "        print(image_path)\n",
        "        image = load_image(image_path, target_size=(299, 299))\n",
        "        if image.shape == (299, 299, 3):\n",
        "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "            image = preprocess_input(image)\n",
        "            feature = model.predict(image, verbose=0)\n",
        "            print(feature)\n",
        "            return feature\n",
        "\n",
        "def generate_desc(model, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[word] for word in in_text.split() if word in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = ixtoword[yhat]\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORIOVS1IMaB",
        "colab_type": "text"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2aIfCVPhBAN",
        "outputId": "9d1d5213-6edf-4ea6-9459-b0529d372ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg'#'/Users/vinutahegde/Documents/Personal/IMG_3501.JPG'\n",
        "feature=extract_features_from_images(image_path)\n",
        "print(generate_desc(model, feature, max_length))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg\n",
            "[[0.         0.         0.14329363 ... 0.13233422 0.         0.        ]]\n",
            "startseq and they were on the shore as well . endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef857nMlhfGJ",
        "outputId": "41deee59-7844-46dc-f5cc-8d0ff3224612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg'\n",
        "feature=extract_features_from_images(image_path)\n",
        "print(generate_desc(model, feature, max_length))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg\n",
            "[[0.         0.         0.16217265 ... 0.11656351 0.         0.        ]]\n",
            "startseq watching the boats come in endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-THdS55IplV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}