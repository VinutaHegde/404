{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "lX-B3q67XANg",
    "outputId": "d7900194-16f4-493a-f572-4a3ce258c4f6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from numpy import argmax\n",
    "# from pickle import load\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import load_model\n",
    "# # from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "# import random\n",
    "import csv\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# SEED = 10\n",
    "# #IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
    "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
    "# NUM_IMAGE_EMBEDDING_CHUNKS = 1\n",
    "# GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'\n",
    "# MAX_SEQUENCE_LENGTH = 92\n",
    "# WORD_EMBEDDING_DIM = 300\n",
    "# CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
    "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
    "\n",
    "\n",
    "SEED = 10\n",
    "IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
    "NUM_IMAGE_EMBEDDING_CHUNKS = 1\n",
    "GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.50d.txt' #'glove.6B.300d.txt'\n",
    "MAX_SEQUENCE_LENGTH = 92\n",
    "WORD_EMBEDDING_DIM = 50#300\n",
    "CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuq6CmUtYsEE"
   },
   "source": [
    "#PreProcess Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16o6JbtNXHVa"
   },
   "outputs": [],
   "source": [
    "def getCaptions(id_list):\n",
    "    caption_dict = {}\n",
    "    with open(CAPTION_FILE_NAME) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if row[1] in id_list:\n",
    "                caption_dict[row[1]] = ['startseq ' + row[2] + ' endseq']\n",
    "                #caption_dict[row[1]] = 'startseq ' + row[2] + ' endseq'\n",
    "    return caption_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brA9HhUmYZQ4"
   },
   "outputs": [],
   "source": [
    "def vocab_fun(captions):\n",
    "  index_to_word = {}\n",
    "  word_to_index = {}\n",
    "  all_words = {}\n",
    "  for img_id, cap in captions.items():\n",
    "      for c in cap:\n",
    "          for word in c.split():\n",
    "            all_words[word] = 1\n",
    "  all_vocab=[w for w in all_words]\n",
    "  index = 0\n",
    "  for word in all_vocab:\n",
    "      word_to_index[word] = index\n",
    "      index_to_word[index] = word\n",
    "      index += 1\n",
    "  return (all_vocab, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl15hHkcYxYj"
   },
   "source": [
    "#Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90V-9vXXYjgx"
   },
   "outputs": [],
   "source": [
    "def Merge(dict1, dict2): \n",
    "    res = {**dict1, **dict2} \n",
    "    return res \n",
    "\n",
    "def getImageEmbedding():\n",
    "    image_embedding = {}\n",
    "    for i in range (NUM_IMAGE_EMBEDDING_CHUNKS):\n",
    "        file_name = IMAGE_EMBEDDING_DIR + 'cnn_group'+str(i+1)+'.json'\n",
    "        with open(file_name) as json_file:\n",
    "#     with open('json_1000_images') as json_file:\n",
    "#             print(file_name)\n",
    "            json_data = json.load(json_file)\n",
    "            json_data = json.loads(json_data)\n",
    "            image_embedding = Merge(image_embedding, json_data) \n",
    "            image_embedding = json_data \n",
    "            return image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = getImageEmbedding()\n",
    "# yy = {}\n",
    "# for x in list(reversed(list(xx)))[0:1000]:\n",
    "#     yy[x] = xx[x]\n",
    "    \n",
    "# j = json.dumps(yy)\n",
    "# with open(\"json_1000_images\", 'w') as outfile:\n",
    "#     json.dump(j, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZtfju_3Z_6E"
   },
   "source": [
    "#for fit_generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWul5J87aDpv"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch): #descriptions are captions\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split() if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photos[key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dJwBxEGaYNy"
   },
   "source": [
    "#Use Prev to get captions and images and pre_process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XMqm0aZXaYgo",
    "outputId": "1e17e5b1-94a7-4ae8-c24d-3bd3370f7253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5803\n",
      "Vocabulary Size: 5605\n"
     ]
    }
   ],
   "source": [
    "image_embd =  getImageEmbedding()\n",
    "print(len(image_embd))\n",
    "# for val in image_embd.values():\n",
    "#   print(np.shape(val))\n",
    "#   break\n",
    "image_ids = list(image_embd.keys())\n",
    "caption_map = getCaptions(image_ids)\n",
    "all_vocab, wordtoix, ixtoword=vocab_fun(caption_map)\n",
    "#tokenizer = create_tokenizer(caption_map)\n",
    "\n",
    "vocab_size = len(all_vocab)#len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = 94 #max_length(caption_map)\n",
    "#X1train, X2train, ytrain = create_sequences(tokenizer, max_length, caption_map, image_embd, vocab_size)\n",
    "#X1test, X2test, ytest = create_sequences(tokenizer, max_length, caption_map, image_embd, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8amlOYORaY6U"
   },
   "source": [
    "#Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d0TgQ6BKaZSA",
    "outputId": "d80ff5df-99b1-4922-de8b-5701bb995289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#get matrxi embedding for glove\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_dim = 50#300\n",
    "\n",
    "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpo33OpBdU2L"
   },
   "source": [
    "#Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaC1BgwxdZsM"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  inputs1 = Input(shape=(2048,),name='images')\n",
    "  fe1 = Dropout(0.5)(inputs1)\n",
    "  fe2 = Dense(256, activation='relu')(fe1)\n",
    "  inputs2 = Input(shape=(max_length,),name='sequences')\n",
    "  se1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
    "  se2 = Dropout(0.7)(se1)\n",
    "#   se3 = LSTM(256)(se2)\n",
    "  se3 = GRU(256)(se2)\n",
    "  decoder1 = add([fe2, se3])\n",
    "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCFNBmPWdZ71"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "Z9RFuY7wfx4t",
    "outputId": "9c32d320-f0d7-4d50-9feb-dbcc641afb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequences (InputLayer)          (None, 94)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "images (InputLayer)             (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 94, 50)       280250      sequences[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 2048)         0           images[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 94, 50)       0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          524544      dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (None, 256)          235776      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_186 (Add)                   (None, 256)          0           dense_16[0][0]                   \n",
      "                                                                 gru_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          65792       add_186[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 5605)         1440485     dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,546,847\n",
      "Trainable params: 2,546,847\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=build_model()\n",
    "model.summary()\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# optz = SGD(lr=0.005, momentum=0.0, decay=0.1, nesterov=False)\n",
    "optz = Adam(lr=0.01, decay=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optz)#'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yD99jV4BdVBf",
    "outputId": "78c0fcd5-b739-4c43-9c76-c396f071403c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 163/5803 [..............................] - ETA: 12:03:23 - loss: 5.7916"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4a2792f35e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordtoix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'late_late_night_model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "steps = len(caption_map)\n",
    "for i in range(1):\n",
    "    generator = data_generator(caption_map, image_embd, wordtoix, max_length, batch_size)\n",
    "    history = model.fit_generator(generator, epochs=100, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('late_late_night_model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5Qc5X3m8e+vqm9zn9HMIAlJIGEwF7MgjIwh2F5OHHsB28EJ2CYbnMSbtZKsfYLPcbyL48ROfPbibHaTXeILxoFj7OVgJ2BiksXrmNj4cmwwQhZY4ipAWCN0GUlzn+mZvvz2j6oZ9dzEzEillqaezzlzpruquup91aN++n3fqrfM3RERkfQK6l0AERGpLwWBiEjKKQhERFJOQSAiknIKAhGRlFMQiIiknIJAZIHM7Mtm9p8XuO0uM/uVY92PyImgIBARSTkFgYhIyikIZFmJu2Q+ZmZPmtmImd1hZivN7FtmNmRmD5lZR832v2pmO8ys38weNrPza9ZdYmZb49d9HSjMONY7zWxb/Nofm9lFSyzzB81sp5kdNrMHzOz0eLmZ2V+b2QEzGzSzn5vZhfG6a83sqbhse8zsj5b0DyaCgkCWp+uBtwGvBd4FfAv4Y6Cb6G/+DwHM7LXAPcBH4nUPAv9oZjkzywH/AHwVWAH8fbxf4tdeAtwJ/B7QCXwReMDM8ospqJn9MvDfgPcCq4GXga/Fq98OvCWuR1u8zaF43R3A77l7C3Ah8N3FHFekloJAlqO/cff97r4H+CHwqLv/zN2LwP3AJfF27wP+r7t/x91LwP8AGoBfAi4HssD/cveSu98LPFZzjM3AF939UXevuPtdwHj8usX4TeBOd9/q7uPAx4ErzGw9UAJagPMAc/en3X1v/LoScIGZtbp7n7tvXeRxRaYoCGQ52l/zeGyO583x49OJvoED4O5VYDewJl63x6fPyvhyzeMzgY/G3UL9ZtYPrItftxgzyzBM9K1/jbt/F/gs8DnggJndbmat8abXA9cCL5vZ983sikUeV2SKgkDS7BWiD3Qg6pMn+jDfA+wF1sTLJp1R83g38F/cvb3mp9Hd7znGMjQRdTXtAXD3W939UuACoi6ij8XLH3P364DTiLqw/m6RxxWZoiCQNPs74B1m9lYzywIfJere+THwE6AM/KGZZc3s14HLal77JeD3zeyN8aBuk5m9w8xaFlmGe4APmNnGeHzhvxJ1Ze0yszfE+88CI0ARqMZjGL9pZm1xl9YgUD2GfwdJOQWBpJa7PwvcBPwNcJBoYPld7j7h7hPArwO/AxwmGk/4Rs1rtwAfJOq66QN2xtsutgwPAX8K3EfUCnkNcGO8upUocPqIuo8OAX8Zr3s/sMvMBoHfJxprEFkS041pRETSTS0CEZGUUxCIiKScgkBEJOUUBCIiKZepdwEWq6ury9evX1/vYoiInFIef/zxg+7ePde6Uy4I1q9fz5YtW+pdDBGRU4qZvTzfOnUNiYiknIJARCTlFAQiIil3yo0RzKVUKtHT00OxWKx3URJXKBRYu3Yt2Wy23kURkWViWQRBT08PLS0trF+/numTRS4v7s6hQ4fo6elhw4YN9S6OiCwTy6JrqFgs0tnZuaxDAMDM6OzsTEXLR0ROnGURBMCyD4FJaamniJw4yyYIXk2xVGHfQJFSRdO2i4jUSlUQHBgqUqke/2m3+/v7+fznP7/o11177bX09/cf9/KIiCxGaoJgskclidsvzBcE5XL5qK978MEHaW9vP/4FEhFZhGVx1tDCTPatH/8kuOWWW3jhhRfYuHEj2WyWQqFAR0cHzzzzDM899xzvfve72b17N8VikZtvvpnNmzcDR6bLGB4e5pprruFNb3oTP/7xj1mzZg3f/OY3aWhoOO5lFRGZadkFwZ//4w6eemVw1vJK1SmWKjTkQoJFDrhecHorn3rX6+Zd/5nPfIbt27ezbds2Hn74Yd7xjnewffv2qVM877zzTlasWMHY2BhveMMbuP766+ns7Jy2j+eff5577rmHL33pS7z3ve/lvvvu46abblpUOUVElmLZBcHJ4LLLLpt2nv+tt97K/fffD8Du3bt5/vnnZwXBhg0b2LhxIwCXXnopu3btOmHlFZF0W3ZBMN8396FiiZcOjvCa7maa8slWu6mpaerxww8/zEMPPcRPfvITGhsbueqqq+a8DiCfz089DsOQsbGxRMsoIjIpNYPFSWppaWFoaGjOdQMDA3R0dNDY2MgzzzzDI488coJLJyJydMuuRTCf5IaKobOzkyuvvJILL7yQhoYGVq5cObXu6quv5rbbbuP888/n3HPP5fLLL0+gBCIiS2eexPmUCdq0aZPPvDHN008/zfnnn3/U1w0Xy7x4cJizuppoLpzaE7YtpL4iIrXM7HF33zTXusS6hsysYGY/NbMnzGyHmf35HNvkzezrZrbTzB41s/XJlSf6fWrFnohI8pIcIxgHftndLwY2Aleb2cx+kd8F+tz9bOCvgb9IsDwiIjKHxILAI8Px02z8M/ML+XXAXfHje4G32hJnVVtoF9ep3iI41bryROTkl+hZQ2YWmtk24ADwHXd/dMYma4DdAO5eBgaAzhnbYGabzWyLmW3p7e2ddZxCocChQ4eO+iFpSY4WnyCT9yMoFAr1LoqILCOJnjXk7hVgo5m1A/eb2YXuvn0J+7kduB2iweKZ69euXUtPTw9zhcSkiXKVA0PjlA/naMiGiy3CSWPyDmUiIsfLCTl91N37zex7wNVAbRDsAdYBPWaWAdqAQ4vdfzabfdU7du14ZYAP3v0jbrvpUq4+f9ViDyEismwledZQd9wSwMwagLcBz8zY7AHgt+PHNwDf9YQ6wcMg6huqqo9dRGSaJFsEq4G7zCwkCpy/c/d/MrNPA1vc/QHgDuCrZrYTOAzcmFRhQlMQiIjMJbEgcPcngUvmWP7JmsdF4D1JlaHW5MlISdyYRkTkVJaauYbUNSQiMrf0BMFUi6DOBREROcmkJggmryNQi0BEZLrUBMFU15DGCEREpkldEFTUIhARmSY1QRBMnT5a54KIiJxkUhQE0W91DYmITJeaIJjqGlIQiIhMk5ogCHQdgYjInNITBJpiQkRkTqkJAl1QJiIyt9QEQRDXVC0CEZHp0hMEpgvKRETmkpogmOoaUotARGSa1ATBkbOG6lwQEZGTTGqCAKKLytQ1JCIyXaqCIAxMXUMiIjOkKggCM501JCIyQ/qCQF1DIiLTpCoIwsB0QZmIyAypCoLAdEGZiMhM6QqCQGMEIiIzpSoIQjNNQy0iMkNiQWBm68zse2b2lJntMLOb59jmKjMbMLNt8c8nkyoPqEUgIjKXTIL7LgMfdfetZtYCPG5m33H3p2Zs90N3f2eC5ZgSmlHVYLGIyDSJtQjcfa+7b40fDwFPA2uSOt5CBKa5hkREZjohYwRmth64BHh0jtVXmNkTZvYtM3vdPK/fbGZbzGxLb2/vkssRBLqOQERkpsSDwMyagfuAj7j74IzVW4Ez3f1i4G+Af5hrH+5+u7tvcvdN3d3dSy5LqDECEZFZEg0CM8sShcDd7v6NmevdfdDdh+PHDwJZM+tKqjyBGRXlgIjINEmeNWTAHcDT7v5X82yzKt4OM7ssLs+hpMqk2UdFRGZL8qyhK4H3Az83s23xsj8GzgBw99uAG4A/MLMyMAbc6J5c3426hkREZkssCNz9R4C9yjafBT6bVBlmCnRBmYjILKm6sljTUIuIzJaqIIi6hupdChGRk0uqgiAw1DUkIjJDuoJAg8UiIrOkKghCjRGIiMySqiDQWUMiIrOlKwgCNPuoiMgMqQoCXVAmIjJbqoIgmmtIQSAiUit1QaC5hkREpktVEISBWgQiIjOlKgii2UfrXQoRkZNLyoJAg8UiIjOlKgjCQNcRiIjMlKog0BQTIiKzpSsITLOPiojMlKogCDX7qIjILKkKAnUNiYjMlq4g0AVlIiKzpCoIQk0xISIyS6qCINCtKkVEZklXEBjqGhIRmSGxIDCzdWb2PTN7ysx2mNnNc2xjZnarme00syfN7PVJlQc015CIyFwyCe67DHzU3beaWQvwuJl9x92fqtnmGuCc+OeNwBfi34nQYLGIyGyJtQjcfa+7b40fDwFPA2tmbHYd8BWPPAK0m9nqpMqkC8pERGY7IWMEZrYeuAR4dMaqNcDumuc9zA4LzGyzmW0xsy29vb1LLkcY6IIyEZGZEg8CM2sG7gM+4u6DS9mHu9/u7pvcfVN3d/eSy6ILykREZks0CMwsSxQCd7v7N+bYZA+wrub52nhZIjQNtYjIbEmeNWTAHcDT7v5X82z2APBb8dlDlwMD7r43qTKFpmmoRURmSvKsoSuB9wM/N7Nt8bI/Bs4AcPfbgAeBa4GdwCjwgQTLowvKRETmkFgQuPuPAHuVbRz4UFJlmCmIS1OtOkFw1KKJiKRGqq4sDi368NdFZSIiR6QqCCZbARonEBE5Il1BELcI1CAQETkiVUEQxrVV15CIyBGpCoLJFoG6hkREjkhVEITBZNeQgkBEZFKqgkAtAhGR2dIVBIFOHxURmSlVQRDqrCERkVlSFQSTFxOra0hE5Ih0BYEuKBMRmSVVQaCuIRGR2RYUBGZ2s5m1xtNF32FmW83s7UkX7ngLdEGZiMgsC20R/Lv47mJvBzqIppf+TGKlSohOHxURmW2hQTA5Z/O1wFfdfQevMsX0yUgXlImIzLbQIHjczP6ZKAi+bWYtQDW5YiUj0DTUIiKzLPTGNL8LbARedPdRM1tBwncTS4K6hkREZltoi+AK4Fl37zezm4A/AQaSK1YyjnQN1bkgIiInkYUGwReAUTO7GPgo8ALwlcRKlRBdUCYiMttCg6Ac31/4OuCz7v45oCW5YiVDcw2JiMy20DGCITP7ONFpo282swDIJlesZExeUFZVi0BEZMpCWwTvA8aJrifYB6wF/jKxUiVkcrBYOSAicsSCgiD+8L8baDOzdwJFdz/1xggmryxWEoiITFnoFBPvBX4KvAd4L/Comd3wKq+508wOmNn2edZfZWYDZrYt/vnkYgu/WFNdQxojEBGZstAxgk8Ab3D3AwBm1g08BNx7lNd8GfgsRz+76Ifu/s4FluGYTQ4WKwhERI5Y6BhBMBkCsUOv9lp3/wFweKkFS4IuKBMRmW2hLYL/Z2bfBu6Jn78PePA4HP8KM3sCeAX4o3gOo1nMbDOwGeCMM85Y8sFCtQhERGZZUBC4+8fM7HrgynjR7e5+/zEeeytwprsPm9m1wD8A58xz/NuB2wE2bdq05E/xI6ePLnUPIiLLz0JbBLj7fcB9x+vA8bTWk48fNLPPm1mXux88XseYySavLFaLQERkylGDwMyGgLk+NQ1wd29d6oHNbBWw393dzC4jGnM4tNT9LcRU15DGCEREphw1CNx9ydNImNk9wFVAl5n1AJ8ivhrZ3W8DbgD+wMzKwBhwoyd8o4AjYwRJHkVE5NSy4K6hxXL333iV9Z8lOr30hAnUNSQiMkuqbl4faK4hEZFZUhUEOn1URGS2VAWBLigTEZktXUGgFoGIyCypCoJsHARltQhERKakKwjCqLqlsi4tFhGZlKogyIRRi6BUUYtARGRSqoJgskUwUVGLQERkUiqDoKwWgYjIlFQFQRgYYWCU1CIQEZmSqiAAyIYKAhGRWikMgkBjBCIiNVIXBLkwUItARKRG6oIgExqlsgaLRUQmpS4IsmoRiIhMk7ogyIUBJU0xISIyJXVBkA0DTTEhIlIjfUGQ0emjIiK10hcEOn1URGSaVAaBWgQiIkekMAhMs4+KiNRIYRCoRSAiUiuxIDCzO83sgJltn2e9mdmtZrbTzJ40s9cnVZZaURCoRSAiMinJFsGXgauPsv4a4Jz4ZzPwhQTLMkVTTIiITJdYELj7D4DDR9nkOuArHnkEaDez1UmVZ5JmHxURma6eYwRrgN01z3viZbOY2WYz22JmW3p7e4/poLqgTERkulNisNjdb3f3Te6+qbu7+5j2lQkDJjRGICIypZ5BsAdYV/N8bbwsUTl1DYmITFPPIHgA+K347KHLgQF335v0QXX6qIjIdJmkdmxm9wBXAV1m1gN8CsgCuPttwIPAtcBOYBT4QFJlqZXNBLp5vYhIjcSCwN1/41XWO/ChpI4/n8m5htwdMzvRhxcROemcEoPFx1MujD78y7ongYgIkMIgyIZRlTVOICISSV0QZCaDQPctFhEBUhgEk11DuieBiEgkdUGgriERkelSGwQ6hVREJJK+IMhEVVbXkIhIJHVBMDlGoK4hEZFI6oJAYwQiItOlLggyCgIRkWlSFwTZydNHdR2BiAiQwiDIqUUgIjJN6oJg6vTRqoJARARSHATqGhIRiaQuCHIZnT4qIlIrdUGQCTRGICJSK3VBMHllsYJARCSSviCYmn1UYwQiIpDCIJg6fbSsFoGICKQwCHT6qIjIdKkNgpK6hkREgFQGweQUE2oRiIhACoPAzMgEprOGRERiiQaBmV1tZs+a2U4zu2WO9b9jZr1mti3++fdJlmdSNgwUBCIisUxSOzazEPgc8DagB3jMzB5w96dmbPp1d/9wUuWYSzY0jRGIiMSSbBFcBux09xfdfQL4GnBdgsdbsFwm0K0qRURiSQbBGmB3zfOeeNlM15vZk2Z2r5mtm2tHZrbZzLaY2Zbe3t5jLlg+E1IsVY55PyIiy0G9B4v/EVjv7hcB3wHummsjd7/d3Te5+6bu7u5jPmhrQ5bBsdIx70dEZDlIMgj2ALXf8NfGy6a4+yF3H4+f/i1waYLlmdLRmKVvVEEgIgLJBsFjwDlmtsHMcsCNwAO1G5jZ6pqnvwo8nWB5pnQ05ugbnTgRhxIROekldtaQu5fN7MPAt4EQuNPdd5jZp4Et7v4A8Idm9qtAGTgM/E5S5anV3pilXy0CEREgwSAAcPcHgQdnLPtkzeOPAx9PsgxziYJggmrVCQI70YcXETmp1HuwuC46GnNUHYbGy/UuiohI3aUyCNobcwD0a5xARCSdQdDRmAXQmUMiIqQ0CCZbBDpzSEQktUEQtQjUNSQiktIg6JgaI1DXkIhIKoOgrSGLmcYIREQgpUEQBkZrIauuIRERUhoEoPmGREQmpTYI2hpzahGIiJDiIIhaBAoCEZHUBsFZXc08v39YN6gRkdRLbRC8+bVdjJerPLbrcL2LIiJSV6kNgjduWEEuDPjh8wfrXRQRkbpKbRA05jJsWt/BD5479nsgi4icylIbBAC/fN5pPLNviPt/1lPvooiI1E2qg+D9V5zJFWd18rG/f5LPfW8nA7quQERSyNy93mVYlE2bNvmWLVuO2/6GiiX+471P8q3t+wA4d2ULrzmtiZ6+MS5bv4JzVjYTmDFWqjAyXqFYqtDWkKWrJc+KeM6ifYNFqlXntNY861Y00pANOTg8Tt9oiWxoFLIhDZM/uZBCJiSfDRgcKzFernJaa558JgTA3ekbLVGuVOluyQNgNv0uahPlKrlMqjNcRBbJzB53901zrUv0VpWngpZCli/cdCmPv3yYR148zPef6+WJ3QOc3l7grp/solQ5MUHZ1pClkA3oH43CAaCQDZgoVzmtpUDFnWKpQiYw+kZLnLeqhTAwsmHA6rYCw+NlhsfLnN3dTGdznmKpQrlapTGXYWS8THMhQ2shS1MupLUhy84Dw5ze3sDoRJlSxTl3ZQvj5SqjE2XWdDTQWshyYKjI4FiZruY8q9sLlCtRGZoLGZrzGUqVKjteGQTgvFUtU+H0zL4hGrIhnU05BsZKrGwtTIWduzNRqZILg1kBJyL1kfoWwdGMlyscGBwHoDEX0pjLkMtE3+QPjYxzeKREuVrl9LYGwsDYN1hk9+FRiqXo23xHY5Zy1RkrVRgvVRgrVRibqDJWiloWLYUM+UzAgcFxeofHKZYqtDfmWNVaIDDo6Rsjnw3YO1AkGwQ05EImKlVWNObYtrufTGgUSxV6h8ZpbchSyIQ8s2+QkYkKDdmQwGB0okJTPsNwscxEpTpVNzM40W99V3OO8XKVoWKZTGCcfVoz5arTnM/QmAvpHy3R0ZTl0PAEuUzAeKnKRKXK+atbyIUBtcXNBAHtjVn2DRZpzIasbm+gKRdyYGicg8PjtDdkGR6v4DgXrG6lkA0ZGCtxaHiCQyPjNOZCfuX8lTy7f4jT2xrIZwIq7rQ1ZDk8MkEmCFjZmicbBgwVywQBBGYMF8t4/PfQkAuj39mQg8MT5DMBZ5/WTKlS5We/6KerOU8hG5ANo/du+54BLlrbTkM2xAwK2ZDxcoW+kRJdzTnMjKf3DtLTN8obN3QyMFbitNY8pbJzYKjI+q4mXukfY3VbA5nAKFddLUNZsKO1CBQEKVGtRt/Eh8fL9I1McGZnE/sHixSyIdnQePHgyNSH2ksHRyiWKqxsLdDakKV3aJy9A2PRB1o2nGp9BGacc1ozZsaLvcOUq0656pzd3cxYqcxQsXzk9f3FqX2sbM0zVCzz/IFhcmHAYLE01eXWP1aisynHRMXJhUYYGM/vH6YS/51OtiEmylX6RkusbM1TLFXZP1TEHRqyIV0tOfpHSjTmQypVODg8PvXv0JQL6WzOc2h4nJGJE38x4WQABxa1RgfGonGphmxIxZ2JcnXa9rkwoOrRv+vka1vyGRwYHi/TWsjQ1ZKnqynPYLGEO3Q0Zal61M1YqTrV+N+luZBh78AYE+Uq565qpVp1nts/NBWSXc05zupupqs5T//oBLsOjbCuo5G+0RJ7B8bIBMbr1rTR3ZznmX2DjJWqZAJjolylOZ9hbUcD+4fG+XlPP4dHJjh3VQvnrWplVVuBTGAcHpmgd2icXCbgLa/tZv9gkYPD46xqa2BlS56n9g5iGOu7Gqm689z+YdZ3NvLiwRECM0bHoxB+z6Xr2Nk7RFMuM9Vlm8sE7BsYo7M5z7qORoqlCrv7RhkvVxkvVSlXqxwamSAXBvzr13Zz6foOduwZ5PvP9dI3MsHK1jyvW9NGSyHD7sOjZIKA9V1NdDXn6B0apyEX0tGYoz3+cveLQ6OsbiuwoinHnv4x+kZKnLe6hb7RCQ4MjnNGZyOHhifYdXCEyzasYHffKOs6GgkDo290gmwYUMiGNOVCfvzCIcbLFS5c08ZLvSO8dmUL7Y1Zdh8e4+DIOJWq82TPAGvaC2xc18GqtsIS//YUBLLMlSrVqNsqn5nW5eTuDMRjMVH3WzQWM1gs8fjLfVy0po0DQ+NU3QkDY2C0RGdznnK1yt6BIuWK01rITH2wNuUzWNzSGpuoRL9LFVY0ZRmdqLDr4AjucPG6dvrHSlSq0QfRwFiJ81a3su0X/YQBTFScvpEJTmvJ096U46XeETKhcf7qFla3NfDYS4fpbsnz4sERwsDY0NXEroMjrOloYPueQTKB0d0SBdrB4QkODo/TUsgSWHSfDbNolt3AjCAwRsbLDBVLrGlvIBMGbH25jyAwNq5rp1Sp0lLIcmCwyIsHRxgYLdFcyHDGikZ2Hx6lvTHLmZ1NFEsVdrwyyMBYiXUrGljRlKdcicarhoplfnF4lO7mPP9qTRsrmnM8t2+IZ/cNMTReBqJQ627JM1QsMViMlgUG1fgjaGYrdfL5ZBjmMwHlqk91nS6GWXQfktGJMsXSkddnQ2NFU46DwxNUqov/LGzJZ6bqVysbRi222vrUhvqrvX6+FvsH37yBT7zjgkWXM9pnnYLAzK4G/jcQAn/r7p+ZsT4PfAW4FDgEvM/ddx1tnwoCkWPn7kseo5nvZIX59jk2EXXRFTIhQWCUK1Ue29XHytY86zub2DcYtRbPW9VKLhNELcCqc+6qFnr6RlnT0UA2iI63f6jId585wMVr26lUnUI2pKWQYbxcZXVbgYPD4+w+PEYuY2zoaqYhG5LLBIRBVK5iqcKjLx1m68t9XHB6K1ee3UVzPsPoRJnn9g8zOl7m9PYGKu7sOjjCoZEJVrYWGJuo0D86Qd9oCcdZ39nE3oEiLx8aYXVbA6e3F3jhwDBdLXk6m/I8uaefhmzIRWvb+Nkv+lnf2cRzB4YIzVi3opFypcrweIVfHB7h4rXtdDXnefHgMK/pbuaF3mGGimVOay1weluBStW5eF07eweKtDVk2dDVtKT3rS5BYGYh8BzwNqAHeAz4DXd/qmab/wBc5O6/b2Y3Ar/m7u872n4VBCIii3e0IEhypOkyYKe7v+juE8DXgOtmbHMdcFf8+F7graZTSURETqgkg2ANsLvmeU+8bM5t3L0MDACdM3dkZpvNbIuZbent1ZQQIiLH0ylx7pm73+7um9x9U3d3d72LIyKyrCQZBHuAdTXP18bL5tzGzDJAG9GgsYiInCBJBsFjwDlmtsHMcsCNwAMztnkA+O348Q3Ad/1UO59VROQUl9gUE+5eNrMPA98mOn30TnffYWafBra4+wPAHcBXzWwncJgoLERE5ARKdK4hd38QeHDGsk/WPC4C70myDCIicnSnxGCxiIgk55SbYsLMeoGXl/jyLiCN96ZMY71V53RQnRfuTHef87TLUy4IjoWZbZnvyrrlLI31Vp3TQXU+PtQ1JCKScgoCEZGUS1sQ3F7vAtRJGuutOqeD6nwcpGqMQEREZktbi0BERGZQEIiIpFxqgsDMrjazZ81sp5ndUu/yJMXMdpnZz81sm5ltiZetMLPvmNnz8e+OepfzWJjZnWZ2wMy21yybs44WuTV+3580s9fXr+RLN0+d/8zM9sTv9TYzu7Zm3cfjOj9rZv+mPqU+Nma2zsy+Z2ZPmdkOM7s5Xr5s3+uj1DnZ99rdl/0P0VxHLwBnATngCeCCepcrobruArpmLPvvwC3x41uAv6h3OY+xjm8BXg9sf7U6AtcC3yK67/3lwKP1Lv9xrPOfAX80x7YXxH/jeWBD/Lcf1rsOS6jzauD18eMWojseXrCc3+uj1DnR9zotLYKF3C1tOau9E9xdwLvrWJZj5u4/IJqksNZ8dbwO+IpHHgHazWz1iSnp8TNPnedzHfA1dx9395eAnUT/B04p7r7X3bfGj4eAp4luZrVs3+uj1Hk+x+W9TksQLORuacuFA/9sZo+b2eZ42Up33xs/3gesrE/REjVfHZf7e//huBvkzpouv2VXZzNbD1wCPEpK3usZdenr9gwAAAM0SURBVIYE3+u0BEGavMndXw9cA3zIzN5Su9Kj9uSyPmc4DXWMfQF4DbAR2Av8z/oWJxlm1gzcB3zE3Qdr1y3X93qOOif6XqclCBZyt7Rlwd33xL8PAPcTNRP3TzaR498H6lfCxMxXx2X73rv7fnevuHsV+BJHugSWTZ3NLEv0gXi3u38jXrys3+u56pz0e52WIFjI3dJOeWbWZGYtk4+BtwPbmX4nuN8GvlmfEiZqvjo+APxWfEbJ5cBATbfCKW1G//evEb3XENX5RjPLm9kG4Bzgpye6fMfKzIzo5lVPu/tf1axatu/1fHVO/L2u9yj5CRyNv5ZoBP4F4BP1Lk9CdTyL6AyCJ4Adk/UEOoF/AZ4HHgJW1Lusx1jPe4iaxyWiPtHfna+ORGeQfC5+338ObKp3+Y9jnb8a1+nJ+ANhdc32n4jr/CxwTb3Lv8Q6v4mo2+dJYFv8c+1yfq+PUudE32tNMSEiknJp6RoSEZF5KAhERFJOQSAiknIKAhGRlFMQiIiknIJA5AQys6vM7J/qXQ6RWgoCEZGUUxCIzMHMbjKzn8Zzv3/RzEIzGzazv47nif8XM+uOt91oZo/EE4LdXzM//tlm9pCZPWFmW83sNfHum83sXjN7xszujq8mFakbBYHIDGZ2PvA+4Ep33whUgN8EmoAt7v464PvAp+KXfAX4T+5+EdHVn5PL7wY+5+4XA79EdGUwRDNKfoRoLvmzgCsTr5TIUWTqXQCRk9BbgUuBx+Iv6w1EE5tVga/H2/wf4Btm1ga0u/v34+V3AX8fz/m0xt3vB3D3IkC8v5+6e0/8fBuwHvhR8tUSmZuCQGQ2A+5y949PW2j2pzO2W+r8LOM1jyvo/6HUmbqGRGb7F+AGMzsNpu6ReybR/5cb4m3+LfAjdx8A+szszfHy9wPf9+juUj1m9u54H3kzazyhtRBZIH0TEZnB3Z8ysz8hutNbQDTj54eAEeCyeN0BonEEiKZCvi3+oH8R+EC8/P3AF83s0/E+3nMCqyGyYJp9VGSBzGzY3ZvrXQ6R401dQyIiKacWgYhIyqlFICKScgoCEZGUUxCIiKScgkBEJOUUBCIiKff/AYxCvB3r9eSOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QPwcGZLhbMz"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "from pprint import pprint\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import data, color, io\n",
    "import skimage\n",
    "import PIL\n",
    "import scipy\n",
    "import json\n",
    "import os.path\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def load_image(image_path,target_size):\n",
    "    img = skimage.io.imread(image_path)\n",
    "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
    "    return image_resized\n",
    "\n",
    "def load_cnn_model():\n",
    "    model = Xception()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    return model\n",
    "\n",
    "def extract_features_from_images(image_path):\n",
    "    model = load_cnn_model()\n",
    "    if path.exists(image_path):\n",
    "        print(image_path)\n",
    "        image = load_image(image_path, target_size=(299, 299))\n",
    "        if image.shape == (299, 299, 3):\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            print(feature)\n",
    "            return feature\n",
    "\n",
    "def generate_desc(model, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[word] for word in in_text.split() if word in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = ixtoword[yhat]\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "s2aIfCVPhBAN",
    "outputId": "9d1d5213-6edf-4ea6-9459-b0529d372ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg\n",
      "[[0.         0.         0.14329363 ... 0.13233422 0.         0.        ]]\n",
      "startseq watching the boats come in endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg'#'/Users/vinutahegde/Documents/Personal/IMG_3501.JPG'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ef857nMlhfGJ",
    "outputId": "41deee59-7844-46dc-f5cc-8d0ff3224612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg\n",
      "[[0.         0.         0.16217265 ... 0.11656351 0.         0.        ]]\n",
      "startseq watching the boats come in endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/1191943.jpg\n",
      "[[0.         0.         0.15539224 ... 0.12000053 0.         0.        ]]\n",
      "startseq watching the boats come in endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/1191943.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/1144535.jpg\n",
      "[[0.         0.         0.14820762 ... 0.12520814 0.         0.        ]]\n",
      "startseq we had a very nice day shopping in the city.we each bought a nice tee shirt and had a nice lunch as well . endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/1144535.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/1191942.jpg\n",
      "[[0.         0.         0.13261661 ... 0.14653273 0.         0.        ]]\n",
      "startseq and they were on the shore as well . endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/1191942.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fixing_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
