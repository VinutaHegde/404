{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lX-B3q67XANg",
    "outputId": "bd8549cd-9e10-4e0e-ad12-c7f3049ca0c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from numpy import argmax\n",
    "# from pickle import load\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import load_model\n",
    "# # from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "# import random\n",
    "import csv\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Wesam\n",
    "# SEED = 10\n",
    "IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
    "IMAGE_EMBEDDING_VAL_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
    "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
    "# NUM_IMAGE_EMBEDDING_CHUNKS = 1\n",
    "GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.50d.txt'\n",
    "# MAX_SEQUENCE_LENGTH = 92\n",
    "# WORD_EMBEDDING_DIM = 300\n",
    "CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'\n",
    "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
    "\n",
    "#Vinuta\n",
    "SEED = 10\n",
    "#IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
    "NUM_IMAGE_EMBEDDING_CHUNKS = 5\n",
    "#GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
    "MAX_SEQUENCE_LENGTH = 92\n",
    "WORD_EMBEDDING_DIM = 50\n",
    "#CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MCvM68FbJ7h6",
    "outputId": "d84d7ac6-d3dc-4de8-a3f4-bb6d7ad88ee8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuq6CmUtYsEE"
   },
   "source": [
    "#PreProcess Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16o6JbtNXHVa"
   },
   "outputs": [],
   "source": [
    "def getCaptions(id_list):\n",
    "    caption_dict = {}\n",
    "    with open(CAPTION_FILE_NAME) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if row[1] in id_list:\n",
    "                caption_dict[row[1]] = ['startseq ' + row[2] + ' endseq']\n",
    "                #caption_dict[row[1]] = 'startseq ' + row[2] + ' endseq'\n",
    "    return caption_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brA9HhUmYZQ4"
   },
   "outputs": [],
   "source": [
    "def vocab_fun(captions):\n",
    "  index_to_word = {}\n",
    "  word_to_index = {}\n",
    "  all_words = {}\n",
    "  for img_id, cap in captions.items():\n",
    "      for c in cap:\n",
    "          for word in c.split():\n",
    "            all_words[word] = 1\n",
    "  all_vocab=[w for w in all_words]\n",
    "  index = 0\n",
    "  for word in all_vocab:\n",
    "      word_to_index[word] = index\n",
    "      index_to_word[index] = word\n",
    "      index += 1\n",
    "  return (all_vocab, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl15hHkcYxYj"
   },
   "source": [
    "#Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90V-9vXXYjgx"
   },
   "outputs": [],
   "source": [
    "def Merge(dict1, dict2): \n",
    "    res = {**dict1, **dict2} \n",
    "    return res \n",
    "\n",
    "def getImageEmbedding(path):\n",
    "    image_embedding = {}\n",
    "    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):\n",
    "         file_name = path + 'cnn_group'+str(i+1)+'.json'\n",
    "         with open(file_name) as json_file:\n",
    "#    with open(file_name) as json_file:\n",
    "            print(file_name)\n",
    "            json_data = json.load(json_file)\n",
    "            json_data = json.loads(json_data)\n",
    "            image_embedding = Merge(image_embedding, json_data) \n",
    "            #image_embedding = json_data \n",
    "    return image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWk3ZLVkIplE"
   },
   "outputs": [],
   "source": [
    "# xx = getImageEmbedding()\n",
    "# yy = {}\n",
    "# for x in list(reversed(list(xx)))[0:1000]:\n",
    "#     yy[x] = xx[x]\n",
    "    \n",
    "# j = json.dumps(yy)\n",
    "# with open(\"json_1000_images\", 'w') as outfile:\n",
    "#     json.dump(j, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZtfju_3Z_6E"
   },
   "source": [
    "#for fit_generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWul5J87aDpv"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch): #descriptions are captions\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split() if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photos[key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dJwBxEGaYNy"
   },
   "source": [
    "#Use Prev to get captions and images and pre_process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XMqm0aZXaYgo",
    "outputId": "a714b8d6-f08c-43e8-9957-6948fd683804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/cnn_group1.json\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/cnn_group2.json\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/cnn_group3.json\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/cnn_group4.json\n",
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/cnn_group5.json\n",
      "29005\n",
      "29005\n",
      "Vocabulary Size: 12062\n"
     ]
    }
   ],
   "source": [
    "#for training\n",
    "image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
    "print(len(image_embd))\n",
    "image_embd_train=image_embd\n",
    "#split training and validation\n",
    "#split_ix = int((len(image_embd)-1)*0.9)\n",
    "#image_embd_train = dict(list(image_embd.items())[:split_ix])\n",
    "#image_embd_val = dict(list(image_embd.items())[split_ix:])\n",
    "\n",
    "# image_embd_train = np.take((0.9*(len(image_embd))), image_embd.items())\n",
    "# for key ,val in image_embd.items():\n",
    "#   if key not in image_embd_train:\n",
    "#     image_embd_val[key] = val\n",
    "print(len(image_embd_train))\n",
    "#print(len(image_embd_val))\n",
    "\n",
    "image_ids = list(image_embd_train.keys())\n",
    "caption_map = getCaptions(image_ids)\n",
    "all_vocab, wordtoix, ixtoword=vocab_fun(caption_map)\n",
    "vocab_size = len(all_vocab)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = 94 #max_length(caption_map)\n",
    "#for validation\n",
    "#image_ids_val = list(image_embd_val.keys())\n",
    "#caption_map_val = getCaptions(image_ids_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8amlOYORaY6U"
   },
   "source": [
    "#Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d0TgQ6BKaZSA",
    "outputId": "e3affd15-9315-4e13-b050-5922b3e09fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#get matrxi embedding for glove\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpo33OpBdU2L"
   },
   "source": [
    "#Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaC1BgwxdZsM"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  inputs1 = Input(shape=(2048,),name='images')\n",
    "  fe1 = Dropout(0.5)(inputs1)\n",
    "  fe2 = Dense(50, activation='relu')(fe1)\n",
    "  inputs2 = Input(shape=(max_length,),name='sequences')\n",
    "  se1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
    "  se2 = Dropout(0.5)(se1)\n",
    "  se3 = GRU(50,recurrent_dropout=0.3)(se2)\n",
    "  decoder1 = add([fe2, se3])\n",
    "  decoder2 = Dense(50, activation='relu')(decoder1)\n",
    "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCFNBmPWdZ71"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z9RFuY7wfx4t",
    "outputId": "0c732971-b1d1-40af-ac9a-c96e40f612fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequences (InputLayer)          (None, 94)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "images (InputLayer)             (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 94, 50)       603100      sequences[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 2048)         0           images[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 94, 50)       0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 50)           102450      dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (None, 50)           15150       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 50)           0           dense_18[0][0]                   \n",
      "                                                                 gru_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 50)           2550        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 12062)        615162      dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,338,412\n",
      "Trainable params: 1,338,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=build_model()\n",
    "model.summary()\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "#optz = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "#optz = Adams(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "optz = Adam(lr=0.01, decay=0.1)\n",
    "#optz = RMSprop(lr=0.001, rho=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optz)#'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uocQxUZRaRe-"
   },
   "source": [
    "#Using Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PK8tkr49aQ4a"
   },
   "outputs": [],
   "source": [
    "def all_data(descriptions, photos, wordtoix, max_length): #descriptions are captions\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        # loop for ever over images\n",
    "        for key, desc_list in descriptions.items():\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split() if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    X1.append(photos[key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "        return (array(X1), array(X2), array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "YR4FhSY2cW1C",
    "outputId": "5910763c-efb4-4687-de13-1c281a2228c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 317590 samples, validate on 35288 samples\n",
      "Epoch 1/50\n",
      "317590/317590 [==============================] - 574s 2ms/step - loss: 5.9422 - val_loss: 5.8253\n",
      "\n",
      "Epoch 00001: saving model to fit_adam_50d_1_split-ep001-loss5.942.h5\n",
      "Epoch 2/50\n",
      "317590/317590 [==============================] - 573s 2ms/step - loss: 5.8631 - val_loss: 5.8105\n",
      "\n",
      "Epoch 00002: saving model to fit_adam_50d_1_split-ep002-loss5.863.h5\n",
      "Epoch 3/50\n",
      "317590/317590 [==============================] - 602s 2ms/step - loss: 6.0008 - val_loss: 5.8016\n",
      "\n",
      "Epoch 00003: saving model to fit_adam_50d_1_split-ep003-loss6.001.h5\n",
      "Epoch 4/50\n",
      "317590/317590 [==============================] - 581s 2ms/step - loss: 5.8591 - val_loss: 5.7979\n",
      "\n",
      "Epoch 00004: saving model to fit_adam_50d_1_split-ep004-loss5.859.h5\n",
      "Epoch 5/50\n",
      "317590/317590 [==============================] - 580s 2ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00005: saving model to fit_adam_50d_1_split-ep005-lossnan.h5\n",
      "Epoch 6/50\n",
      "317590/317590 [==============================] - 591s 2ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00006: saving model to fit_adam_50d_1_split-ep006-lossnan.h5\n",
      "Epoch 7/50\n",
      "128224/317590 [===========>..................] - ETA: 5:50 - loss: nan"
     ]
    }
   ],
   "source": [
    "X1train, X2train, ytrain = all_data(caption_map, image_embd, wordtoix, max_length)\n",
    "filepath_checkpoint = \"fit_adam_50d_1_split-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=False, mode='min')\n",
    "history = model.fit([X1train, X2train], ytrain, epochs=50, verbose=1, batch_size=32, callbacks=[checkpoint], validation_split=0.1, shuffle=True, workers=10, use_multiprocessing=True)\n",
    "model.save('fit_adam_50d_1_split_finale.h5')\n",
    "\n",
    "# 1 epoch 40 min, loss from 12+ to 5+ # 256 image , dense\n",
    "# 50 dense :  12 min/ epoch 5 splits? 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtbD8xuDaTtd"
   },
   "source": [
    "#using Fit Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yD99jV4BdVBf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# batch_size=60\n",
    "# steps = len(caption_map)\n",
    "# generator = data_generator(caption_map, image_embd, wordtoix, max_length, batch_size)\n",
    "# valid_data = data_generator(caption_map_val, image_embd_val, wordtoix, max_length, batch_size)\n",
    "\n",
    "# filepath_checkpoint = \"/content/drive/My Drive/Colab_Notebooks/DL_data/adam_50d_1_split-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
    "# checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=False, mode='min')\n",
    "# history = model.fit_generator(generator, epochs=300, steps_per_epoch=steps, verbose=1, validation_steps=len(caption_map_val) , validation_data=valid_data, callbacks=[checkpoint])#use_multiprocessing=True, workers=10,\n",
    "# model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/adam_50d_1_split_model_finale.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fu6qxITIplR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QPwcGZLhbMz"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "from pprint import pprint\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import data, color, io\n",
    "import skimage\n",
    "import PIL\n",
    "import scipy\n",
    "import json\n",
    "import os.path\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def load_image(image_path,target_size):\n",
    "    img = skimage.io.imread(image_path)\n",
    "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
    "    return image_resized\n",
    "\n",
    "def load_cnn_model():\n",
    "    model = Xception()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    return model\n",
    "\n",
    "def extract_features_from_images(image_path):\n",
    "    model = load_cnn_model()\n",
    "    if path.exists(image_path):\n",
    "        print(image_path)\n",
    "        image = load_image(image_path, target_size=(299, 299))\n",
    "        if image.shape == (299, 299, 3):\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            print(feature)\n",
    "            return feature\n",
    "\n",
    "def generate_desc(model, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[word] for word in in_text.split() if word in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = ixtoword[yhat]\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "s2aIfCVPhBAN",
    "outputId": "9d1d5213-6edf-4ea6-9459-b0529d372ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg\n",
      "[[0.         0.         0.14329363 ... 0.13233422 0.         0.        ]]\n",
      "startseq and they were on the shore as well . endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124919.jpg'#'/Users/vinutahegde/Documents/Personal/IMG_3501.JPG'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ef857nMlhfGJ",
    "outputId": "41deee59-7844-46dc-f5cc-8d0ff3224612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg\n",
      "[[0.         0.         0.16217265 ... 0.11656351 0.         0.        ]]\n",
      "startseq watching the boats come in endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/images/train/images/train/124871.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-THdS55IplV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_with_SGD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
