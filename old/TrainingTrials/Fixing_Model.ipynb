{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "lX-B3q67XANg",
    "outputId": "d7900194-16f4-493a-f572-4a3ce258c4f6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from numpy import argmax\n",
    "# from pickle import load\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import load_model\n",
    "# # from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "# import random\n",
    "import csv\n",
    "\n",
    "\n",
    "# SEED = 10\n",
    "# #IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
    "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
    "# NUM_IMAGE_EMBEDDING_CHUNKS = 1\n",
    "# GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'\n",
    "# MAX_SEQUENCE_LENGTH = 92\n",
    "# WORD_EMBEDDING_DIM = 300\n",
    "# CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
    "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
    "\n",
    "\n",
    "SEED = 10\n",
    "IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
    "NUM_IMAGE_EMBEDDING_CHUNKS = 1\n",
    "GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
    "MAX_SEQUENCE_LENGTH = 92\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuq6CmUtYsEE"
   },
   "source": [
    "#PreProcess Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16o6JbtNXHVa"
   },
   "outputs": [],
   "source": [
    "def getCaptions(id_list):\n",
    "    caption_dict = {}\n",
    "    with open(CAPTION_FILE_NAME) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if row[1] in id_list:\n",
    "                caption_dict[row[1]] = ['startseq ' + row[2] + ' endseq']\n",
    "                #caption_dict[row[1]] = 'startseq ' + row[2] + ' endseq'\n",
    "    return caption_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brA9HhUmYZQ4"
   },
   "outputs": [],
   "source": [
    "def vocab_fun(captions):\n",
    "  index_to_word = {}\n",
    "  word_to_index = {}\n",
    "  all_words = {}\n",
    "  for img_id, cap in captions.items():\n",
    "      for c in cap:\n",
    "          for word in c.split():\n",
    "            all_words[word] = 1\n",
    "  all_vocab=[w for w in all_words]\n",
    "  index = 0\n",
    "  for word in all_vocab:\n",
    "      word_to_index[word] = index\n",
    "      index_to_word[index] = word\n",
    "      index += 1\n",
    "  return (all_vocab, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl15hHkcYxYj"
   },
   "source": [
    "#Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90V-9vXXYjgx"
   },
   "outputs": [],
   "source": [
    "def Merge(dict1, dict2): \n",
    "    res = {**dict1, **dict2} \n",
    "    return res \n",
    "    \n",
    "def getImageEmbedding():\n",
    "    image_embedding = {}\n",
    "    for i in range (NUM_IMAGE_EMBEDDING_CHUNKS):\n",
    "        file_name = IMAGE_EMBEDDING_DIR + 'group_'+str(i+1)+'.json'\n",
    "        with open(file_name) as json_file:\n",
    "            print(file_name)\n",
    "            json_data = json.load(json_file)\n",
    "            json_data = json.loads(json_data)\n",
    "            image_embedding = Merge(image_embedding, json_data) \n",
    "            image_embedding = json_data \n",
    "        return image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZtfju_3Z_6E"
   },
   "source": [
    "#for fit_generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWul5J87aDpv"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch): #descriptions are captions\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split() if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photos[key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dJwBxEGaYNy"
   },
   "source": [
    "#Use Prev to get captions and images and pre_process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XMqm0aZXaYgo",
    "outputId": "1e17e5b1-94a7-4ae8-c24d-3bd3370f7253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/group_1.json\n",
      "4480\n",
      "Vocabulary Size: 4435\n"
     ]
    }
   ],
   "source": [
    "image_embd =  getImageEmbedding()\n",
    "print(len(image_embd))\n",
    "# for val in image_embd.values():\n",
    "#   print(np.shape(val))\n",
    "#   break\n",
    "image_ids = list(image_embd.keys())\n",
    "caption_map = getCaptions(image_ids)\n",
    "all_vocab, wordtoix, ixtoword=vocab_fun(caption_map)\n",
    "#tokenizer = create_tokenizer(caption_map)\n",
    "\n",
    "vocab_size = len(all_vocab)#len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = 94 #max_length(caption_map)\n",
    "#X1train, X2train, ytrain = create_sequences(tokenizer, max_length, caption_map, image_embd, vocab_size)\n",
    "#X1test, X2test, ytest = create_sequences(tokenizer, max_length, caption_map, image_embd, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8amlOYORaY6U"
   },
   "source": [
    "#Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d0TgQ6BKaZSA",
    "outputId": "d80ff5df-99b1-4922-de8b-5701bb995289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#get matrxi embedding for glove\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_dim = 300\n",
    "\n",
    "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpo33OpBdU2L"
   },
   "source": [
    "#Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaC1BgwxdZsM"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  inputs1 = Input(shape=(2048,),name='images')\n",
    "  fe1 = Dropout(0.5)(inputs1)\n",
    "  fe2 = Dense(256, activation='relu')(fe1)\n",
    "  inputs2 = Input(shape=(max_length,),name='sequences')\n",
    "  se1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
    "  se2 = Dropout(0.5)(se1)\n",
    "  se3 = LSTM(256)(se2)\n",
    "  decoder1 = add([fe2, se3])\n",
    "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCFNBmPWdZ71"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "Z9RFuY7wfx4t",
    "outputId": "9c32d320-f0d7-4d50-9feb-dbcc641afb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/404/404/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequences (InputLayer)          (None, 94)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "images (InputLayer)             (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 94, 300)      1330500     sequences[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           images[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 94, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          570368      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4435)         1139795     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,630,999\n",
      "Trainable params: 3,630,999\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=build_model()\n",
    "model.summary()\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yD99jV4BdVBf",
    "outputId": "78c0fcd5-b739-4c43-9c76-c396f071403c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  28/4480 [..............................] - ETA: 33:48:22 - loss: 5.3477"
     ]
    }
   ],
   "source": [
    "batch_size=556\n",
    "epochs = 2\n",
    "steps = len(caption_map)\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(caption_map, image_embd, wordtoix, max_length, batch_size)\n",
    "    model.fit_generator(generator, epochs=2, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/late_late_night_model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QPwcGZLhbMz"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "from pprint import pprint\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import data, color, io\n",
    "import skimage\n",
    "import PIL\n",
    "import scipy\n",
    "import json\n",
    "import os.path\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "def load_image(image_path,target_size):\n",
    "    img = skimage.io.imread(image_path)\n",
    "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
    "    return image_resized\n",
    "\n",
    "def load_cnn_model():\n",
    "    model = Xception()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    return model\n",
    "\n",
    "def extract_features_from_images(image_path):\n",
    "    model = load_cnn_model()\n",
    "    if path.exists(image_path):\n",
    "        print(image_path)\n",
    "        image = load_image(image_path, target_size=(299, 299))\n",
    "        if image.shape == (299, 299, 3):\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            print(feature)\n",
    "            return feature\n",
    "\n",
    "def generate_desc(model, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[word] for word in in_text.split() if word in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = ixtoword[yhat]\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "s2aIfCVPhBAN",
    "outputId": "9d1d5213-6edf-4ea6-9459-b0529d372ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels.h5\n",
      "91889664/91884032 [==============================] - 7s 0us/step\n",
      "/content/drive/My Drive/Colab_Notebooks/DL_data/images/1.jpg\n",
      "[[0.         0.         0.19784988 ... 0.1310169  0.         0.        ]]\n",
      "startseq the family is having a nice time playing games endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/content/drive/My Drive/Colab_Notebooks/DL_data/images/1.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ef857nMlhfGJ",
    "outputId": "41deee59-7844-46dc-f5cc-8d0ff3224612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab_Notebooks/DL_data/images/2.jpg\n",
      "[[0.         0.         0.15794557 ... 0.13588832 0.         0.        ]]\n",
      "startseq the family is having a nice time playing games endseq\n"
     ]
    }
   ],
   "source": [
    "image_path='/content/drive/My Drive/Colab_Notebooks/DL_data/images/2.jpg'\n",
    "feature=extract_features_from_images(image_path)\n",
    "print(generate_desc(model, feature, max_length))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fixing_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
