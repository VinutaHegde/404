{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "404",
      "language": "python",
      "name": "404"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "vinuta_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vexlw8nTD2ZH",
        "colab_type": "code",
        "outputId": "1a0205a4-7763-4515-e44b-3a8f1d09f2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# from nltk.translate.bleu_score import corpus_bleu\n",
        "import json\n",
        "import random\n",
        "import csv\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "import os\n",
        "from pprint import pprint\n",
        "from skimage.transform import rescale, resize\n",
        "from skimage import data, color, io\n",
        "import skimage\n",
        "import PIL\n",
        "import scipy\n",
        "import json\n",
        "import os.path\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications.xception import preprocess_input\n",
        "from keras.applications.xception import Xception\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlKDOxwEE8Oh",
        "colab_type": "code",
        "outputId": "0bc460d2-9f69-45b2-e9b1-43e5f3cba1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnzmWT6CD2ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 10\n",
        "#IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'\n",
        "IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
        "NUM_IMAGE_EMBEDDING_CHUNKS = 1\n",
        "GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 92\n",
        "WORD_EMBEDDING_DIM = 300\n",
        "CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rUaObRBD2ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class text_encoder:\n",
        "    def get_embedding_matrix(self,filename, WORD_EMBEDDING_DIM):\n",
        "        embeddings_index = {}\n",
        "        word_to_index = {}\n",
        "\n",
        "        with open(filename) as f:\n",
        "            for index, line in enumerate(f):\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "                word_to_index[word] = index\n",
        "\n",
        "          # Vocabulary\n",
        "        vocabulary = embeddings_index.keys()\n",
        "        embedding_matrix = np.zeros((len(vocabulary) + 1, WORD_EMBEDDING_DIM))\n",
        "\n",
        "        for word, i in word_to_index.items():\n",
        "            embedding_vector = embeddings_index[word]\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return embedding_matrix, vocabulary, word_to_index\n",
        "    \n",
        "    def load_embeddings(self):\n",
        "        embedding_matrix, vocabulary, word_to_index = self.get_embedding_matrix(GLOVE_EMBEDDING_FILE_NAME, WORD_EMBEDDING_DIM)      \n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.vocabulary = vocabulary\n",
        "        self.word_to_index = word_to_index\n",
        "        \n",
        "    def get_sentence_embedding(self,sentence):\n",
        "        words = sentence.split()    \n",
        "        sentence_embedding =  np.zeros(shape=(MAX_SEQUENCE_LENGTH,WORD_EMBEDDING_DIM))\n",
        "        mask =  np.zeros(MAX_SEQUENCE_LENGTH)\n",
        "        i=0;\n",
        "        for w in words:\n",
        "            mask[i] = 1\n",
        "            index = self.word_to_index.get(w,-1)\n",
        "            if(index != -1):\n",
        "                sentence_embedding[i] = self.embedding_matrix[index]\n",
        "            else:\n",
        "                sentence_embedding[i] = np.zeros(WORD_EMBEDDING_DIM)\n",
        "            i+=1       \n",
        "        return  sentence_embedding, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkjbVYvnD2ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "# loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key][0]\n",
        "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "            yield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_LcycMCD2Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "#     print(photos)\n",
        "    # walk through each image identifier\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # walk through each description for the image\n",
        "#         print(desc_list)\n",
        "#         print(photos[key])\n",
        "        for desc in desc_list:\n",
        "#             print(desc)\n",
        "            # encode the sequence\n",
        "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "            # split one sequence into multiple X,y pairs\n",
        "#             print(seq)\n",
        "            for i in range(1, len(seq)):\n",
        "                # split into input and output pair\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "#                 print(in_seq)\n",
        "#                 print(out_seq)\n",
        "#                 print()\n",
        "                # pad input sequence\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                # encode output sequence\n",
        "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                # store\n",
        "                X1.append(photos[key])\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "#                 print(photos[key][0])\n",
        "    return array(X1), array(X2), array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fupq7-IoD2Zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        print('type(photo)')\n",
        "        print(type(photo))\n",
        "        print('type(sequence)')\n",
        "        print(type(sequence))\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        print(yhat)\n",
        "        yhat = argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVggP9B7D2Zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Merge(dict1, dict2): \n",
        "    res = {**dict1, **dict2} \n",
        "    return res "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3K__VrcD2Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getImageEmbedding():\n",
        "    image_embedding = {}\n",
        "    for i in range (NUM_IMAGE_EMBEDDING_CHUNKS):\n",
        "        file_name = IMAGE_EMBEDDING_DIR + 'group_'+str(i+1)+'.json'\n",
        "        with open(file_name) as json_file:\n",
        "            print(file_name)\n",
        "            json_data = json.load(json_file)\n",
        "            json_data = json.loads(json_data)\n",
        "            image_embedding = Merge(image_embedding, json_data) \n",
        "            image_embedding = json_data \n",
        "        return image_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktjB7iRpD2Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getCaptions(id_list):\n",
        "    caption_dict = {}\n",
        "    with open(CAPTION_FILE_NAME) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for row in csv_reader:\n",
        "            if row[1] in id_list:\n",
        "                caption_dict[row[1]] = ['startseq ' + row[2] + ' endseq']\n",
        "    return caption_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZHUmWQtD2Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "#         print(descriptions[key])\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "#     print(all_desc)\n",
        "    return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHDfROuiD2Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txbEOmqzIIum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocab_fun(captions):\n",
        "  index_to_word = {}\n",
        "  word_to_index = {}\n",
        "  all_words = {}\n",
        "  for cap in captions:\n",
        "      for word in cap.split(' '):\n",
        "          all_words[word] = 1\n",
        "  all_vocab=[w for w in all_words]\n",
        "  index = 1\n",
        "  for word in all_vocab:\n",
        "      word_to_index[word] = index\n",
        "      index_to_word[index] = word\n",
        "      index += 1\n",
        "  return (all_vocab, word_to_index, index_to_word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCmoNmwYD2Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    # decoder model\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.layers[2].set_weights([embedding_matrix])\n",
        "    model.layers[2].trainable = False\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtLk8nbD2Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(descriptions):\n",
        "#     print(descriptions)\n",
        "    lines = to_lines(descriptions)\n",
        "#     print(lines)\n",
        "#     return max(len(d.split()) for d in lines)\n",
        "    return 94"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEUFhUUqD2Zm",
        "colab_type": "code",
        "outputId": "26177624-ea1e-428e-e445-bb03262ab33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "image_embd =  getImageEmbedding()\n",
        "image_ids = list(image_embd.keys())\n",
        "text_ebmd_encoder = text_encoder()\n",
        "text_ebmd_encoder.load_embeddings()\n",
        "caption_map = getCaptions(image_ids)\n",
        "# print(caption_map)\n",
        "all_vocab, wordtoix, ixtoword=vocab_fun(getCaptions(image_ids))\n",
        "tokenizer = create_tokenizer(caption_map)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = 94 #max_length(caption_map)\n",
        "print(max_length)\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, caption_map, image_embd, vocab_size)\n",
        "# print(X1train.shape)\n",
        "# print(X2train.shape)\n",
        "# print(ytrain.shape)\n",
        "\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, caption_map, image_embd, vocab_size)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/group_1.json\n",
            "Vocabulary Size: 4386\n",
            "94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p14XFGwCD5lR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23d1896a-1fe9-4d47-e70e-766fef4a1b6f"
      },
      "source": [
        "#embedding matrix\n",
        "# def get_embedding_matrix(filename, WORD_EMBEDDING_DIM):\n",
        "#         embeddings_index = {}\n",
        "#         word_to_index = {}\n",
        "\n",
        "#         with open(filename) as f:\n",
        "#             for index, line in enumerate(f):\n",
        "#                 values = line.split()\n",
        "#                 word = values[0]\n",
        "#                 coefs = np.asarray(values[1:], dtype='float32')\n",
        "#                 embeddings_index[word] = coefs\n",
        "#                 word_to_index[word] = index\n",
        "\n",
        "#           # Vocabulary\n",
        "#         vocabulary = embeddings_index.keys()\n",
        "#         embedding_matrix = np.zeros((len(vocabulary) + 1, WORD_EMBEDDING_DIM))\n",
        "\n",
        "#         for word, i in word_to_index.items():\n",
        "#             embedding_vector = embeddings_index[word]\n",
        "#             if embedding_vector is not None:\n",
        "#                 embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#         return embedding_matrix, vocabulary, word_to_index\n",
        "# embedding_matrix, vocabulary, word_to_index = get_embedding_matrix(GLOVE_EMBEDDING_FILE_NAME, WORD_EMBEDDING_DIM) \n",
        "# Load Glove vectors\n",
        "\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "embedding_dim = 300\n",
        "\n",
        "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA7-CNoys6vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "acedcfc1-9ae3-49a9-8fb1-e8d1f0621cbc"
      },
      "source": [
        "model = define_model(vocab_size, max_length)\n",
        "filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit([X1train, X2train], ytrain, epochs=2, verbose=2, callbacks=[checkpoint])#, validation_data=([X1test, X2test], ytest))\n",
        "#model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/saved_model_1.h5')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 94)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 2048)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 94, 300)      1315800     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 2048)         0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 94, 300)      0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 256)          524544      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 256)          570368      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 256)          0           dense_10[0][0]                   \n",
            "                                                                 lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 256)          65792       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 4386)         1127202     dense_11[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,603,706\n",
            "Trainable params: 2,287,906\n",
            "Non-trainable params: 1,315,800\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            " - 290s - loss: 6.0311\n",
            "Epoch 2/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " - 291s - loss: 5.7811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd2b39db6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkl4sFK4psAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model\n",
        "model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/late_night_model.h5')\n",
        "\n",
        "#load model\n",
        "\n",
        "###saved_model='/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep020-loss1.914-val_loss1.359.h5'\n",
        "#mymodel = load_model(saved_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2kvwyAJ0z2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "def load_image(image_path,target_size):\n",
        "    img = skimage.io.imread(image_path)\n",
        "    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)\n",
        "    return image_resized\n",
        "\n",
        "def load_cnn_model():\n",
        "    model = Xception()\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    return model\n",
        "\n",
        "def extract_features_from_images(image_path):\n",
        "    model = load_cnn_model()\n",
        "    if path.exists(image_path):\n",
        "        print(image_path)\n",
        "        image = load_image(image_path, target_size=(299, 299))\n",
        "        if image.shape == (299, 299, 3):\n",
        "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "            image = preprocess_input(image)\n",
        "            feature = model.predict(image, verbose=0)\n",
        "            print(feature)\n",
        "            return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnB2XZuu16KT",
        "colab_type": "code",
        "outputId": "f92f782a-2650-4992-ac4c-635fa56fc647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "image_path='/content/drive/My Drive/Colab_Notebooks/DL_data/images/1.jpg'\n",
        "feature=extract_features_from_images(image_path)\n",
        "print(generate_desc(model, tokenizer, feature, max_length))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/images/1.jpg\n",
            "[[0.         0.         0.19785109 ... 0.13101783 0.         0.        ]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[6.1435645e-07 5.2448905e-07 1.1625147e-03 ... 1.0574661e-06\n",
            "  1.1881805e-06 6.3044484e-07]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[8.0450013e-07 6.3264753e-07 8.5392315e-04 ... 7.9277794e-07\n",
            "  7.8763543e-07 5.3757917e-06]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[4.3201891e-07 3.3763084e-07 4.1261385e-03 ... 1.2644450e-06\n",
            "  6.8983906e-07 1.8189119e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[1.94798099e-06 1.70177861e-06 1.19203795e-02 ... 8.24908420e-06\n",
            "  4.28143812e-06 3.92038964e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[2.9027931e-06 2.5370653e-06 2.5508953e-02 ... 1.7076118e-05\n",
            "  9.4803245e-06 2.8608176e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[3.8887906e-06 3.3388285e-06 3.9145935e-02 ... 2.7251930e-05\n",
            "  1.7512655e-05 1.9276169e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[4.7465232e-06 4.0010996e-06 5.2635822e-02 ... 4.1132364e-05\n",
            "  2.7935459e-05 1.2566131e-05]]\n",
            "startseq the was a the the the endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSTT5FNPPzGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "5f769777-817c-4a13-e80b-19d59ba3db23"
      },
      "source": [
        "image_path='/content/drive/My Drive/Colab_Notebooks/DL_data/images/2.jpg'\n",
        "feature=extract_features_from_images(image_path)\n",
        "print(generate_desc(model, tokenizer, feature, max_length))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_Notebooks/DL_data/images/2.jpg\n",
            "[[0.         0.         0.15794466 ... 0.13588776 0.         0.        ]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[7.2674345e-07 6.2345947e-07 1.1963164e-03 ... 1.2358080e-06\n",
            "  1.4018223e-06 7.4154372e-07]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[8.4933782e-07 6.6880216e-07 8.6247822e-04 ... 8.4257880e-07\n",
            "  8.3450595e-07 5.6679178e-06]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[4.3426610e-07 3.3987908e-07 4.1198018e-03 ... 1.2790514e-06\n",
            "  6.9565817e-07 1.8155726e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[1.9301801e-06 1.6900352e-06 1.1906997e-02 ... 8.2302486e-06\n",
            "  4.2447100e-06 3.9095747e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[2.8894260e-06 2.5273987e-06 2.5363958e-02 ... 1.6986061e-05\n",
            "  9.4382258e-06 2.8595050e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[3.8251674e-06 3.2883308e-06 3.9349012e-02 ... 2.7090211e-05\n",
            "  1.7367438e-05 1.8997889e-05]]\n",
            "type(photo)\n",
            "<class 'numpy.ndarray'>\n",
            "type(sequence)\n",
            "<class 'numpy.ndarray'>\n",
            "[[4.6839136e-06 3.9573852e-06 5.3308643e-02 ... 4.0853607e-05\n",
            "  2.7922060e-05 1.2418387e-05]]\n",
            "startseq the was a the the the endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c6-aSmJQmhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
