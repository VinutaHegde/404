{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of 6_FinalWithGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfSNell0rRx9",
        "colab_type": "text"
      },
      "source": [
        "###NOTES TO MYSELF:\n",
        "+ Using a Generator and categorical cross entropy with 1 hot encoding\n",
        "\n",
        "+ I'm using 2 inputs here\n",
        "\n",
        "+ I'm also using bidirectional ENCODER\n",
        "\n",
        "+ using simple GRU decoder\n",
        "\n",
        "+ No image embedding\n",
        "\n",
        "+ max sequence length is 12\n",
        "\n",
        "+ using rmsprop optimizer\n",
        "\n",
        "-- Check validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lX-B3q67XANg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "76f57532-b40b-427c-ff36-75edccd42138"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU, Masking, Lambda, Concatenate, Average, Reshape\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "import json\n",
        "import csv\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "# tf.test.gpu_device_name()\n",
        "\n",
        "\n",
        "#Wesam\n",
        "# SEED = 10\n",
        "#IMAGE_EMBEDDING_VAL_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/validation/'\n",
        "# IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/one_sample_cnn/'\n",
        "# filepath = '/content/drive/My Drive/Colab_Notebooks/DL_data/model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
        "#CAPTION_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/image_to_caption.csv'\n",
        "\n",
        "\n",
        "#on my Colab\n",
        "ALL_CAPTIONS_FILE = '/content/drive/My Drive/dldata/all_captions.txt'\n",
        "COMPLETE_STORIES_FILE = '/content/drive/My Drive/dldata/complete_stories_all_splits.json'\n",
        "IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/CNNFeatureVectors/'\n",
        "GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/dldata/glove.6B.300d.txt'\n",
        "\n",
        "\n",
        "# #For my GCP:\n",
        "# ALL_CAPTIONS_FILE = 'all_captions.txt'\n",
        "# COMPLETE_STORIES_FILE = 'complete_stories_all_splits.json'\n",
        "# IMAGE_EMBEDDING_DIR = 'CNNFeatureVectors/'\n",
        "# GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "\n",
        "\n",
        "#Vinuta\n",
        "SEED = 10\n",
        "IMAGE_EMBEDDING_DIM = 2048\n",
        "#IMAGE_EMBEDDING_DIR = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/'\n",
        "NUM_IMAGE_EMBEDDING_CHUNKS = 10\n",
        "#GLOVE_EMBEDDING_FILE_NAME = 'glove.6B.300d.txt'\n",
        "MAX_SEQUENCE_LENGTH = 12\n",
        "WORD_EMBEDDING_DIM = 300\n",
        "#CAPTION_FILE_NAME = '/Users/vinutahegde/Documents/USC/SEM3/DL/project/ws/image_to_caption.csv'\n",
        "SENTENCE_EMBEDDING_DIM = 512\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvM68FbJ7h6",
        "colab_type": "code",
        "outputId": "98dbc943-3901-4d21-da4a-9abd211233e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#commented for GCP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iuq6CmUtYsEE"
      },
      "source": [
        "#PreProcess Captions / Stories\n",
        "\n",
        "Either call this function or simply load preprocessed from a file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqQ7qPFcRDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to check if story ids are repeated or unique\n",
        "# story_list = list()\n",
        "# for key in list(all_captions_dict.keys()):\n",
        "#   story_list += list(all_captions_dict[key].keys())\n",
        "# from collections import Counter\n",
        "# print(len(story_dict))\n",
        "# d =  Counter(story_dict)\n",
        "# res = [k for k, v in d.items() if v > 1]\n",
        "# print(len(res)) ## gave zero .. so story ids are -in fact- unique\n",
        "\n",
        "\n",
        "#image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "\n",
        "def get_existing_stories(image_embeddings):\n",
        "  #load all_captions file\n",
        "  with open(ALL_CAPTIONS_FILE) as json_file:\n",
        "    all_captions_dict = json.load(json_file)\n",
        "\n",
        "  #Create a story dict (no album ids (already checked that story ids are unique))\n",
        "  story_dict = {}\n",
        "  for key in list(all_captions_dict.keys()):\n",
        "    story_dict.update(all_captions_dict[key])\n",
        "\n",
        "\n",
        "  # Create a Story dict where all images are available in the image_embeddings\n",
        "  existing_stories = {}\n",
        "  c=0\n",
        "  for key in list(story_dict.keys()):\n",
        "    lists = story_dict[key]\n",
        "    images = [item[0] for item in lists]\n",
        "    #captions = ['startseq ' + item[1] + ' endseq' for item in lists]\n",
        "    captions = [item[1] for item in lists]\n",
        "    if all(img in list(image_embeddings.keys()) for img in images):\n",
        "      existing_stories[key] = [images,captions]\n",
        "      c+=1\n",
        "      \n",
        "  print(\"Number of Stories Found: \")\n",
        "  print(c)\n",
        "\n",
        "  # Saving the complete existing story dict in a file\n",
        "  with open(COMPLETE_STORIES_FILE, 'w') as fp:\n",
        "      json.dump(existing_stories, fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEKE0VJHSsU",
        "colab_type": "text"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16o6JbtNXHVa",
        "colab": {}
      },
      "source": [
        " def vocab_fun(existing_stories_dict):\n",
        "  index_to_word = {}\n",
        "  word_to_index = {}\n",
        "  max_seq_len=0\n",
        "  all_words = {}\n",
        "  all_words['startseq'] = 1\n",
        "  all_words['endseq'] = 1\n",
        "  cap_list = list()\n",
        "  for story_id, lists in existing_stories_dict.items():\n",
        "    for cap in lists[1]:\n",
        "      if(len(cap.split())>max_seq_len):\n",
        "        max_seq_len = len(cap.split())\n",
        "      for word in cap.split():\n",
        "        all_words[word] = 1\n",
        "  all_vocab=[w for w in all_words]\n",
        "  index = 1\n",
        "  for word in all_vocab:\n",
        "      word_to_index[word] = index\n",
        "      index_to_word[index] = word\n",
        "      index += 1\n",
        "\n",
        "\n",
        "\n",
        "  #MAX_SEQUENCE_LENGTH = #max_seq_len + 1\n",
        "  return (all_vocab, word_to_index, index_to_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2-PgnES0nhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #to deciede sentence length:\n",
        "\n",
        "# cap_lengths=[]\n",
        "# for key, lists in existing_stories.items():\n",
        "#     cap_list=lists[1]\n",
        "#     for x in cap_list:\n",
        "#       for c in x.split():\n",
        "#         cap_lengths.append(len(c))\n",
        "# print(np.mean(cap_lengths)+ 2 * np.std(cap_lengths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl15hHkcYxYj"
      },
      "source": [
        "#Preprocess images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90V-9vXXYjgx",
        "colab": {}
      },
      "source": [
        "def Merge(dict1, dict2): \n",
        "    res = {**dict1, **dict2} \n",
        "    return res \n",
        "\n",
        "def getImageEmbedding(path):\n",
        "    image_embedding = {}\n",
        "    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):\n",
        "         file_name = path + 'cnn_group'+str(i+1)+'.json'\n",
        "         with open(file_name) as json_file:\n",
        "#    with open(file_name) as json_file:\n",
        "            print(file_name)\n",
        "            json_data = json.load(json_file)\n",
        "            json_data = json.loads(json_data)\n",
        "            image_embedding = Merge(image_embedding, json_data) \n",
        "            #image_embedding = json_data \n",
        "    return image_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKPEQrk-FBz",
        "colab_type": "text"
      },
      "source": [
        "#Load Stories (captions with corresponding Image ids)\n",
        "##Dict items as follows (per story)\n",
        "[ [img_id1, img_id2, img_id3, img_id4, img_id5] , [cap1, cap2, cap3, cap4, cap5] ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsfYfloT9dz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_existing_stories_from_file():\n",
        "  with open(COMPLETE_STORIES_FILE, 'r') as fp:\n",
        "      existing_stories = json.load(fp)\n",
        "  return existing_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dJwBxEGaYNy"
      },
      "source": [
        "#Use Prev to get captions / stories and images and pre_process them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XMqm0aZXaYgo",
        "outputId": "bc3d5fe1-7da8-4691-b580-ff429209603f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#for training\n",
        "image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)\n",
        "print(len(image_embd))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group1.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group2.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group3.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group4.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group5.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group6.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group7.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group8.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group9.json\n",
            "/content/drive/My Drive/CNNFeatureVectors/cnn_group10.json\n",
            "58197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YljSlURBPJBU",
        "colab_type": "code",
        "outputId": "88e76d10-d431-4d7a-be01-f67c97b4c899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FZmz0inN6Z",
        "colab_type": "code",
        "outputId": "e3d8f349-1fa1-488b-fe63-6895f1f11be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#get existing_stories (either load from file or using a function , preferably load from file) -- uncomment one of the following 2 lines\n",
        "existing_stories = get_existing_stories_from_file()\n",
        "#existing_stories = get_existing_stories(image_embd) #Number of Stories Found: 35565\n",
        "\n",
        "all_vocab, wordtoix, ixtoword=vocab_fun(existing_stories)\n",
        "print('Max Seq Len: %d' %MAX_SEQUENCE_LENGTH)\n",
        "vocab_size = len(all_vocab) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size) #26571"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Seq Len: 12\n",
            "Vocabulary Size: 26572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8amlOYORaY6U"
      },
      "source": [
        "#Word Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d0TgQ6BKaZSA",
        "outputId": "3e6c8412-71fe-45bc-dd70-d6de851124e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get matrix embedding for glove\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(GLOVE_EMBEDDING_FILE_NAME, encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8fQm4YyfNix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #save embedding matrix:\n",
        "# np.save('/content/drive/My Drive/Colab_Notebooks/DL_data/embedding_matrix.npy', embedding_matrix) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m_4EtnggGhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #save dicts needed during inference:\n",
        "# with open('/content/drive/My Drive/Colab_Notebooks/DL_data/ixtoword.json', 'w') as fp:\n",
        "#     json.dump(ixtoword, fp)\n",
        "\n",
        "# with open('/content/drive/My Drive/Colab_Notebooks/DL_data/wordtoix.json', 'w') as fp:\n",
        "#     json.dump(wordtoix, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfq2bjxq_WW7",
        "colab_type": "text"
      },
      "source": [
        "#Input and output for the model\n",
        "###X1, X2, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8tkr49aQ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def all_data(stories_dict, image_embd, wordtoix, max_length, num_of_stories):\n",
        "  X1, X2, X3, y = list(), list(), list(), list()\n",
        "  #to generate X1,X2,X3 and y. ////append-> have a list of image embeds , have a list for curr caption, have sentences embed for previous sentences, next word\n",
        "  #for each story:\n",
        "  for key, lists in stories_dict.items():\n",
        "    #break after retreiving num_of_stories\n",
        "    if num_of_stories <= 0:\n",
        "      break\n",
        "    num_of_stories -= 1\n",
        "    \n",
        "    img_list=lists[0]\n",
        "    img_list_embed=[image_embd[img_id] for img_id in img_list]#[imgtoix[img_id] for img_id in img_list]\n",
        "    # prev_list=lists[1].copy()\n",
        "    # prev_list.pop()\n",
        "    # prev_list.insert(0,'')\n",
        "    in_cap_list = np.zeros((5,max_length))\n",
        "    out_cap_list = np.zeros((5,max_length))\n",
        "    for c in range(5):\n",
        "      #prev_embeddings = [wordtoix[word] for word in prev_list[c].split() if word in wordtoix]\n",
        "      #prev_list[c] = pad_sequences([prev_embeddings], maxlen=max_length)[0]\n",
        "\n",
        "      cap=lists[1][c]\n",
        "      cap_in = 'startseq ' + cap\n",
        "      cap_out = cap + ' endseq'\n",
        "      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]\n",
        "      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]\n",
        "      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      if len(cap_out.split()) > max_length:\n",
        "        seq_out[-1]=wordtoix['endseq']\n",
        "      in_cap_list[c] = np.array(seq_in)\n",
        "      out_cap_list[c] = np.array(seq_out)\n",
        "    X1.append(img_list_embed)\n",
        "    X2.append(in_cap_list)\n",
        "    #X3.append(prev_list)\n",
        "    y.append(np.expand_dims(np.concatenate(out_cap_list).ravel(), -1))##out_cap_list\n",
        "\n",
        "  return (array(X1), array(X2),  array(y))#array(X3),\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rfJRzcLME5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X1,X2,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 2)#35565\n",
        "\n",
        "# print(\"X\")\n",
        "# print(X2)\n",
        "# print(\"Y\")\n",
        "# print(y)\n",
        "# print(np.shape(X2))\n",
        "# print(np.shape(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y31ZtUjq0cle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"X\")\n",
        "# print(np.shape(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ZXWjtxxkMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X1,X2,X3,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 2)\n",
        "\n",
        "# print(\"X\")\n",
        "# print(X2)\n",
        "# print(\"Y\")\n",
        "# print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSm7P42Yy-Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(np.shape(y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_ORm98fwxE",
        "colab_type": "text"
      },
      "source": [
        "#Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoKMi9Cofzh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, X1, X2, y, batch_size=32, shuffle=True, vocab_size = 5000):\n",
        "        'Initialization'\n",
        "        self.X1=X1\n",
        "        self.X2=X2\n",
        "        self.y=y\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list(range(np.shape(X1)[0]))\n",
        "        self.shuffle = shuffle\n",
        "        self.vocab_size = vocab_size\n",
        "        self.on_epoch_end()\n",
        "        self.max_len = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "    def someFunction(self,list_IDs_temp):\n",
        "        return self.__data_generation(list_IDs_temp)\n",
        "    \n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        X1_batch,X2_batch, y_batch = self.__data_generation(list_IDs_temp)\n",
        "        return [X1_batch,X2_batch], y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' \n",
        "        X1_batch = np.empty((self.batch_size, 5, IMAGE_EMBEDDING_DIM))\n",
        "        X2_batch = np.empty((self.batch_size, 5, self.max_len))\n",
        "        y_batch = np.empty((self.batch_size, self.max_len*5, 1))\n",
        "\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "          X1_batch[i] = self.X1[ID]\n",
        "          X2_batch[i] = self.X2[ID]\n",
        "          y_batch[i] = self.y[ID]#np.expand_dims(self.y[ID].copy(), -1)# self.y[ID].copy()\n",
        "\n",
        "\n",
        "        return X1_batch, X2_batch, y_batch#keras.utils.to_categorical(y_batch, num_classes=self.vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqpTKSwhoILv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(X1.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvpLXOZ8n6nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training_generator = DataGenerator(X1,X2,y, vocab_size=vocab_size)\n",
        "\n",
        "# train, train_Y = training_generator.__getitem__(62)\n",
        "# train_X1, train_X2 = train\n",
        "# print(train_X1.shape)\n",
        "# print(train_X2.shape)\n",
        "# print(train_Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpo33OpBdU2L"
      },
      "source": [
        "#Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IaC1BgwxdZsM",
        "colab": {}
      },
      "source": [
        "# # Things I should try:\n",
        "# # 1- bidirectional\n",
        "# # 2- return sequences and merge the two layers\n",
        "# # 3- must get good results on validation (the model must be generalizable)\n",
        "# # 4- modify hyper parameters (hidden layers, batch size, learning rate) (with larger batch size we can use larger learning rate)\n",
        "\n",
        "\n",
        "from keras.regularizers import l2, l1\n",
        "from keras import backend as K\n",
        "\n",
        "def build_model():\n",
        "###slice = Lambda(lambda x: x[:, i])(input)\n",
        "    #Image Encoder\n",
        "    img_input_whole = Input(shape=(5,IMAGE_EMBEDDING_DIM))\n",
        "    img_input_compressed = TimeDistributed(Dense(640))(img_input_whole)\n",
        "    img_encoder = GRU(640, recurrent_dropout=0, return_sequences=True , activity_regularizer=l2(0.000))(img_input_compressed)\n",
        "    img1_enc = Lambda(lambda x: x[:, 0, :])(img_encoder)\n",
        "    img2_enc = Lambda(lambda x: x[:, 1, :])(img_encoder)\n",
        "    img3_enc = Lambda(lambda x: x[:, 2, :])(img_encoder)\n",
        "    img4_enc = Lambda(lambda x: x[:, 3, :])(img_encoder)\n",
        "    img5_enc = Lambda(lambda x: x[:, 4, :])(img_encoder)\n",
        "    \n",
        "    #Embed each:\n",
        "    Word_Embedder = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)\n",
        "\n",
        "    #define previouse sentences encoder: #512\n",
        "    prev_encoder = Bidirectional(GRU(1, recurrent_dropout=0, dropout=0.0 , return_sequences=False, activity_regularizer=l2(0.000)),  merge_mode='ave')\n",
        "\n",
        "    #Decoder: #512\n",
        "    decoder = GRU(641,recurrent_dropout=0, dropout=0.0 ,return_sequences=True ,activity_regularizer=l2(0.000))\n",
        "    decoder2 = GRU(641,recurrent_dropout=0, dropout=0.0 ,return_sequences=True ,activity_regularizer=l2(0.000))\n",
        "    #Current captions\n",
        "    captions_input = Input(shape=(5,MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "    #split\n",
        "    cap1_in = Lambda(lambda x: x[:, 0, :])(captions_input)\n",
        "    cap2_in = Lambda(lambda x: x[:, 1, :])(captions_input)\n",
        "    cap3_in = Lambda(lambda x: x[:, 2, :])(captions_input)\n",
        "    cap4_in = Lambda(lambda x: x[:, 3, :])(captions_input)\n",
        "    cap5_in = Lambda(lambda x: x[:, 4, :])(captions_input)\n",
        "    \n",
        "    #Embed each:\n",
        "    cap1_emb = Word_Embedder(cap1_in)\n",
        "    cap2_emb = Word_Embedder(cap2_in)\n",
        "    cap3_emb = Word_Embedder(cap3_in)\n",
        "    cap4_emb = Word_Embedder(cap4_in)\n",
        "    cap5_emb = Word_Embedder(cap5_in)\n",
        "\n",
        "    # Concat\n",
        "    #hidden_1 = concatenate([sent1_enc,img1_enc])\n",
        "\n",
        "    #Decode\n",
        "    paddings = tf.constant([[0,0],[0,1]])\n",
        "    padded_1_image = tf.pad(img1_enc, paddings, mode='CONSTANT',constant_values=1)\n",
        "    cap1_dec = decoder2(decoder(cap1_emb, initial_state = padded_1_image ))\n",
        "    cap1_dec = TimeDistributed(Dense(500))(cap1_dec)\n",
        "\n",
        "    prev_1 = prev_encoder(cap1_dec)\n",
        "    hidden_2 = concatenate([prev_1,img2_enc])\n",
        "    cap2_dec = decoder2(decoder(cap2_emb , initial_state = hidden_2))\n",
        "    cap2_dec = TimeDistributed(Dense(500))(cap2_dec)\n",
        "\n",
        "    prev_2 = prev_encoder(cap2_dec)\n",
        "    print(prev_2.shape)\n",
        "    hidden_3 = concatenate([prev_2,img3_enc])\n",
        "    cap3_dec = decoder2(decoder(cap3_emb , initial_state = hidden_3))\n",
        "    cap3_dec = TimeDistributed(Dense(500))(cap3_dec)\n",
        "\n",
        "    prev_3 = prev_encoder(cap3_dec)\n",
        "    hidden_4 = concatenate([prev_3,img4_enc])\n",
        "    cap4_dec = decoder2(decoder(cap4_emb , initial_state = hidden_4))\n",
        "    cap4_dec = TimeDistributed(Dense(500))(cap4_dec)\n",
        "\n",
        "    prev_4 = prev_encoder(cap4_dec)\n",
        "    hidden_5 = concatenate([prev_4,img5_enc])\n",
        "    cap5_dec = decoder2(decoder(cap5_emb , initial_state = hidden_5))\n",
        "    cap5_dec = TimeDistributed(Dense(500))(cap4_dec)\n",
        "\n",
        "    decoder_out = concatenate([cap1_dec, cap2_dec, cap3_dec, cap4_dec, cap5_dec], axis=-2)\n",
        "\n",
        "    decoder_dense = Dense(512, activation=None, kernel_regularizer=l2(0.000))(decoder_out)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder_dense) ##was softmax /// used linear because it's recommended with the custom loss: https://github.com/tensorflow/tensorflow/issues/17150\n",
        "    model = Model(inputs=[img_input_whole, captions_input], outputs=outputs)\n",
        "\n",
        "    model.summary()\n",
        "    #decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "    #model.compile(loss=sparse_cross_entropy, optimizer='rmsprop', target_tensors=[decoder_target])#'adam') #, target_tensors=[decoder_target]\n",
        "    #opt = optimizers.adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, clipnorm = 1.0, clipvalue = 0.5) #0.25 gradeint clipping internet says it's good for LSTM language models\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['sparse_categorical_accuracy']) #tf.losses.sparse_softmax_cross_entropy #'categorical_crossentropy'\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCFNBmPWdZ71"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9RFuY7wfx4t",
        "outputId": "f0f1a5d7-9b5f-44ef-f1a7-7ea7eebd605e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=build_model()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 5, 12)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 12)           0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 12, 300)      7971600     lambda_16[0][0]                  \n",
            "                                                                 lambda_17[0][0]                  \n",
            "                                                                 lambda_18[0][0]                  \n",
            "                                                                 lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_7 (GRU)                     (None, 12, 641)      1811466     embedding_2[0][0]                \n",
            "                                                                 embedding_2[1][0]                \n",
            "                                                                 concatenate_6[0][0]              \n",
            "                                                                 embedding_2[2][0]                \n",
            "                                                                 concatenate_7[0][0]              \n",
            "                                                                 embedding_2[3][0]                \n",
            "                                                                 concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 5, 2048)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (None, 12, 641)      2467209     gru_7[0][0]                      \n",
            "                                                                 gru_7[1][0]                      \n",
            "                                                                 gru_7[2][0]                      \n",
            "                                                                 gru_7[3][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_7 (TimeDistrib (None, 5, 640)       1311360     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 12, 500)      321000      gru_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_5 (GRU)                     (None, 5, 640)       2459520     time_distributed_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 12)           0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 1)            3012        time_distributed_8[0][0]         \n",
            "                                                                 time_distributed_9[0][0]         \n",
            "                                                                 time_distributed_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 640)          0           gru_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 641)          0           bidirectional_2[0][0]            \n",
            "                                                                 lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, 12, 500)      321000      gru_8[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 12)           0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 640)          0           gru_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 641)          0           bidirectional_2[1][0]            \n",
            "                                                                 lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_10 (TimeDistri (None, 12, 500)      321000      gru_8[2][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 12)           0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 640)          0           gru_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 641)          0           bidirectional_2[2][0]            \n",
            "                                                                 lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_11 (TimeDistri (None, 12, 500)      321000      gru_8[3][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_12 (TimeDistri (None, 12, 500)      250500      time_distributed_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 60, 500)      0           time_distributed_8[0][0]         \n",
            "                                                                 time_distributed_9[0][0]         \n",
            "                                                                 time_distributed_10[0][0]        \n",
            "                                                                 time_distributed_11[0][0]        \n",
            "                                                                 time_distributed_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 60, 512)      256512      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 60, 26572)    13631436    dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 31,446,615\n",
            "Trainable params: 23,475,015\n",
            "Non-trainable params: 7,971,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V620wYUxnhZh",
        "colab_type": "text"
      },
      "source": [
        "#Or load saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6PjcblnmDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = load_model('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_checkpoint-ep033-loss3.835.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGbmz4VFhb83",
        "colab_type": "text"
      },
      "source": [
        "#Prep for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR4FhSY2cW1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1,X2,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 1000)#35565\n",
        "#for GCP\n",
        "# filepath_checkpoint = \"final_model_all_stories-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "# filepath_model = 'final_model_all_stories_finale.h5'\n",
        "\n",
        "# # #for Colab\n",
        "# filepath_checkpoint = \"/content/drive/My Drive/Colab_Notebooks/DL_data/models/Suggested_10_-ep{epoch:03d}-loss{loss:.3f}.h5\"\n",
        "# filepath_model = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/Suggested_10_epoch1.h5'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXPVmFTlxkbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split = int(0.9*np.shape(X1)[0])\n",
        "training_generator = DataGenerator(X1[:split,:,:],X2[:split,:,:],y[:split,:], vocab_size=vocab_size, batch_size=BATCH_SIZE)\n",
        "validation_generator = DataGenerator(X1[split:,:,:],X2[split:,:,:],y[split:,:], vocab_size=vocab_size, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO8jb5bIMDMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# con_y = y.\n",
        "#weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnO5JdeKhhtS",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vHfOqvchi3S",
        "colab_type": "code",
        "outputId": "29e21f98-feb6-40ff-cf15-571ced66f5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "################### Increase batch size!  ##20:53\n",
        "################### LIMIT VOCAB to 5000, all other things will be UNK\n",
        "#checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "#history = model.fit(training_generator, epochs=5, verbose=1, shuffle=True, max_queue_size=5, workers=5, use_multiprocessing=True )#class_weight=weights #callbacks=[checkpoint] \n",
        "history = model.fit([X1,X2],y, epochs=20, verbose=1,batch_size = BATCH_SIZE, shuffle=True, max_queue_size=5, workers=10, use_multiprocessing=True, validation_split=0.1 ) \n",
        "# model.save('/content/drive/model_epoch10.h5')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 900 samples, validate on 100 samples\n",
            "Epoch 1/20\n",
            "900/900 [==============================] - 19s 21ms/step - loss: 8.2323 - sparse_categorical_accuracy: 0.0564 - val_loss: 6.3923 - val_sparse_categorical_accuracy: 0.0299\n",
            "Epoch 2/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 5.9869 - sparse_categorical_accuracy: 0.0970 - val_loss: 6.1256 - val_sparse_categorical_accuracy: 0.1252\n",
            "Epoch 3/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 5.7825 - sparse_categorical_accuracy: 0.1123 - val_loss: 6.2253 - val_sparse_categorical_accuracy: 0.0779\n",
            "Epoch 4/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 5.6076 - sparse_categorical_accuracy: 0.1146 - val_loss: 5.9472 - val_sparse_categorical_accuracy: 0.0924\n",
            "Epoch 5/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 5.4348 - sparse_categorical_accuracy: 0.1195 - val_loss: 5.6213 - val_sparse_categorical_accuracy: 0.1520\n",
            "Epoch 6/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 5.1739 - sparse_categorical_accuracy: 0.1467 - val_loss: 5.6774 - val_sparse_categorical_accuracy: 0.1560\n",
            "Epoch 7/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.9568 - sparse_categorical_accuracy: 0.1607 - val_loss: 5.6971 - val_sparse_categorical_accuracy: 0.1622\n",
            "Epoch 8/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.7876 - sparse_categorical_accuracy: 0.1763 - val_loss: 5.6262 - val_sparse_categorical_accuracy: 0.1455\n",
            "Epoch 9/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.6942 - sparse_categorical_accuracy: 0.1757 - val_loss: 5.4738 - val_sparse_categorical_accuracy: 0.1615\n",
            "Epoch 10/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.5051 - sparse_categorical_accuracy: 0.1870 - val_loss: 5.6381 - val_sparse_categorical_accuracy: 0.1573\n",
            "Epoch 11/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.4579 - sparse_categorical_accuracy: 0.1920 - val_loss: 5.4688 - val_sparse_categorical_accuracy: 0.1727\n",
            "Epoch 12/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.2821 - sparse_categorical_accuracy: 0.1996 - val_loss: 5.6166 - val_sparse_categorical_accuracy: 0.1763\n",
            "Epoch 13/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.1999 - sparse_categorical_accuracy: 0.2006 - val_loss: 5.6613 - val_sparse_categorical_accuracy: 0.1328\n",
            "Epoch 14/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 4.0504 - sparse_categorical_accuracy: 0.2085 - val_loss: 5.6938 - val_sparse_categorical_accuracy: 0.1461\n",
            "Epoch 15/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 3.9943 - sparse_categorical_accuracy: 0.2114 - val_loss: 5.5517 - val_sparse_categorical_accuracy: 0.1725\n",
            "Epoch 16/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 3.7904 - sparse_categorical_accuracy: 0.2275 - val_loss: 5.6978 - val_sparse_categorical_accuracy: 0.1604\n",
            "Epoch 17/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 3.7162 - sparse_categorical_accuracy: 0.2331 - val_loss: 5.6415 - val_sparse_categorical_accuracy: 0.1544\n",
            "Epoch 18/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 3.5744 - sparse_categorical_accuracy: 0.2504 - val_loss: 5.7112 - val_sparse_categorical_accuracy: 0.1740\n",
            "Epoch 19/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 3.4403 - sparse_categorical_accuracy: 0.2632 - val_loss: 5.7784 - val_sparse_categorical_accuracy: 0.1775\n",
            "Epoch 20/20\n",
            "900/900 [==============================] - 7s 8ms/step - loss: 3.3525 - sparse_categorical_accuracy: 0.2768 - val_loss: 5.8434 - val_sparse_categorical_accuracy: 0.1693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbj6I0CcV7Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save('/content/drive/My Drive/dldata/epoch20.h5')\n",
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puL_rviU8znu",
        "colab_type": "code",
        "outputId": "27e53621-5667-4680-d426-1acfb53b9437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit([X1,X2],y, epochs=50, verbose=1, batch_size = 16, shuffle=True, max_queue_size=5, workers=10, use_multiprocessing=True, validation_split=0.1 ) \n",
        "# model.save('/content/drive/My Drive/dldata/epoch270.h5')\n",
        "# model.save_weights('/content/drive/My Drive/dldata/model_epoch50_wt.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 900 samples, validate on 100 samples\n",
            "Epoch 1/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 3.4801 - sparse_categorical_accuracy: 0.2618 - val_loss: 5.5751 - val_sparse_categorical_accuracy: 0.1881\n",
            "Epoch 2/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 3.2742 - sparse_categorical_accuracy: 0.2938 - val_loss: 5.6972 - val_sparse_categorical_accuracy: 0.1783\n",
            "Epoch 3/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 3.0428 - sparse_categorical_accuracy: 0.3267 - val_loss: 5.8550 - val_sparse_categorical_accuracy: 0.1671\n",
            "Epoch 4/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 2.7686 - sparse_categorical_accuracy: 0.3749 - val_loss: 5.8750 - val_sparse_categorical_accuracy: 0.1692\n",
            "Epoch 5/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 2.4831 - sparse_categorical_accuracy: 0.4272 - val_loss: 6.0335 - val_sparse_categorical_accuracy: 0.1777\n",
            "Epoch 6/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 2.1890 - sparse_categorical_accuracy: 0.4920 - val_loss: 6.2850 - val_sparse_categorical_accuracy: 0.1586\n",
            "Epoch 7/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.9280 - sparse_categorical_accuracy: 0.5509 - val_loss: 6.5011 - val_sparse_categorical_accuracy: 0.1598\n",
            "Epoch 8/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 1.7043 - sparse_categorical_accuracy: 0.6101 - val_loss: 6.7343 - val_sparse_categorical_accuracy: 0.1727\n",
            "Epoch 9/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.5191 - sparse_categorical_accuracy: 0.6590 - val_loss: 7.0055 - val_sparse_categorical_accuracy: 0.1574\n",
            "Epoch 10/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.4003 - sparse_categorical_accuracy: 0.6926 - val_loss: 7.1741 - val_sparse_categorical_accuracy: 0.1696\n",
            "Epoch 11/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.3128 - sparse_categorical_accuracy: 0.7167 - val_loss: 7.3200 - val_sparse_categorical_accuracy: 0.1610\n",
            "Epoch 12/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.2463 - sparse_categorical_accuracy: 0.7352 - val_loss: 7.4565 - val_sparse_categorical_accuracy: 0.1650\n",
            "Epoch 13/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.2067 - sparse_categorical_accuracy: 0.7448 - val_loss: 7.5839 - val_sparse_categorical_accuracy: 0.1649\n",
            "Epoch 14/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.1747 - sparse_categorical_accuracy: 0.7541 - val_loss: 7.4754 - val_sparse_categorical_accuracy: 0.1605\n",
            "Epoch 15/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.1313 - sparse_categorical_accuracy: 0.7662 - val_loss: 7.5374 - val_sparse_categorical_accuracy: 0.1684\n",
            "Epoch 16/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.1064 - sparse_categorical_accuracy: 0.7689 - val_loss: 7.6113 - val_sparse_categorical_accuracy: 0.1756\n",
            "Epoch 17/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.0907 - sparse_categorical_accuracy: 0.7745 - val_loss: 7.6619 - val_sparse_categorical_accuracy: 0.1658\n",
            "Epoch 18/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.0744 - sparse_categorical_accuracy: 0.7783 - val_loss: 7.5975 - val_sparse_categorical_accuracy: 0.1734\n",
            "Epoch 19/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.0531 - sparse_categorical_accuracy: 0.7818 - val_loss: 7.6333 - val_sparse_categorical_accuracy: 0.1643\n",
            "Epoch 20/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.0457 - sparse_categorical_accuracy: 0.7826 - val_loss: 7.6094 - val_sparse_categorical_accuracy: 0.1631\n",
            "Epoch 21/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.0197 - sparse_categorical_accuracy: 0.7889 - val_loss: 7.7186 - val_sparse_categorical_accuracy: 0.1582\n",
            "Epoch 22/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 1.0089 - sparse_categorical_accuracy: 0.7895 - val_loss: 7.6568 - val_sparse_categorical_accuracy: 0.1738\n",
            "Epoch 23/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9997 - sparse_categorical_accuracy: 0.7905 - val_loss: 7.5692 - val_sparse_categorical_accuracy: 0.1690\n",
            "Epoch 24/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9805 - sparse_categorical_accuracy: 0.7935 - val_loss: 7.6790 - val_sparse_categorical_accuracy: 0.1705\n",
            "Epoch 25/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9688 - sparse_categorical_accuracy: 0.7964 - val_loss: 7.7408 - val_sparse_categorical_accuracy: 0.1722\n",
            "Epoch 26/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9558 - sparse_categorical_accuracy: 0.7988 - val_loss: 7.7929 - val_sparse_categorical_accuracy: 0.1662\n",
            "Epoch 27/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9453 - sparse_categorical_accuracy: 0.8007 - val_loss: 7.7686 - val_sparse_categorical_accuracy: 0.1632\n",
            "Epoch 28/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9297 - sparse_categorical_accuracy: 0.8028 - val_loss: 7.9448 - val_sparse_categorical_accuracy: 0.1693\n",
            "Epoch 29/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.9195 - sparse_categorical_accuracy: 0.8023 - val_loss: 7.8420 - val_sparse_categorical_accuracy: 0.1582\n",
            "Epoch 30/50\n",
            "900/900 [==============================] - 22s 24ms/step - loss: 0.8996 - sparse_categorical_accuracy: 0.8060 - val_loss: 7.9502 - val_sparse_categorical_accuracy: 0.1648\n",
            "Epoch 31/50\n",
            "900/900 [==============================] - 22s 24ms/step - loss: 0.8833 - sparse_categorical_accuracy: 0.8093 - val_loss: 7.8405 - val_sparse_categorical_accuracy: 0.1692\n",
            "Epoch 32/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.8689 - sparse_categorical_accuracy: 0.8099 - val_loss: 7.9965 - val_sparse_categorical_accuracy: 0.1666\n",
            "Epoch 33/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.8540 - sparse_categorical_accuracy: 0.8132 - val_loss: 7.8609 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 34/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.8398 - sparse_categorical_accuracy: 0.8159 - val_loss: 7.9824 - val_sparse_categorical_accuracy: 0.1652\n",
            "Epoch 35/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.8296 - sparse_categorical_accuracy: 0.8150 - val_loss: 7.9197 - val_sparse_categorical_accuracy: 0.1651\n",
            "Epoch 36/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.7960 - sparse_categorical_accuracy: 0.8206 - val_loss: 8.0591 - val_sparse_categorical_accuracy: 0.1633\n",
            "Epoch 37/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.7839 - sparse_categorical_accuracy: 0.8226 - val_loss: 8.0239 - val_sparse_categorical_accuracy: 0.1743\n",
            "Epoch 38/50\n",
            "900/900 [==============================] - 22s 24ms/step - loss: 0.7752 - sparse_categorical_accuracy: 0.8226 - val_loss: 8.1224 - val_sparse_categorical_accuracy: 0.1662\n",
            "Epoch 39/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.7559 - sparse_categorical_accuracy: 0.8259 - val_loss: 8.2139 - val_sparse_categorical_accuracy: 0.1592\n",
            "Epoch 40/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.7378 - sparse_categorical_accuracy: 0.8284 - val_loss: 8.2085 - val_sparse_categorical_accuracy: 0.1704\n",
            "Epoch 41/50\n",
            "900/900 [==============================] - 22s 24ms/step - loss: 0.7314 - sparse_categorical_accuracy: 0.8290 - val_loss: 8.3038 - val_sparse_categorical_accuracy: 0.1652\n",
            "Epoch 42/50\n",
            "900/900 [==============================] - 22s 24ms/step - loss: 0.7077 - sparse_categorical_accuracy: 0.8328 - val_loss: 8.3897 - val_sparse_categorical_accuracy: 0.1625\n",
            "Epoch 43/50\n",
            "900/900 [==============================] - 22s 24ms/step - loss: 0.6984 - sparse_categorical_accuracy: 0.8362 - val_loss: 8.3666 - val_sparse_categorical_accuracy: 0.1618\n",
            "Epoch 44/50\n",
            "900/900 [==============================] - 22s 25ms/step - loss: 0.6848 - sparse_categorical_accuracy: 0.8365 - val_loss: 8.4158 - val_sparse_categorical_accuracy: 0.1728\n",
            "Epoch 45/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 0.6704 - sparse_categorical_accuracy: 0.8413 - val_loss: 8.4724 - val_sparse_categorical_accuracy: 0.1676\n",
            "Epoch 46/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 0.6574 - sparse_categorical_accuracy: 0.8413 - val_loss: 8.5111 - val_sparse_categorical_accuracy: 0.1617\n",
            "Epoch 47/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 0.6391 - sparse_categorical_accuracy: 0.8452 - val_loss: 8.5855 - val_sparse_categorical_accuracy: 0.1629\n",
            "Epoch 48/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 0.6294 - sparse_categorical_accuracy: 0.8494 - val_loss: 8.6300 - val_sparse_categorical_accuracy: 0.1611\n",
            "Epoch 49/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 0.6201 - sparse_categorical_accuracy: 0.8492 - val_loss: 8.6049 - val_sparse_categorical_accuracy: 0.1642\n",
            "Epoch 50/50\n",
            "900/900 [==============================] - 23s 25ms/step - loss: 0.6062 - sparse_categorical_accuracy: 0.8515 - val_loss: 8.7421 - val_sparse_categorical_accuracy: 0.1643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8wXFc4zEi0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/dldata/pretty_ok_final.h5')\n",
        "model.save_weights('/content/drive/My Drive/dldata/pretty_ok_final_wt.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FoLOOVLH8Qg",
        "colab_type": "text"
      },
      "source": [
        "#Plot loss curve\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fu6qxITIplR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fsSx4SPhK1b",
        "colab_type": "code",
        "outputId": "030a7d02-1c55-45ee-d5bc-e728324cc3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "x1 = X1[0].copy()\n",
        "x2 = X2[0].copy()\n",
        "y_ = np.concatenate(y[0].copy()).ravel()\n",
        "tmp=''\n",
        "for w in y_:\n",
        "  if w in ixtoword:\n",
        "    if w == 'endseq': continue\n",
        "    tmp += ' ' + ixtoword[w]\n",
        "print(tmp)\n",
        "\n",
        "yhat = model.predict([x1[np.newaxis,...],x2[np.newaxis,...]], verbose=0)\n",
        "print(\"yhat\")\n",
        "print(np.shape(yhat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " my trip to location was amazing . endseq i saw some colorful people . endseq i even made some knew friends . endseq i visited a beautiful pagoda . endseq then i saw a ordinary bicycle . endseq\n",
            "yhat\n",
            "(1, 100, 26572)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Kz14jsimQV",
        "colab_type": "code",
        "outputId": "82176bd9-1e2d-417a-d033-e814e038c62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "yhat_s = yhat[0,:,:]\n",
        "# out_x=''\n",
        "# for j in range(MAX_SEQUENCE_LENGTH*5): \n",
        "#     yhat_s = yhat[0,j,:]\n",
        "#     cur_index_tmp = sample(yhat_s)\n",
        "#     out_x += ' '+ixtoword[cur_index_tmp]\n",
        "# print(out_x)\n",
        "\n",
        "print(yhat_s.shape)\n",
        "print(np.argmax(yhat_s, axis=-1))\n",
        "tmp=''\n",
        "for w in np.argmax(yhat_s, axis=-1):\n",
        "  if w in ixtoword:\n",
        "    if w == 2: continue #endseq\n",
        "    tmp += ' ' + ixtoword[w]\n",
        "print(tmp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 26572)\n",
            "[169 214   5   6   7   8   9   2   2   2   2   2   2   2   2   2   2   2\n",
            "   2   2  10 122  12  13  14   9   2   2   2   2   2   2   2   2   2   2\n",
            "   2   2   2   2  45 251  16  12  17  18   9   2   2   2   2   2   2   2\n",
            "   2   2   2   2   2   2  31   7  20  21  22   9   2   2   2   2   2   2\n",
            "   2   2   2   2   2   2   2   2 424   7  11  20  24  25   9   9   9   9\n",
            "   9   9   9   9   9   9   9   9   9   9]\n",
            " today first to location was amazing . i am some colorful people . and love made some knew friends . the was a beautiful pagoda . how was saw a ordinary bicycle . . . . . . . . . . . . . .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgcEVNMR8x5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_range_data(stories_dict, image_embd, wordtoix, max_length, start, end):\n",
        "  X1, X2, X3, y = list(), list(), list(), list()\n",
        "  #to generate X1,X2,X3 and y. ////append-> have a list of image embeds , have a list for curr caption, have sentences embed for previous sentences, next word\n",
        "  #for each story:\n",
        "\n",
        "  story_keys = list(stories_dict.keys())[start:end]\n",
        "  # for key, lists in stories_dict.items():\n",
        "  for key in story_keys:\n",
        "    lists = stories_dict[key]\n",
        "    img_list=lists[0]\n",
        "    img_list_embed=[image_embd[img_id] for img_id in img_list]#[imgtoix[img_id] for img_id in img_list]\n",
        "\n",
        "    in_cap_list = np.zeros((5,max_length))\n",
        "    out_cap_list = np.zeros((5,max_length))\n",
        "    for c in range(5):\n",
        "      cap=lists[1][c]\n",
        "      cap_in = 'startseq ' + cap\n",
        "      cap_out = cap + ' endseq'\n",
        "      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]\n",
        "      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]\n",
        "      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "      if len(cap_out.split()) > max_length:\n",
        "        seq_out[-1]=wordtoix['endseq']\n",
        "      in_cap_list[c] = np.array(seq_in)\n",
        "      out_cap_list[c] = np.array(seq_out)\n",
        "    X1.append(img_list_embed)\n",
        "    X2.append(in_cap_list)\n",
        "    #X3.append(prev_list)\n",
        "    y.append(np.expand_dims(np.concatenate(out_cap_list).ravel(), -1))##out_cap_list\n",
        "\n",
        "  return (array(X1), array(X2),  array(y))#array(X3),"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9NfWPSp-iyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1,X2,y = get_range_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 2000, 2050)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR7eneAV-Fos",
        "colab_type": "text"
      },
      "source": [
        "#TRY validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfzGfkAc-Iub",
        "colab_type": "code",
        "outputId": "663e7147-dae1-42f7-85db-e1ae751d6c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import shift\n",
        "#X1: 1x5x2048 --images\n",
        "#X2: 1x5x20  --captions\n",
        "index_v = 6\n",
        "from random import choices\n",
        "# items = range(1, 999)\n",
        "# new_items = choices(items, k = 20)\n",
        "for i in range(50):\n",
        "  index_v = i\n",
        "  x1_v = X1[index_v].copy()\n",
        "  x2_v = X2[index_v].copy()\n",
        "\n",
        "  x2_v_in = np.zeros_like(x2_v)\n",
        "  x2_v_out = np.zeros_like(x2_v)\n",
        "  for i in range(5):\n",
        "    x2_v_in[i][0] = wordtoix['startseq']\n",
        "    for j in range(MAX_SEQUENCE_LENGTH): \n",
        "      yhat = model.predict([x1_v[np.newaxis,...],x2_v_in[np.newaxis,...]], verbose=0)\n",
        "      yhat_s = yhat[0,(i*MAX_SEQUENCE_LENGTH)+j,:]\n",
        "      cur_index_tmp = np.argmax(yhat_s)\n",
        "      #cur_index_tmp = np.random.choice(len(yhat_s), p=yhat_s)\n",
        "      #cur_index_tmp = sample(yhat_s)\n",
        "      x2_v_out[i][j] = cur_index_tmp\n",
        "      if cur_index_tmp == 0 or cur_index_tmp == 2: break ##reached endseq\n",
        "      if j+1<MAX_SEQUENCE_LENGTH:\n",
        "        x2_v_in[i][j+1] = cur_index_tmp\n",
        "      \n",
        "      \n",
        "\n",
        "  caps=''\n",
        "  here = np.concatenate(x2_v_out).ravel()\n",
        "  for w in here:\n",
        "    if w in ixtoword:\n",
        "      if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue\n",
        "      caps += \" \" + ixtoword[w]\n",
        "  # print(\"Captions:\")\n",
        "  print(caps)\n",
        "\n",
        "  # print('Original X2')\n",
        "  or_f = X2[index_v].copy()\n",
        "  or_f = np.concatenate(or_f).ravel()\n",
        "\n",
        "  o=''\n",
        "  for w in or_f:\n",
        "    if w in ixtoword:\n",
        "      if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue\n",
        "      o += \" \"+ ixtoword[w]\n",
        "  # print(\"Original\") #X3[1]\n",
        "  print(\"-------Original-----\"+o)\n",
        "# the family went to the beach to go fishing . they had a lot of fun activities . they had a lot of fun activities . they had a great time . they had a great time .\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the ponds were a beautiful sight to behold . the stay here was full of natural beauty and wonderful wonderful wonderful\n",
            "-------Original----- a couple went to the beach together . their friend was a third wheel . they had a photographer there to take photos of them . they brought a raft to go out in the water on their friend remained alone under the shade .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the ponds were a beautiful sight to behold . the stay here was full of natural beauty and wonderful wonderful wonderful\n",
            "-------Original----- we 're here . this little boat survived bringing all of how long do i have to stay this way . i wonderful day for a beach shoot . i love my job what do you think of this pose ? i do n't i could be here all day getting my picture taken .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the ponds were a beautiful sight to behold . the stay here was full of natural beauty and wonderful wonderful wonderful\n",
            "-------Original----- a day at the beach our inflatable boat awaits . i tried to stay cool in the water . my friend waded in the shallow water . the cabana was behind me . a pretty girl modeled for the camera man\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the ponds were a beautiful sight to behold . the stay here was full of natural beauty and wonderful wonderful wonderful\n",
            "-------Original----- everyone was excited when we landed at the beach . [male] was having fun relaxing near the water . [female] and [male] were swimming out in the crystal clear water [female] and [male] decided to relax in the shade but [male] we had a great time at the beach that day .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the game is finally over and sun is looking over as\n",
            "-------Original----- day 1 of the alaskan photography expedition . day 2 visiting the penguin reef , no penguins in sight day 3 , a wonderful adventure to visit live active volcano morning day 4 , sight seen while the photography club was last day of the trip . this photo ranked 1 from\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the game is finally over and sun is looking over as\n",
            "-------Original----- i love to visit any body of water there are sandy also rocky beaches . some beaches are surrounded by greenery . others are surrounded by weeds . but the most beautiful is the one with the mountain view\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the game is finally over and sun is looking over as\n",
            "-------Original----- i went to the beach last weekend . i could see some boats out on the ocean . there were some mountains in the distance . i had a great time there . i felt like going hiking on those mountains .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the game is finally over and sun is looking over as\n",
            "-------Original----- a nice sunny day in next to the water . the shore was beautiful but foreboding . not much grows near the water . looking across the bay the sun shown in the daylight . snow capped mountains made you understand how cold it was there\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the game is finally over and sun is looking over as\n",
            "-------Original----- i went to visit a lake on the weekend . the landscape had a lot of rocks on it . there were also lots of green areas . these plants delicately lined the shore . there were snow capped mountains in the distance that i hope\n",
            " the graduation is taking place at location location location . the crowd was sparse but enthusiastic . they clapped and cheered it is so cold he had to carry the dog over the bridge itself was a sight to behold . i would into the city , and saw some of of of\n",
            "-------Original----- i went down to the water earlier . i met a raccoon along the way . there were some birds flying overhead . i had a great time there . it was very beautiful .\n",
            " what we need to know before we swim . location location , what a beauty . the book arrives in a box , neatly sealed . this one just likes to float . he floated all over i left watch red\n",
            "-------Original----- the sunset was so beautiful , that i stayed long after this was the largest she 'll that i had ever seem i was shocked that this raccoon sat still for me to this eagle soared high , and just looked to transcend the this shell seemed to be there for a very long time\n",
            " the graduation is taking place at location location location . the crowd was sparse but enthusiastic . they clapped and cheered it is so cold he had to carry the dog over the bridge itself was a sight to behold . i would into the city , and saw some of of of\n",
            "-------Original----- is that a sea urchin ? who knows ? i was distracted by a raccoon . and then this pretty bird . it made me think how neat it would be to fly i wish i was a bird .\n",
            " the graduation is taking place at location location location . the crowd was sparse but enthusiastic . they clapped and cheered it is so cold he had to carry the dog over the bridge itself was a sight to behold . i would into the city , and saw some of of of\n",
            "-------Original----- looking in the water i come across this thing . i i do know this is a raccoon and they love to spotted an eagle flying free . i 'm lucky to get these pictures they are hard to one last one before it flies off .\n",
            " the graduation is taking place at location location location . the crowd was sparse but enthusiastic . they clapped and cheered it is so cold he had to carry the dog over the bridge itself was a sight to behold . i would into the city , and saw some of of of\n",
            "-------Original----- the ocean is awakening for the day . in a nearby grassy patch , a young raccoon awakes . birds begin to fly to fetch food for their families . off he goes , hopeful of finding something . otherwise his family will be hungry .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the fireworks is finally over and sun is looking over as\n",
            "-------Original----- the pier is great for fishing . there are lots of great hotels at the beach . sand art fish were sculpted . is that location ? location guards the beach .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the fire was enchanting as it grew . the fireworks so enchanting we decided over light fireworks fireworks fireworks fireworks\n",
            "-------Original----- there were not many people at the beach today . i went for a short walk . i like the buildings on the side of the beach . there were some impressive sand sculptures . they were very good .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the fireworks is finally over and sun is looking over as\n",
            "-------Original----- the pier next to ocean to see the view . the buildings close to ocean near the sand . a sand made dolphin next to the ocean . more sand made people in the sand made . an a statue standing with a iguana on it .\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the fireworks is finally over and sun is looking over as\n",
            "-------Original----- we went to the beach to see the sandcastle competition . it was a nice day at the beach even though it the first sandcastle was this beautiful fish with nice texture and while not as nice as the fish , this sandcastle showed compared to the previous sandcastle , this version of poseidon was\n",
            " what we need to know before we swim . outside the sights are nice like a large white ferris wheel the streets were welcome to new visitors . the game is in full swing as the action makes johns the fireworks is finally over and sun is looking over as\n",
            "-------Original----- we went down near the pier one evening . we walked along the beach . there were some really attractive statues . this sandcastle was amazing ! i think this could have been location .\n",
            " the scenery was breathtaking . the family enjoyed playing in the pool together . the statue of [female] in location location location . the bridge towered above us lighting up the dark sky . the fireworks to row back home and pass by the old\n",
            "-------Original----- it was a long drive to the party . there were many christmas decorations . the sky was very clear . i had a great time with my friends . when we got back to the apartment we were very tired\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYQYernp0wW",
        "colab_type": "code",
        "outputId": "feb87564-e401-4f72-d5e2-5eca28900153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(x2_v_out)\n",
        "print('---------')\n",
        "print(x2_v_in)\n",
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.  4.  5.  6.  7.  8.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [10. 11. 12. 13. 14.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [10. 15. 16. 12. 17. 18.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [10. 19. 20. 21. 22.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [23. 10. 11. 20. 24. 25.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]]\n",
            "---------\n",
            "[[ 1.  3.  4.  5.  6.  7.  8.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [ 1. 10. 11. 12. 13. 14.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [ 1. 10. 15. 16. 12. 17. 18.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [ 1. 10. 19. 20. 21. 22.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]\n",
            " [ 1. 23. 10. 11. 20. 24. 25.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.]]\n",
            "[ 3.  4.  5.  6.  7.  8.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0. 10. 11. 12. 13. 14.  9.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0. 10. 15. 16. 12. 17. 18.  9.  2.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0. 10. 19. 20. 21. 22.  9.  2.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0. 23. 10. 11. 20. 24. 25.  9.  2.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IWWkFQDqdij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save weights:\n",
        "model.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_all_weights_sure.h5')\n",
        "model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_all_stories_finale_pad_sure.h5')\n",
        "##model2.load_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiNcLTa2vorh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath_model_load = '/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_finale.h5'\n",
        "#sen3_layer\n",
        "# model_loaded = load_model(filepath_model_load, custom_objects={'sparse_cross_entropy': tf.nn.softmax_cross_entropy_with_logits})\n",
        "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "weights = model.get_weights()\n",
        "\n",
        "for name, weight in zip(names, weights):\n",
        "    print(name, weight)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
