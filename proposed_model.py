# -*- coding: utf-8 -*-
"""Proposed Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HT19-gJ6jLGKDLLgvU_-M-iFe46H1M1G
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from numpy import array
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import string
import os
from PIL import Image
import glob
from pickle import dump, load
from time import time
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\
                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, GRU, Masking, Lambda, Concatenate, Average, Reshape
from keras.optimizers import Adam, RMSprop
from keras.layers.wrappers import Bidirectional
from keras.layers.merge import add
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.models import Model
from keras import Input, layers
from keras import optimizers
from keras.applications.inception_v3 import preprocess_input
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import load_model
import json
import csv

from keras.optimizers import SGD
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
import tensorflow as tf
tf.test.gpu_device_name()

# Change to corresponding links
ALL_CAPTIONS_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/all_captions.txt'
COMPLETE_STORIES_FILE = '/content/drive/My Drive/Colab_Notebooks/DL_data/complete_stories_all_splits.json'
IMAGE_EMBEDDING_DIR = '/content/drive/My Drive/Colab_Notebooks/DL_data/CNNFeatureVectors/'
GLOVE_EMBEDDING_FILE_NAME = '/content/drive/My Drive/Colab_Notebooks/DL_data/glove.6B.300d.txt'

SEED = 10
IMAGE_EMBEDDING_DIM = 2048
NUM_IMAGE_EMBEDDING_CHUNKS = 10
MAX_SEQUENCE_LENGTH = 20
WORD_EMBEDDING_DIM = 300
SENTENCE_EMBEDDING_DIM = 512

# #commented for GCP
# from google.colab import drive
# drive.mount('/content/drive')

"""PreProcess Captions / Stories

Either call this function or simply load preprocessed from a file
"""

#to check if story ids are repeated or unique
# story_list = list()
# for key in list(all_captions_dict.keys()):
#   story_list += list(all_captions_dict[key].keys())
# from collections import Counter
# print(len(story_dict))
# d =  Counter(story_dict)
# res = [k for k, v in d.items() if v > 1]
# print(len(res)) ## gave zero .. so story ids are -in fact- unique


#image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)

def get_existing_stories(image_embeddings):
  #load all_captions file
  with open(ALL_CAPTIONS_FILE) as json_file:
    all_captions_dict = json.load(json_file)

  #Create a story dict (no album ids (already checked that story ids are unique))
  story_dict = {}
  for key in list(all_captions_dict.keys()):
    story_dict.update(all_captions_dict[key])


  # Create a Story dict where all images are available in the image_embeddings
  existing_stories = {}
  c=0
  for key in list(story_dict.keys()):
    lists = story_dict[key]
    images = [item[0] for item in lists]
    #captions = ['startseq ' + item[1] + ' endseq' for item in lists]
    captions = [item[1] for item in lists]
    if all(img in list(image_embeddings.keys()) for img in images):
      existing_stories[key] = [images,captions]
      c+=1
      
  print("Number of Stories Found: ")
  print(c)

  # Saving the complete existing story dict in a file
  with open(COMPLETE_STORIES_FILE, 'w') as fp:
      json.dump(existing_stories, fp)
  return existing_stories

"""Vocab"""

def vocab_fun(existing_stories_dict):
  index_to_word = {}
  word_to_index = {}
  max_seq_len=0
  all_words = {}
  all_words['startseq'] = 1
  all_words['endseq'] = 1
  cap_list = list()
  for story_id, lists in existing_stories_dict.items():
    for cap in lists[1]:
      if(len(cap.split())>max_seq_len):
        max_seq_len = len(cap.split())
      for word in cap.split():
        all_words[word] = all_words.get(word, 0) + 1
  all_vocab=[w for w in all_words]
  index = 1
  for word in all_vocab:
      word_to_index[word] = index
      index_to_word[index] = word
      index += 1

  #MAX_SEQUENCE_LENGTH = #max_seq_len + 1
  return (all_vocab, word_to_index, index_to_word)

# #to deciede sentence length:

# cap_lengths=[]
# for key, lists in existing_stories.items():
#     cap_list=lists[1]
#     for x in cap_list:
#       for c in x.split():
#         cap_lengths.append(len(c))
# print(np.mean(cap_lengths)+ 2 * np.std(cap_lengths))

"""Preprocess images"""

def Merge(dict1, dict2): 
    res = {**dict1, **dict2} 
    return res 

def getImageEmbedding(path):
    image_embedding = {}
    for i in range(NUM_IMAGE_EMBEDDING_CHUNKS):
         file_name = path + 'cnn_group'+str(i+1)+'.json'
         with open(file_name) as json_file:
            print(file_name)
            json_data = json.load(json_file)
            json_data = json.loads(json_data)
            image_embedding = Merge(image_embedding, json_data) 
    return image_embedding

"""Load Stories (captions with corresponding Image ids)
Dict items as follows (per story)
[ [img_id1, img_id2, img_id3, img_id4, img_id5] , [cap1, cap2, cap3, cap4, cap5] ]
"""

def get_existing_stories_from_file():
  with open(COMPLETE_STORIES_FILE, 'r') as fp:
      existing_stories = json.load(fp)
  return existing_stories

"""Use Prev to get captions / stories and images and pre_process them"""

#for training
image_embd =  getImageEmbedding(IMAGE_EMBEDDING_DIR)
print(len(image_embd))

#get existing_stories (either load from file or using a function , preferably load from file) -- uncomment one of the following 2 lines
existing_stories = get_existing_stories_from_file()
#existing_stories = get_existing_stories(image_embd) #Number of Stories Found: 35565

all_vocab, wordtoix, ixtoword=vocab_fun(existing_stories)
print('Max Seq Len: %d' %MAX_SEQUENCE_LENGTH)
vocab_size = len(all_vocab) + 1
print('Vocabulary Size: %d' % vocab_size) #26571

"""Word Embedding Matrix"""

#get matrix embedding for glove
embeddings_index = {} # empty dictionary
f = open(GLOVE_EMBEDDING_FILE_NAME, encoding="utf-8")

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))


# Get 300-dim dense vector for each of the 10000 words in out vocabulary
embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))
for word, i in wordtoix.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in the embedding index will be all zeros
        embedding_matrix[i] = embedding_vector

"""Input and output for the model
X1, X2, y
"""

import tensorflow as tf
import tensorflow_hub as hub
def all_data(stories_dict, image_embd, wordtoix, max_length, num_of_stories):
  X1, X2, X3, y = list(), list(), list(), list()
  #to generate X1,X2 and y.
  #for each story:
  for key, lists in stories_dict.items():
    #break after retreiving num_of_stories
    if num_of_stories <= 0:
      break
    num_of_stories -= 1
    
    img_list=lists[0]
    img_list_embed=[image_embd[img_id] for img_id in img_list]

    in_cap_list = np.zeros((5,max_length))
    out_cap_list = np.zeros((5,max_length))
    for c in range(5):
      cap=lists[1][c]
      cap_in = 'startseq ' + cap
      cap_out = cap + ' endseq'
      seq_in = [wordtoix[word] for word in cap_in.split() if word in wordtoix]
      seq_in = pad_sequences([seq_in], maxlen=max_length, padding='post', truncating='post')[0]
      seq_out = [wordtoix[word] for word in cap_out.split() if word in wordtoix]
      seq_out = pad_sequences([seq_out], maxlen=max_length, padding='post', truncating='post')[0]
      if len(cap_out.split()) > max_length:
        seq_out[-1]=wordtoix['endseq']
      in_cap_list[c] = np.array(seq_in)
      out_cap_list[c] = np.array(seq_out)
    X1.append(img_list_embed)
    X2.append(in_cap_list)
    y.append(np.expand_dims(np.concatenate(out_cap_list).ravel(), -1))##out_cap_list

  return (array(X1), array(X2),  array(y))

"""Generator"""

import keras
class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, X1, X2, y, batch_size=32, shuffle=True, vocab_size = 5000):
        'Initialization'
        self.X1=X1
        self.X2=X2
        self.y=y
        self.batch_size = batch_size
        self.list_IDs = list(range(np.shape(X1)[0]))
        self.shuffle = shuffle
        self.vocab_size = vocab_size
        self.on_epoch_end()
        self.max_len = MAX_SEQUENCE_LENGTH

    def someFunction(self,list_IDs_temp):
        return self.__data_generation(list_IDs_temp)
    
    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        X1_batch,X2_batch, y_batch = self.__data_generation(list_IDs_temp)
        return [X1_batch,X2_batch], y_batch

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' 
        X1_batch = np.empty((self.batch_size, 5, IMAGE_EMBEDDING_DIM))
        X2_batch = np.empty((self.batch_size, 5, self.max_len))
        y_batch = np.empty((self.batch_size, self.max_len*5, 1))

        for i, ID in enumerate(list_IDs_temp):
          X1_batch[i] = self.X1[ID]
          X2_batch[i] = self.X2[ID]
          y_batch[i] = self.y[ID]#np.expand_dims(self.y[ID].copy(), -1)# self.y[ID].copy()


        return X1_batch, X2_batch, y_batch#keras.utils.to_categorical(y_batch, num_classes=self.vocab_size)

"""Building Model"""

from keras.regularizers import l2, l1
from keras import backend as K

def build_model():
    #Image Encoder
    img_hidden = 1024
    prev_gru = 1024
    prev_hidden = 1024
    cap_dense = 1024

    img_input = Input(shape=(5,IMAGE_EMBEDDING_DIM))
    img_input_dense = TimeDistributed(Dense(img_hidden))(img_input)
    img_encoder = GRU(img_hidden, recurrent_dropout=0, return_sequences=True , activity_regularizer=l2(0.000))(img_input_dense)
    img1_enc = Lambda(lambda x: x[:, 0, :])(img_encoder)
    img2_enc = Lambda(lambda x: x[:, 1, :])(img_encoder)
    img3_enc = Lambda(lambda x: x[:, 2, :])(img_encoder)
    img4_enc = Lambda(lambda x: x[:, 3, :])(img_encoder)
    img5_enc = Lambda(lambda x: x[:, 4, :])(img_encoder)
    
    #Word Embedder:
    Word_Embedder = Embedding(vocab_size, WORD_EMBEDDING_DIM, mask_zero=True, weights=[embedding_matrix], trainable=False)

    #define previouse sentences encoder:
    prev_encoder = Bidirectional(GRU(prev_gru, recurrent_dropout=0, dropout=0.0 , return_sequences=False, activity_regularizer=l2(0.000)),  merge_mode='ave')
    prev_dense = Dense(prev_hidden)
    #Decoder:
    decoder = GRU(prev_hidden+img_hidden,recurrent_dropout=0, dropout=0.0 ,return_sequences=True ,activity_regularizer=l2(0.000))
    
    #Current captions
    captions_input = Input(shape=(5,MAX_SEQUENCE_LENGTH))

    #split
    cap1_in = Lambda(lambda x: x[:, 0, :])(captions_input)
    cap2_in = Lambda(lambda x: x[:, 1, :])(captions_input)
    cap3_in = Lambda(lambda x: x[:, 2, :])(captions_input)
    cap4_in = Lambda(lambda x: x[:, 3, :])(captions_input)
    cap5_in = Lambda(lambda x: x[:, 4, :])(captions_input)
    
    #Embed each:
    cap_emb_dense = Dense(cap_dense)
    cap1_emb = cap_emb_dense(Word_Embedder(cap1_in))
    cap2_emb = cap_emb_dense(Word_Embedder(cap2_in))
    cap3_emb = cap_emb_dense(Word_Embedder(cap3_in))
    cap4_emb = cap_emb_dense(Word_Embedder(cap4_in))
    cap5_emb = cap_emb_dense(Word_Embedder(cap5_in))

    # Concat
    #hidden_1 = concatenate([sent1_enc,img1_enc])

    #Decode
    #paddings = tf.constant([[0,0],[0,prev_hidden]])
    #padded_1_image = tf.pad(img1_enc, paddings, mode='CONSTANT',constant_values=0.0)
    padded_1_image = Dense(prev_hidden+img_hidden)(img1_enc)
    cap1_dec = decoder(cap1_emb, initial_state = padded_1_image )

    prev_1 = prev_dense(prev_encoder(cap1_dec))
    hidden_2 = concatenate([prev_1,img2_enc], axis=-1)
    cap2_dec = decoder(cap2_emb , initial_state = hidden_2)

    prev_2 = prev_dense(prev_encoder(concatenate([cap1_dec,cap2_dec], axis=-2)))
    hidden_3 = concatenate([prev_2,img3_enc], axis=-1)
    cap3_dec = decoder(cap3_emb , initial_state = hidden_3)

    prev_3 = prev_dense(prev_encoder(concatenate([cap1_dec,cap2_dec,cap3_dec], axis=-2)))
    hidden_4 = concatenate([prev_3,img4_enc], axis=-1)
    cap4_dec = decoder(cap4_emb , initial_state = hidden_4)

    prev_4 = prev_dense(prev_encoder(concatenate([cap1_dec,cap2_dec,cap3_dec,cap4_dec], axis=-2)))
    hidden_5 = concatenate([prev_4,img5_enc], axis=-1)
    cap5_dec = decoder(cap5_emb , initial_state = hidden_5)

    decoder_out = concatenate([cap1_dec, cap2_dec, cap3_dec, cap4_dec, cap5_dec], axis=-2)

    decoder_dense = TimeDistributed(Dense(1000, activation=None, kernel_regularizer=l2(0.000)))(decoder_out)
    outputs = Dense(vocab_size, activation='softmax')(decoder_dense)
    model = Model(inputs=[img_input, captions_input], outputs=outputs)

    model.summary()
    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['sparse_categorical_accuracy'])

    return model

"""Training"""

model=build_model()

"""Or load saved model"""

#model = load_model('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_100_stories_checkpoint-ep033-loss3.835.h5')

"""Prep for training"""

X1,X2,y = all_data(existing_stories, image_embd, wordtoix, MAX_SEQUENCE_LENGTH, 50)#35565

# Or use a Generator
split = int(0.9*np.shape(X1)[0])
training_generator = DataGenerator(X1[:split,:,:],X2[:split,:,:],y[:split,:], vocab_size=vocab_size, batch_size=32)
validation_generator = DataGenerator(X1[split:,:,:],X2[split:,:,:],y[split:,:], vocab_size=vocab_size, batch_size=32)

"""Training"""

# Use check point if needed , make sure to specify file path
# checkpoint = ModelCheckpoint(filepath_checkpoint, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
# You can use a generator or planly feed in all the data to the trainer
# history = model.fit(training_generator, epochs=5, verbose=1, shuffle=True, max_queue_size=5, workers=5, use_multiprocessing=True )
history = model.fit([X1,X2],y, epochs=100, verbose=1, shuffle=True, validation_split=0.1, batch_size=64 )

"""Plot loss curve"""

import matplotlib.pyplot as plt
print(history.history.keys())
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()

"""Save Model:"""

# save Model and Weights:
# Make sure to modify file path as needed
model.save_weights('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_all_weights_sure.h5')
model.save('/content/drive/My Drive/Colab_Notebooks/DL_data/models/final_model_basic_all_stories_finale_pad_sure.h5')

"""Test some stories"""

import os.path as osp
import os
from pprint import pprint
from skimage.transform import rescale, resize
from skimage import data, color, io
import skimage
import PIL
import scipy
import json
import os.path
from os import path
import tensorflow as tf

from keras.applications.xception import preprocess_input
from keras.applications.xception import Xception
from keras.models import Model
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img
from keras.applications.imagenet_utils import decode_predictions
def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None
def load_image(image_path,target_size):
    img = skimage.io.imread(image_path)
    image_resized = skimage.transform.resize(img, target_size, anti_aliasing=True)
    return image_resized

def load_cnn_model():
    model = Xception()
    model.layers.pop()
    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)
    return model

def extract_features_from_images(image_path):
    model = load_cnn_model()
#     if path.exists(image_path):
    image = load_image(image_path, target_size=(299, 299))
    if image.shape == (299, 299, 3):
        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
        image = preprocess_input(image)
        feature = model.predict(image, verbose=0)
        return feature
feature1 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story2/3333520.jpg')
feature2 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story2/3333553.jpg')
feature3 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story2/3333572.jpg')
feature4 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story2/3333598.jpg')
feature5 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story2/3333640.jpg')

feature6 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story1/3204125.jpg')
feature7 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story1/3204140.jpg')
feature8 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story1/3204172.jpg')
feature9 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story1/3204197.jpg')
feature10 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story1/3204207.jpg')

feature11 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story3/4566221.jpg')
feature12 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story3/4566247.jpg')
feature13 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story3/4566203.jpg')
feature14 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story3/4566238.jpg')
feature15 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story3/4566294.jpg')

feature16 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story4/6053976.jpg')
feature17 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story4/6053966.jpg')
feature18 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story4/6053990.jpg')
feature19 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story4/6053960.jpg')
feature20 = extract_features_from_images('/content/drive/My Drive/Colab_Notebooks/DL_data/TEST_STORIES/story4/6053947.jpg')

x1_v = np.zeros(shape=(1,5,2048))
x2_v =np.zeros(shape=(1,5,20))
x1_v[0][0] = feature1[0]
x1_v[0][1] = feature2[0]
x1_v[0][2] = feature3[0]
x1_v[0][3] = feature4[0]
x1_v[0][4] = feature5[0]

# print(x1_v)

x2_v_in = np.zeros(shape=(1,5,20))
x2_v_out = np.zeros(shape=(1,5,20))
for i in range(5):
  x2_v_in[0][i][0] = wordtoix['startseq']
  for j in range(MAX_SEQUENCE_LENGTH): 
    yhat = model.predict([x1_v,x2_v_in], verbose=0)
    yhat_s = yhat[0,(i*MAX_SEQUENCE_LENGTH)+j,:]
    cur_index_tmp = np.argmax(yhat_s)
    #cur_index_tmp = np.random.choice(len(yhat_s), p=yhat_s)
    x2_v_out[0][i][j] = cur_index_tmp
    if cur_index_tmp == 0 or cur_index_tmp == 2: break ##reached endseq
    if j+1<MAX_SEQUENCE_LENGTH:
      x2_v_in[0][i][j+1] = cur_index_tmp
    
caps=''
here = np.concatenate(x2_v_out).ravel()
for w in here:
  if w in ixtoword:
    if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue
    caps += " " + ixtoword[w]
print(caps)

x1_v = np.zeros(shape=(1,5,2048))
x2_v =np.zeros(shape=(1,5,20))
x1_v[0][0] = feature6[0]
x1_v[0][1] = feature7[0]
x1_v[0][2] = feature8[0]
x1_v[0][3] = feature9[0]
x1_v[0][4] = feature10[0]

# print(x1_v)

x2_v_in = np.zeros(shape=(1,5,20))
x2_v_out = np.zeros(shape=(1,5,20))
for i in range(5):
  x2_v_in[0][i][0] = wordtoix['startseq']
  for j in range(MAX_SEQUENCE_LENGTH): 
    yhat = model.predict([x1_v,x2_v_in], verbose=0)
    yhat_s = yhat[0,(i*MAX_SEQUENCE_LENGTH)+j,:]
    cur_index_tmp = np.argmax(yhat_s)
    #cur_index_tmp = np.random.choice(len(yhat_s), p=yhat_s)
    x2_v_out[0][i][j] = cur_index_tmp
    if cur_index_tmp == 0 or cur_index_tmp == 2: break ##reached endseq
    if j+1<MAX_SEQUENCE_LENGTH:
      x2_v_in[0][i][j+1] = cur_index_tmp
    
caps=''
here = np.concatenate(x2_v_out).ravel()
for w in here:
  if w in ixtoword:
    if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue
    caps += " " + ixtoword[w]
print(caps)

x1_v = np.zeros(shape=(1,5,2048))
x2_v =np.zeros(shape=(1,5,20))
x1_v[0][0] = feature11[0]
x1_v[0][1] = feature12[0]
x1_v[0][2] = feature13[0]
x1_v[0][3] = feature14[0]
x1_v[0][4] = feature15[0]

# print(x1_v)

x2_v_in = np.zeros(shape=(1,5,20))
x2_v_out = np.zeros(shape=(1,5,20))
for i in range(5):
  x2_v_in[0][i][0] = wordtoix['startseq']
  for j in range(MAX_SEQUENCE_LENGTH): 
    yhat = model.predict([x1_v,x2_v_in], verbose=0)
    yhat_s = yhat[0,(i*MAX_SEQUENCE_LENGTH)+j,:]
    cur_index_tmp = np.argmax(yhat_s)
    #cur_index_tmp = np.random.choice(len(yhat_s), p=yhat_s)
    x2_v_out[0][i][j] = cur_index_tmp
    if cur_index_tmp == 0 or cur_index_tmp == 2: break ##reached endseq
    if j+1<MAX_SEQUENCE_LENGTH:
      x2_v_in[0][i][j+1] = cur_index_tmp
    
caps=''
here = np.concatenate(x2_v_out).ravel()
for w in here:
  if w in ixtoword:
    if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue
    caps += " " + ixtoword[w]
print(caps)

x1_v = np.zeros(shape=(1,5,2048))
x2_v =np.zeros(shape=(1,5,20))
x1_v[0][0] = feature16[0]
x1_v[0][1] = feature17[0]
x1_v[0][2] = feature18[0]
x1_v[0][3] = feature19[0]
x1_v[0][4] = feature20[0]

# print(x1_v)

x2_v_in = np.zeros(shape=(1,5,20))
x2_v_out = np.zeros(shape=(1,5,20))
for i in range(5):
  x2_v_in[0][i][0] = wordtoix['startseq']
  for j in range(MAX_SEQUENCE_LENGTH): 
    yhat = model.predict([x1_v,x2_v_in], verbose=0)
    yhat_s = yhat[0,(i*MAX_SEQUENCE_LENGTH)+j,:]
    cur_index_tmp = np.argmax(yhat_s)
    #cur_index_tmp = np.random.choice(len(yhat_s), p=yhat_s)
    x2_v_out[0][i][j] = cur_index_tmp
    if cur_index_tmp == 0 or cur_index_tmp == 2: break ##reached endseq
    if j+1<MAX_SEQUENCE_LENGTH:
      x2_v_in[0][i][j+1] = cur_index_tmp
    
caps=''
here = np.concatenate(x2_v_out).ravel()
for w in here:
  if w in ixtoword:
    if ixtoword[w] == 'startseq' or ixtoword[w] =='endseq': continue
    caps += " " + ixtoword[w]
print(caps)
